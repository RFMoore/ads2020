[["index.html", "Advanced Data Science 2020 1 Welcome and Syllabus 1.1 Assumptions and pre-requisites 1.2 Learning Objectives 1.3 Course Staff 1.4 Course logistics 1.5 Assignment Due Dates 1.6 The Pandemic 1.7 Grading 1.8 Assignments 1.9 Code of Conduct 1.10 Academic Ethics 1.11 Disability support services 1.12 Email alerts 1.13 Previous versions of the class 1.14 Typos and corrections", " Advanced Data Science 2020 Jeff Leek and Roger D. Peng 2020-11-29 1 Welcome and Syllabus Welcome! We are very excited to have you in our two-term (one semester) course on Advanced Data Science with course numbers 140.711 and 140.712 offered by the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. This course is designed for PhD students at Johns Hopkins Bloomberg School of Public Health. We are usually pretty flexible about permitting outside students but we want everyone to be aware of the goals and assumptions so no one feels like they are surprised by how the class works. The primary goal of the course is to teach you how to deconstruct, perform, and communicate professional data analyses across diverse media. The class is not designed to teach a set of statistical methods or packages - there are a ton of awesome classes, books, and tutorials about those things out there! Rather the goal is to help you to organize your thinking around how to combine the things you have learned about statistics, data manipulation, and visualization into complete data analyses that answer important questions about the world around you. 1.1 Assumptions and pre-requisites The course is designed for PhD students in the Johns Hopkins Biostatistics Masters and PhD programs and assumes significant background in statistics. Specifically we assume: You know the basics of statistics The central dogma (estimates, standard errors, basic distributions, etc.) Key statistical terms and methods Estimtion vs testing vs prediction You know how to fit and interpret statistical models Linear Models Generalized Linear Models Smoothing splines Basic mixture models You know the basics of R or Python You can read in, clean, tidy data You can fit models You can make visualizations You know the basics of reproducible research You know what version control is You know how to use Github You know how to use R/Rmarkdown Since the target audience for this course is advanced students in statistics we will not be able to spend significant time covering these concepts and technologies. To give you some idea about how these prerequisites will impact your experience in the course, we will be turning in all assignments via R markdown documents submitted through Github pull requests. The majority of the assignments will involve critiquing, fitting and interpreting statistical analyses - primarily focused on regression. Data analyses you will perform will also often involve significant data extraction, cleaning, and transformation. Hopefully all of that sounds familiar to you so you can focus on the concepts we will be teaching around deconstructing and constructing data analyses. Some resources that may be useful if you feel you may be missing pieces of this background: Statistics - Mathematical Biostatistics Bootcamp I (Coursera); Mathematical Biostatistics Bootcamp II (Coursera) Basic Data Science - Cloud Data Science (Leanpub); Data Science Specialization (Coursera) Version Control - Github Learning Lab; Happy Git and Github for the useR Rmarkdown - Rmarkdown introduction 1.2 Learning Objectives Our goal is by the end of our class: You will be able to critique a data analysis and separate good from bad analysis. Specifically you will be able to: Identify the underlying question Evaluate the “arc” of the data analysis Identify the underlying type of question Identify the study design Determine if visualizations are appropriate Determine if methods are appropriate Identify pipeline issues Identify reproducibility issues Identify common fallacies and mistakes Distinguish what is a real problem from what is just hard Identify common fallacies and mistakes. Evaluate the relationship between study design, data, and claims to data justification You will be able to produce a complete data analysis. Specifically you will learn to: Translate general questions to data analysis questions Explore your data skeptically Select appropriate data analytic tools given the study design Combine appropriate data analytic tools into pipelines Identify strengths and weaknesses of data pipelines you produce Describe the results of your analysis accurately Decide what is and is not relevant to the “arc” of the data analysis Write the “arc” of the data analysis Avoid “reinventing the wheel” You will be able to produce the components of a data analytic paper: The “arc” of a data analysis Abstracts Introductions Figures Tables Methods sections Discussion/limitations sections You will be able to produce the components of a methods paper: The “arc” of a methods paper Abstracts Introductions Figures Tables Simulation sections Applications sections Discussion/limitations sections You will be able to produce the components of a data analytic presentation for technical and non-technical audiences: Problem introduction Methods Results Conclusions You will be able to identify key issues in data analytic relationships. Specifically you will be able to: Elicit objective functions from collaborators Identify types of data analysis relationships (collaboration, consultation, employment) Identify successful stategies for data analysis based on relationship type Identify key ethical issues in data analysis Understand your responsibility as a data analyst Explain the value of data science to non-technical audiences 1.3 Course Staff The course instructors this year are Jeff Leek and Roger Peng. We are both professors in the Biostatistics Department at Johns Hopkins and Directors of the Johns Hopkins Data Science Lab. Jeff’s research focuses on human genomics, meta-research, and edtech for social good. Roger’s research focuses on air pollution, spatial statistics, and reproducibility. We have been friends for about 10 years and are excited to teach you some of the ins and outs of data science. We also have a couple of amazing TA’s this year: Eric Bridgeford who works on independence testing, manifold embedding, and graph inference; and Athena Chen work works on developing statistical tools to analyzing proteomic and genomic data to facilitate a deeper understanding of disease. 1.4 Course logistics This is a pretty unusual year because we will be entirely online. So our logistics will be a little different than usual. The course webpage will be here at: http://jtleek.com/ads2020/ All communication for the course is going to take place on one of four platforms: Slack - for discussion, sharing resources, collaborating, and announcements - Course slack channel: jhsph-ads-2020.slack.com Github - for submitting assignments - Course Github: https://github.com/jtleek/ads2020 Zoom - for live class discussions - Course Zoom: Link available on Course Slack Hypothesis for annotating/reviewing data analyses The primary communication for the class will go through Slack. That is where we will post course announcements, post all assignments, host most of our asynchronous course discussion, and as the primary means of communication between course participants and course instructors. You should request access to the JHU Advanced Data Science Course Slack immediately. The course TA’s will approve your access. Once you have access you will also be able to find the course Zoom and Zoom password. We will have two synchronous meetings a week for discussion (see section on Discussions below) - the class will be split into two approximately equal groups for these sessions: Available Times: - Mondays 9-10AM Baltimore Time - Mondays 1:30-2:30PM Baltimore Time Location: Zoom - link available on Slack For people who miss the sessions we will try to have a recap and notes that we will post to Slack so people can read them offline. If you haven’t already, please fill out the pre-course survey with your information and your preferred discussion time. 1.5 Assignment Due Dates All course assignment due dates will appear on the weekly course chapter. Please refer to these chapters for due dates. 1.6 The Pandemic This is how 2020 feels: It is super tough to be dealing with a pandemic, an economic crisis, challenges with visas and travel and coordinating school online. Your instructors understand that this is not an ordinary year. We are ultra sympathetic to family challenges and life challenges. We both have small children at home (who may make cameos in class discussions). Our goal is to make as much of the class asynchronous as possible so you can work whenever you have time, our plan is to be as understanding as possible when it comes to grading attendance, and any issues that come up with the course. Please don’t hesitate to reach out to us if you are having issues and we will do our best to direct you to whatever resources we have/accomodate you however we can. We think the material in this course is important, fun, and this is an opportunity to learn a lot. But life is more important than a course and if there was ever a year that life might get in the way of learning, this is that year. Good enough is the excellence of 2020. 1.7 Grading 1.7.1 Philosophy We believe the purpose of graduate education is to train you to be able to think for yourself and initiate and complete your own projects. We are super excited to talk to you about ideas, work out solutions with you, and help you to figure out how to produce professional data analyses. We don’t think that graduate school grades are important for this purpose. This means that we don’t care very much about graduate student grades. That being said, we have to give you a grade so they will be: A - Excellent - 90%+ B - Passing - 80%+ C - Needs improvement - 70%+ We rarely give out grades below a C and if you consistently submit work, participate in discussions, and do your best you are very likely to get an A or a B in the course. 1.7.2 Relative weights This course is primarily focused on deconstructing and constructing data analyses. The grading will be based on your participation in the course and helping each other improve your data analyses. The breakdown of grading will be: 40% for completing required reviews - see section on reviews below 40% for completing required data analysis assignments - see section on data analysis assignments below 20% for course participation on Slack and Zoom - see section on class participation below If you submit each review, it is your own work, and it meets a basic level of completeness and effort you will get 100% for that review. If you submit a review but it doesn’t meet basic completeness and effort you will receive 50%. If you do not submit a review you will receive 0%. If you submit a data analysis assignment, it is your own work, and it meets a basic level of completeness and effort you will get 100% for that data analysis assignment. If you submit a data analysis assignment but it doesn’t meet basic completeness and effort you will receive 50%. If you do not submit a review you will receive 0%. Grading participation is difficult in the best of circumstance and in a pandemic it is basically impossible. If you are at 80% of your assigned discussion sessions and participate in the discussion most of the time and respond to Slack prompts at least 5 times during the course of the term you will receive full participation points. If this level of course participation is challenging for you please reach out to the course instructors and we will work with you to figure out how to ensure you can participate sufficiently to get full points. 1.8 Assignments 1.8.1 Submitting assignments You will be invited to the JHSPH Advanced Data Science course organization: https://github.com/advdatasci. There will be one repo for each assignment. You will see two copies - the template repository (public) and your assigned homework repo which will be suffixed with your github user name (private). The assignment repos will include an Rmd file each week for you to fill out. For each assignment we will provide a time when we will pull your changes from Github. We will assume whatever version we pull at that time is what you are turning in. When we start peer reviewing each other’s work, after the submission deadline you will also be assigned another repo (private) with a suffix of your github user name and “peer-review”. This repo will include your peer’s work and a reviewing form which we will ask you to fill out and submit. After peer review is completed the feedback will be returned to you and you will be able to pull those changes to your computer, fill out a reviewer feedback form, and push your changes back to Github. Instruction submissions will be included with each assignment to remind you of the process you need to take and what dates/times to complete assignments. 1.8.2 Data Analysis Assignments After we have spent a few weeks reviewing data analytic work written by others, we will begin working on data analyses within 1.8.3 Reproducibility We will talk about reproducibility a bit during class, and it will be a part of the homework assignments as well. Reproducibility of scientific code is very challenging, so the faculty and TAs completely understand difficulties that arise. But we think that it is important that you practice reproducible research. In particular, your homework assignments should perform the analyses you are asked to do and create the figures and tables you are asked to make as a part of the compilation of your document. Some pointers for some issues that have come up: You should make sure your code is executed when the document is compiled (so use eval=FALSE sparingly). Most code chunks should be run, especially those that create plots. You should have the document create figures as a part of the document compiling, rather than producing the figures in advance with distinct code and then embedding them in the HTML. A good best practice is to have your library calls all in the first “setup” chunk of your Rmarkdown file. That helps readers take a quick look and see what they need to run your code and to identify any issues or potential hidden dependencies. Your document should knit, in full, with a press of the knit button, with no user input required. This is a really important (and challenging!) part of reproducibility. As a simple check you might try exiting R and then starting a new session and recompiling your document with a clean session to make sure it runs. Another good idea is to move your whole project to a separate sub-folder and trying to run it. The gold standard would be having a friend try to run your document on their computer. As we discussed in the organizing projects lecture, an absolute path is a path which tells you exactly how to get to a file/folder starting from “root” (usually, it will start with a \"/\" or start with the address of a particular drive; ie, \"C://\"). This is not reproducible because an absolute path, more often than not, is going to be unique to your particular computer and layout. Rather, you should use relative paths. A relative path will start with the current directory, and you index off that. In the case of our homework documents, this means that any scripts/files/dependencies/etc you need should be pushed directly to your repository, and then you load them with a relative path inside your actual homework assignment (relative your working directory). Usually, a relative path will start with “./script.R” or you just specify a file directly; e.g., “script.R” (which the computer interprets as “start from the directory I am in”) or might start with telling you to go back a directory or something first (“../”). An important consideration: your R terminal within Rstudio (the little arrow at the bottom; &gt;, that executes raw R commands) will place the “working directory” somewhere (possibly) different from your markdown document. However, if you use the built-in Rmarkdown IDE in RStudio (e.g., pressing the green “run” arrow, or knitting the document) the working directory will be the same directory as the markdown document you are compiling. You might also consider the here package if you are using R to set your paths. Not everything needs to be run in real time. A few students (accurately) note that if you have, say, a code block that takes 20 minutes to run, it is perfectly acceptable/understandable to set that block to eval=FALSE , save whatever that block produces to say a rds object or a csv that you push to your repo, and at the top of the next block, load it in. 1.8.4 Data Analysis Reviews In this course we will be reviewing both published data analyses and each other’s work. You will review both in writing and orally during course discussions. Reviews in our course will take the following format: Written reviews For each assigned data analysis you will provide a written review which will include a summary of the data analysis and answers to key questions. At the beginning of the term these reviews will focus on published/public data analyses, but once we begin to turn in data analysis assignments, they will be peer reviews of each other’s work. Oral reviews The course will be broke up into two groups. Each group will meet once a week to discuss the papers/data analyses for that week. Each week, each group will have 2 discussion leaders: Lead Reviewer 1 and Lead Reviewer 2. We will rotate so each person gets to be a lead reviewer. Your responsibilities as lead reviewer are to: Lead Reviewer 1: Complete your written review and provide an overall summary of the data analysis and your answers to the questions. You are responsible for leading the discussion of the analysis. Lead Reviewer 2: Complete your written review and provide a second opinon on the data analysis, either supporting or providing new viewpoints of Reviewer 1. Both reviewers are encouraged to lead discussion of the data analysis to get feedback from the other participants in the session. 1.8.5 Reviewing Code of Conduct We will be reviewing both public work and each other’s work in both written and oral form. Reviewing well is an art form and is an important skill to master - and not just for this course! Regardless of where you go after Hopkins, you will be tasked with reviewing the work of others. The key principles of doing a good job in reviewing and the foundation for our course code of conduct are: Being concise - nothing extraneous Being precise - stating the specific problems with the manuscript or data analysis Being honest - stating any real issues you perceive Being constructive - stating how the authors could address the problems you have found Being polite - this helps focus on real issues rather than pet peeves. Reviewing each other’s work well is a critical challenge. Remember that there is a person behind the data analysis and you want them to improve. It is very easy to be sucked into the temptation to write a review that is entirely critical or even rude. The best reviews follow the guidelines above and are short, percise, documents that politely suggest constructive critiques. It takes practice to produce these kinds of reviews, which we will work on in class! One of the biggest privileges is the priveledge to say you don’t know or that you need something explained. I use this priveledge all the time and it makes it much easier for me when I’m trying to learn new concepts. I want you all to feel that privilege in this class. It is critical that we are able to have discussions in the class and everyone can voice their opinion without feeling looked down on. So let’s work together to allow everyone space to learn maximally. An amazing benefit of my privilege is being able to say “I didn't understand that. Could you explain it again?” as many times as necessary without having to worry that people will think I'm stupid. — Arvind Narayanan (@random_walker) August 26, 2020 Jeff has previously written a guide for written reviews of papers. 1.9 Code of Conduct We are committed to providing a welcoming, inclusive, and harassment-free experience for everyone, regardless of gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion (or lack thereof), political beliefs/leanings, or technology choices. We do not tolerate harassment of course participants in any form. Sexual language and imagery is not appropriate for any work event, including group meetings, conferences, talks, parties, Twitter and other online media. This code of conduct applies to all course participants, including instructors and TAs, and applies to all modes of interaction, both in-person and online, including GitHub project repos, Slack channels, and Twitter. Course participants violating these rules will be referred to leadership of the Department of Biostatistics and the Title IX coordinator at JHU and may face expulsion from the class. All class participants agree to: Be considerate in speech and actions, and actively seek to acknowledge and respect the boundaries of other members. Be respectful. Disagreements happen, but do not require poor behavior or poor manners. Frustration is inevitable, but it should never turn into a personal attack. A community where people feel uncomfortable or threatened is not a productive one. Course participants should be respectful both of the other course participants and those outside the course. Refrain from demeaning, discriminatory, or harassing behavior and speech. Harassment includes, but is not limited to: deliberate intimidation; stalking; unwanted photography or recording; sustained or willful disruption of talks or other events; inappropriate physical contact; use of sexual or discriminatory imagery, comments, or jokes; and unwelcome sexual attention. If you feel that someone has harassed you or otherwise treated you inappropriately, please alert Jeff Leek or Roger Peng. Take care of each other. Refrain from advocating for, or encouraging, any of the above behavior. And, if someone asks you to stop, then stop. Alert Jeff Leek or Roger Peng if you notice a dangerous situation, someone in distress, or violations of this code of conduct, even if they seem inconsequential. 1.9.1 Need Help? Please speak with Jeff Leek or Roger Peng. You can also reach out to Karen Bandeen-Roche, chair of the department of Biostatistics or Margaret Taub, Ombudsman for the Department of Biostatistics. You may also reach out to any Hopkins resource for sexual harassment, discrimination, or misconduct: JHU Sexual Assault Helpline, 410-516-7333 (confidential) University Sexual Assault Response and Prevention website Johns Hopkins Compliance Hotline, 844-SPEAK2US (844-733-2528) Hopkins Policies Online JHU Office of Institutional Equity 410-516-8075 (nonconfidential) Johns Hopkins Student Assistance Program (JHSAP), 443-287-7000 University Health Services, 410-955-1892 The Faculty and Staff Assistance Program (FASAP), 443-997-7000 1.9.2 Feedback We welcome feedback on this Code of Conduct. 1.9.3 License and attribution This Code of Conduct is distributed under a CC-BY license. Portions of above text comprised of language from the Codes of Conduct adopted by rOpenSci and Django, which are licensed by CC BY-SA 4.0 and CC BY 3.0. This work was further inspired by Ada Initiative’s “how to design a code of conduct for your community” and Geek Feminism’s Code of conduct evaluations and expanded by Ashley Johnson and Shannon Ellis in the Leek group. 1.10 Academic Ethics Students enrolled in the Bloomberg School of Public Health of The Johns Hopkins University assume an obligation to conduct themselves in a manner appropriate to the University’s mission as an institution of higher education. A student is obligated to refrain from acts which he or she knows, or under the circumstances has reason to know, impair the academic integrity of the University. Violations of academic integrity include, but are not limited to: cheating; plagiarism; knowingly furnishing false information to any agent of the University for inclusion in the academic record; violation of the rights and welfare of animal or human subjects in research; and misconduct as a member of either School or University committees or recognized groups or organizations. Students should be familiar with the policies and procedures specified under Policy and Procedure Manual Student-01 (Academic Ethics), available on the school’s portal. The faculty, staff and students of the Bloomberg School of Public Health and the Johns Hopkins University have the shared responsibility to conduct themselves in a manner that upholds the law and respects the rights of others. Students enrolled in the School are subject to the Student Conduct Code (detailed in Policy and Procedure Manual Student-06) and assume an obligation to conduct themselves in a manner which upholds the law and respects the rights of others. They are responsible for maintaining the academic integrity of the institution and for preserving an environment conducive to the safe pursuit of the School’s educational, research, and professional practice missions. 1.11 Disability support services If you are a student with a documented disability who requires an academic accommodation, please contact the Office of Disability Support Services at 410-502-6602 or via email at JHSPH.dss@jhu.edu. Accommodations take effect upon approval and apply to the remainder of the time for which a student is registered and enrolled at the Bloomberg School of Public Health. 1.12 Email alerts The full course content will be available via this website. All assignments will be posted here and on Slack. But we are trying an experiment with Substack and you can sign up here for email alerts when new course chapters are available: https://jhuadvdatasci.substack.com/. If you aren’t at JHU but want to follow along with the content you are welcome to sign up as well! 1.13 Previous versions of the class https://jhu-advdatasci.github.io/2019/ https://jhu-advdatasci.github.io/2018/ http://jtleek.com/advdatasci/ http://jtleek.com/advdatasci16/ http://jtleek.com/advdatasci15/ https://github.com/jtleek/jhsph753and4 1.14 Typos and corrections Feel free to submit typos/errors/etc via the github repository associated with the class: https://github.com/jtleek/ads2020. You will have the thanks of your grateful instructors! "],["week-1.html", "2 Week 1 2.1 Week 1 Learning objectives 2.2 What is advanced data science anyway? 2.3 Types of data analytic questions 2.4 A data analytic rubric 2.5 Your first assignment - deconstructing an analysis 2.6 Additional Resources 2.7 Homework", " 2 Week 1 2.1 Week 1 Learning objectives At the end of this lesson you will: Be able to define data science and advanced data science Be able to define the types of data analytic questions Be able to follow a data analysis rubric to evaluate an analysis 2.2 What is advanced data science anyway? 2.2.1 Maybe we should start by defining data science…. Before we can define advanced data science we need to define data science. The definition we will use is: Data science is the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience. In general the data science process is iterative and the different components blend together a little bit. But for simplicity lets discretize the tasks into the following 7 steps: Define the question of interest Get the data Clean the data Explore the data Fit statistical models Communicate the results Make your analysis reproducible The reality is that the process is usually much more iterative. This is an excellent diagram describing the usual flow of a data science project by a former student in this class, Simina Boca: Feeling preeety good about this diagram that I wrote in sparkly pens for the data analysis class I'm teaching, which starts tomorrow… Hope it's clear now that data scientists and applied statisticians don't simply press a 🖱️or wave a ! Feedback welcome for future iterations! pic.twitter.com/BoDeyUuNvT — Simina M. Boca (@siminaboca) August 27, 2020 A good data science project answers a real scientific or business analytics question. In almost all of these experiments the vast majority of the analyst’s time is spent on getting and cleaning the data (steps 2-3) and communication and reproducibility (6-7). In most cases, if the data scientist has done her job right the statistical models don’t need to be incredibly complicated to identify the important relationships the project is trying to find. In fact, if a complicated statistical model seems necessary, it often means that you don’t have the right data to answer the question you really want to answer. As Tukey said: The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. One option is to spend a huge amount of time trying to tune a statistical model to try to answer the question but serious data scientist’s usually instead try to go back and get the right data. The result of this process is that most well executed and successful data science projects don’t (a) use super complicated tools or (b) fit super complicated statistical models. The characteristics of the most successful data science projects I’ve evaluated or been a part of are: (a) a laser focus on solving the scientific problem, (b) careful and thoughtful consideration of whether the data is the right data and whether there are any lurking confounders or biases and (c) relatively simple statistical models applied and interpreted skeptically. 2.2.2 Data Science is hard, but not like math is hard It turns out doing those three things is actually surprisingly hard and very, very time consuming. It is my experience that data science projects take a solid 2-3 times as long to complete as a project in theoretical statistics. The reason is that inevitably the data are a mess and you have to clean them up, then you find out the data aren’t quite what you wanted to answer the question, so you go find a new data set and clean it up, etc. After a ton of work like that, you have a nice set of data to which you fit simple statistical models and then it looks super easy to someone who either doesn’t know about the data collection and cleaning process or doesn’t care. This poses a major public relations problem for serious data scientists. When you show someone a good data science project they almost invariably think “oh that is easy” or “that is just a trivial statistical/machine learning model” and don’t see all of the work that goes into solving the real problems in data science. A concrete example of this is in academic statistics. It is customary for people to show theorems in their talks and maybe even some of the proof. This gives people working on theoretical projects an opportunity to “show their stuff” and demonstrate how good they are. The equivalent for a data scientist would be showing how they found and cleaned multiple data sets, merged them together, checked for biases, and arrived at a simplified data set. Showing the “proof” would be equivalent to showing how they matched IDs. These things often don’t look nearly as impressive in talks, particularly if the audience doesn’t have experience with how incredibly delicate real data analysis is. I imagine versions of this problem play out in industry as well (candidate X did a good analysis but it wasn’t anything special, candidate Y used Hadoop to do BIG DATA!). The really tricky twist is that bad data science looks easy too. You can scrape a data set off the web and slap a machine learning algorithm on it no problem. So how do you judge whether a data science project is really “hard” and whether the data scientist is an expert? Just like with anything, there is no easy shortcut to evaluating data science projects. You have to ask questions about the details of how the data were collected, what kind of biases might exist, why they picked one data set over another, etc. In the meantime, don’t be fooled by what looks like simple data science - it can often be pretty effective. This course is designed for PhD students in Biostatistics and most of the courses you have taken have been hard by virtue of mathematical difficulty. These courses focus on deductive resasoning whereby you are told a set of principles or facts and you deduce logically some conclusions through proofs. The steps may be tricky and may require deep mathematical understanding. But the important point is that there is a right answer to most of these problems. Data science is much more akin to inductive reasoning. Inductive reasoning involves taking a small set of representative examples (say a sample of data) and trying to generalize these examples to make a broader statement (say a population). Even when the data are correct, the conculions you may draw can be wildly inaccurate. So the hard thing about data science is describing a path from a set of known data to a set of conclusions that can be supported by the data. Ideally, these conclusions will hold up to scrutiny, skepticism, and replication. In other words, data science is hard precisely because there is often not a “right” answer. Good data science is distinguished from bad data science primarily by a repeatable, thoughtful, skeptical application of an analytic process to data in order to arrive at supportable conclusions. Andrew Gelman Paper on Inductive vs Deductive Reasoning 2.2.3 So what is advanced data science? Ask yourselves, what problem have you solved, ever, that was worth solving, where you knew knew all of the given information in advance? Where you didn’t have a surplus of information and have to filter it out, or you didn’t have insufficient information and have to go find some? This quote comes from a Dan Meyer Ted Talk about patient problem solving. But it applies equally to data science. Data science is answering questions with data and it requires a range of skills and therefore a range of classes: Level 0: Background: Basic computing, some calculus with a focus on optimization, basic linear algebra. Level 1: Data science thinking: How to define a question, how to turn a question into a statement about data, how to identify data sets that may be applicable, experimental design, critical thinking about data sets. Level 2: Data science communication: Teaching students how to write about data science, how to express models qualitatively and in mathematical notation, explaining how to interpret results of algorithms/models. Explaining how to make figures. Level 3: Data science tools: Learning the basic tools of R, loading data of various types, reading data, plotting data. Level 4: Real data: Manipulating different file formats, working with “messy” data, trying to organize multiple data sets into one data set. Level 5: Worked examples: Use real data examples, but work them through from start to finish as case studies, don’t make them easy clean data sets, but have a clear path from the beginning of the problem to the end. Level 6: Just the question: Give students a question where you have done a little research to know that it is posisble to get at least some data, but aren’t 100% sure it is the right data or that the problem can be perfectly solved. Part of the learning process here is knowing how to define success or failure and when to keep going or when to quit. Level 7: The student is the scientist: Have the students come up with their own questions and answer them using data. As you move up the hierarchy of data science classes, the emphasis moves away from technological skills and toward synthesis and communication. The hardest part of data science isn't the technology pic.twitter.com/2IslFWrJwa — Caitlin Hudon 👩💻 (@beeonaposy) August 26, 2020 This advanced data science course assumes you have background in statistics, programming, and the basics of project management - it will instead focus on synthesizing these tools into a data analysis and communicating the analysis to an audience. We will instead focus on the “hard” part of data science, which is understanding the way to use data to make generalizable statements about the world and communicating those results to others. Data analysis is often (incorrectly) distilled down to the set of claims or a whether a p-value is lower than some threshold. But the tip of this iceberg conceals a large number of analytic choices, human behaviors, biases, and conventions that underly the data analytic process. This iceberg inspired the ADS 2020 course hex sticker. In this course we will focus on the parts of the data analysis that are often overlooked, but are critical to data analytic success. 2.3 Types of data analytic questions Data can be used to answer many questions, but not all of them. One of the most innovative data scientists of all time said it best. The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. John Tukey Before performing a data analysis the key is to define the type of question being asked. Some questions are easier to answer with data and some are harder. This is a broad categorization of the types of data analysis questions, ranked by how easy it is to answer the question with data. You can also use the data analysis question type flow chart to help define the question type (from the paper What is the question ) The data analysis question type flow chart 2.3.1 Descriptive A descriptive data analysis seeks to summarize the measurements in a single data set without further interpretation. An example is the United States Census. The Census collects data on the residence type, location, age, sex, and race of all people in the United States at a fixed time. The Census is descriptive because the goal is to summarize the measurements in this fixed data set into population counts and describe how many people live in different parts of the United States. The interpretation and use of these counts is left to Congress and the public, but is not part of the data analysis. 2.3.2 Exploratory An exploratory data analysis builds on a descriptive analysis by searching for discoveries, trends, correlations, or relationships between the measurements of multiple variables to generate ideas or hypotheses. An example is the discovery of a four-planet solar system by amateur astronomers using public astronomical data from the Kepler telescope. The data was made available through the planethunters.org website, that asked amateur astronomers to look for a characteristic pattern of light indicating potential planets. An exploratory analysis like this one seeks to make discoveries, but rarely can confirm those discoveries. In the case of the amateur astronomers, follow-up studies and additional data were needed to confirm the existence of the four-planet system. 2.3.3 Inferential An inferential data analysis goes beyond an exploratory analysis by quantifying whether an observed pattern will likely hold beyond the data set in hand. Inferential data analyses are the most common statistical analysis in the formal scientific literature. An example is a study of whether air pollution correlates with life expectancy at the state level in the United States. The goal is to identify the strength of the relationship in both the specific data set and to determine whether that relationship will hold in future data. In non-randomized experiments, it is usually only possible to observe whether a relationship between two measurements exists. It is often impossible to determine how or why the relationship exists - it could be due to unmeasured data, relationships, or incomplete modeling. 2.3.4 Predictive While an inferential data analysis quantifies the relationships among measurements at population-scale, a predictive data analysis uses a subset of measurements (the features) to predict another measurement (the outcome) on a single person or unit. An example is when organizations like FiveThirtyEight.com use polling data to predict how people will vote on election day. In some cases, the set of measurements used to predict the outcome will be intuitive. There is an obvious reason why polling data may be useful for predicting voting behavior. But predictive data analyses only show that you can predict one measurement from another, they don’t necessarily explain why that choice of prediction works. 2.3.5 Causal A causal data analysis seeks to find out what happens to one measurement if you make another measurement change. An example is a randomized clinical trial to identify whether fecal transplants reduces infections due to Clostridium dificile. In this study, patients were randomized to receive a fecal transplant plus standard care or simply standard care. In the resulting data, the researchers identified a relationship between transplants and infection outcomes. The researchers were able to determine that fecal transplants caused a reduction in infection outcomes. Unlike a predictive or inferential data analysis, a causal data analysis identifies both the magnitude and direction of relationships between variables. 2.3.6 Mechanistic Causal data analyses seek to identify average effects between often noisy variables. For example, decades of data show a clear causal relationship between smoking and cancer. If you smoke, it is a sure thing that your risk of cancer will increase. But it is not a sure thing that you will get cancer. The causal effect is real, but it is an effect on your average risk. A mechanistic data analysis seeks to demonstrate that changing one measurement always and exclusively leads to a specific, deterministic behavior in another. The goal is to not only understand that there is an effect, but how that effect operates. An example of a mechanistic analysis is analyzing data on how wing design changes air flow over a wing, leading to decreased drag. Outside of engineering, mechanistic data analysis is extremely challenging and rarely undertaken. 2.4 A data analytic rubric There is often no “right” or “wrong” answer when evaluating a data analysis, but there are some characteristics that separate good analyses from poor analyses. One difficult thing about advanced data science is that these rules have not been formally defined like much of our statistical theory and are not generally agreed on. We are only at the beginning of developing a “theory” of data analysis - we will learn more about these efforts later in the class. You can think of the theory of data analysis more like guidance than hard and fast rules. Roger Peng outlines this idea in his Dean’s lecture which I would encourage you to watch (the key analogy to music begins around 27 minutes in). For now we will use a basic checklist (adapted from the book Elements of Data Analytic Style) when reviewing data analyses. It can be used as a guide during the process of a data analysis, as a rubric for grading data analysis projects, or as a way to evaluate the quality of a reported data analysis. You don’t have to answer every one of these questions for every data analysis, but they are a useful set of ideas ot keep in the back of your mind when reviewing a data analysis. 2.4.1 Answering the question Did you specify the type of data analytic question (e.g. exploration, association causality) before touching the data? Did you define the metric for success before beginning? Did you understand the context for the question and the scientific or business application? Did you record the experimental design? Did you consider whether the question could be answered with the available data? 2.4.2 Checking the data Did you plot univariate and multivariate summaries of the data? Did you check for outliers? Did you identify the missing data code? 2.4.3 Tidying the data Is each variable one column? Is each observation one row? Do different data types appear in each table? Did you record the recipe for moving from raw to tidy data? Did you create a code book? Did you record all parameters, units, and functions applied to the data? 2.4.4 Exploratory analysis Did you identify missing values? Did you make univariate plots (histograms, density plots, boxplots)? Did you consider correlations between variables (scatterplots)? Did you check the units of all data points to make sure they are in the right range? Did you try to identify any errors or miscoding of variables? Did you consider plotting on a log scale? Would a scatterplot be more informative? 2.4.5 Inference Did you identify what large population you are trying to describe? Did you clearly identify the quantities of interest in your model? Did you consider potential confounders? Did you identify and model potential sources of correlation such as measurements over time or space? Did you calculate a measure of uncertainty for each estimate on the scientific scale? 2.4.6 Prediction Did you identify in advance your error measure? Did you immediately split your data into training and validation? Did you use cross validation, resampling, or bootstrapping only on the training data? Did you create features using only the training data? Did you estimate parameters only on the training data? Did you fix all features, parameters, and models before applying to the validation data? Did you apply only one final model to the validation data and report the error rate? 2.4.7 Causality Did you identify whether your study was randomized? Did you identify potential reasons that causality may not be appropriate such as confounders, missing data, non-ignorable dropout, or unblinded experiments? If not, did you avoid using language that would imply cause and effect? 2.4.8 Written analyses Did you describe the question of interest? Did you describe the data set, experimental design, and question you are answering? Did you specify the type of data analytic question you are answering? Did you specify in clear notation the exact model you are fitting? Did you explain on the scale of interest what each estimate and measure of uncertainty means? Did you report a measure of uncertainty for each estimate on the scientific scale? 2.4.9 Figures Does each figure communicate an important piece of information or address a question of interest? Do all your figures include plain language axis labels? Is the font size large enough to read? Does every figure have a detailed caption that explains all axes, legends, and trends in the figure? 2.4.10 Presentations Did you lead with a brief, understandable to everyone statement of your problem? Did you explain the data, measurement technology, and experimental design before you explained your model? Did you explain the features you will use to model data before you explain the model? Did you make sure all legends and axes were legible from the back of the room? 2.4.11 Reproducibility Did you avoid doing calculations manually? Did you create a script that reproduces all your analyses? Did you save the raw and processed versions of your data? Did you record all versions of the software you used to process the data? Did you try to have someone else run your analysis code to confirm they got the same answers? 2.4.12 R packages Did you make your package name “Googleable” Did you write unit tests for your functions? Did you write help files for all functions? Did you write a vignette? Did you try to reduce dependencies to actively maintained packages? Have you eliminated all errors and warnings from R CMD CHECK? 2.5 Your first assignment - deconstructing an analysis Before we leap into doing data analysis we are going to deconstruct some data analyses to help us discover what are the parts that work and don’t in different contexts. Your first assignment will be to read this data analysis about Mortality in the aftermath of Hurricane Maria. You will answer a series of questions, then we will discuss the analysis during class. Write a brief one paragraph summary of the paper highlighting what you consider to be the key parts of the analysis. Who you think the target audience of this paper is? What kind of question (descriptive, exploratory, inferenential, predictive, casual, or mechanistic) is this paper trying to answer? What is the main question the paper is trying to answer? Do you think the data used in the paper are sufficient to answer the question? What are the main sections of the paper? How do they support (or not) the answer to the question? Do you think the analytic methods (plots, summaries, statistical models) are sufficient to answer the question? Do you think it could have been done with simpler methods? Do you think you could reproduce this analysis based on the text? Do you have access to the data, the code, enough of a description of what happened? Why or why not? Do you think that the analysis was done in the order shown in the paper? Do you think the authors only did the analysis shown in the paper? Do you think that the overall analysis makes a convincing point? Why or why not? 2.6 Additional Resources The Elements of Data Analytic Style {data analysis book} The Art of Data Science {data analysis book} Advanced Data Analysis from an Elementary Point of View {methods book} An Introduction to Statistical Learning {methods book} 2.7 Homework Template Repo: https://github.com/advdatasci/homework1 Repo Name: homework1-ind-yourgithubusername Pull Date: 2020/09/07 9:00AM Baltimore Time "],["week-2.html", "3 Week 2 3.1 Week 2 Learning objectives 3.2 The steps in a data analysis 3.3 Raw, informal, and formal data science 3.4 Additional Resources 3.5 Homework", " 3 Week 2 3.1 Week 2 Learning objectives At the end of this lesson you will be able to: Define the steps in a data analysis Structure a data science project Identify the difference between the levels of an analysis - Raw - Informal - Formal Identify the steps between raw, informal and formal analysis 3.2 The steps in a data analysis Recall from week 1 the definition of data science: Data science is the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience. When people talk about data science they often think of big data, or complicated statistical machine learning tools. But the key word in data science is not data, it is science. Specifically the fastest way to end up with an unsuccessful data science project is to start with the data or tools. Good data science almost invariably starts with a good question. Most data science projects follow a common set of steps: Define the question Define the ideal data set Determine what data you can access Obtain the data Clean the data Exploratory data analysis Statistical prediction/modeling Interpret results Challenge results Synthesize/write up results Create reproducible code These steps might vary somewhat depending on the context of the project, but are remarkably consistent across projects in both industry and academia. To illustrate this process I’m going to use a simple, somewhat contrived, example - but it can equally be applied to any new data science project you start. 3.2.1 Defining the question The difference between real success and failure in data analysis often comes down to the question. The key to settling on a good question is to first start without thinking about the data that you have. Think only about the question you care about. It has been suggested that data makes the scientific method obsolete - but the reality is that big data makes careful scientific thinking more important, not less. Regardless of who is involved in the question definition process there are four criteria that help define a good data science question. It should be specific It should be answerable with data It should be answerable with the data you have It should not be designed to discriminate or cause harm It should come with a definition of success So we might start with a general question - something like: Is it possible to consistently label hand-drawn images of hardware using a simple model? This question could be of interest if, for example, you are building a hand-drawn search feature for a hardware store app or website. Note that this question was defined without thinking about what data we might collect, what data we have on hand, or really any of the potential constraints. It is generally a good idea to start with the question you really care about - ignoring these constraints - and work down to a more specific, answerable question. 3.2.2 Defining the ideal data set The ideal data set might be a large collection of hand drawn images, labeled with the intention of each of the people who made the drawing. This kind of data would allow you to directly develop a classification algorithm and measure its success. However, this might also be an expensive data set to collect. You would need to pay people to make the drawings and take the time to go through your inventory with them to get them to provide gold standard labels. This might be well beyond the resources you have available to you - so you might have to settle for a less than ideal data set. However, it is always important to consider what ideal looks like. At a minimum, this will allow you to identify specific, concrete potential limitations of the data you end up using. 3.2.3 Determine the data you can access If you hadn’t already collected data to answer this question, you might go looking for convenience data. For example you might land on the Google Quickdraw project. The QuickDraw project asks people to draw pictures of different objects and then uses a neural network to predict what has been drawn. The data consist of a large number of hand drawn images and the neural-network predicted labels for those objects from Google. So in applying the criteria above we need to refine our question to be more specific and answerable with the data we have in hand: Can we classify images based on the data collected by QuickDraw into labeled categories from the neural network? Sometimes this would be where the data science project may stop. The question may either not be answerable with data at all - or we may not have sufficiently good data to answer the question. Expert data scientists learn through years of practice to identify questions that are likely to lead to fruitful conclusions. For example, in this case past experience has shown that classification of images with high-accuracy is quite possible - at least with complicated models.. While this data is large and potentially incredibly useful for the question we originally set out to answer, it also has some limitations. Since we defined the ideal data set to start, we can use that as a way to highlight potential limitations of our analysis. For example: The data were collected on the internet at large, and not necessarily from the potential customers at the hardware store. The labels are not defined by the the artist, but by the neural network from Google and so may be inaccurate. The pictures represent a large collection of objects, not simply hardware, and so may not reflect all of the types of images you may see in practice. Another important question is whether the proposed application is ethical or a good idea. There are entire courses on the ethics of data science and we will cover this in more detail later in the course. But for this simple example - we might also want to consider relevant questions such as: How will this classifier work for people with disabilities? Are there cultural differences that will make the classifier work better for certain groups of people? What should happen if a person draws something inappropriate? Despite these limitations and considerations, this data is public, free, and large, so represents an opportunity to start to answer our question. Here we will make the question even more specific, simply for ease of exposition. We will narrow the set of data to the hand drawn images of axes and clouds and see if we can separate the two. The final step in defining the question is coming up with the definition of success. It is critically important to do this before you start and will depend heavily on the context. For example, in our simple case we might only need classification accuracy of 80% for the project to be successful - 4 out of 5 customers will be directed to the right project and the other won’t be harmed. But if you are building a self-driving car, you might need to be significantly more accurate! However, using this definition of success we can write down our final question: Can we classify images of clouds and axes to 80% accuracy based on the data collected by QuickDraw into labeled categories from the neural network? This question only answers a fraction of our initial, broad question. That is often the case! Most data science problems start out with a major open question and narrow down to a question answerable with a given data set. One important consideration we have not discussed here is that as a data scientist you are often not the only person involved in the definition of the question. It may be driven by a scientific collaborator, a project manager, or an executive in industry. The process should still be the same - moving from an important content driven question and drilling down to a specific, answerable data science question. It is generally helpful in these conversations to be open about the entire process of question definition - from generality to specificity. This process may involve significant iteration and will require good communication between you and your collaborators or managers. Often the most difficult part of this process is managing expectations - people without significant data analysis expertise will often expect the data to be capable of answering more, or harder, questions than they actually can. 3.2.4 Obtain the data Now that we have defined our question, we need to obtain some data. In this case, the QuickDraw data are publicly available. You can download them from the web. This process also might involve pulling data from an API, accessing a database and sampling data, or collecting the data directly. Having a variety of tools at your disposal for collecting (sometimes called pulling) data is an important component of your toolbox as a data scientist. There are a number of courses and resources focused on getting data including: Getting and cleaning data Getting data Databases using dplyr Data scraping in R In our simple example, I have downloaded and subsampled the data to make them usable for this analysis. One important thing I haven’t done - but is a good idea in general - is to write code that will automatically and reproducibly pull the data from the source. Including this step as a component of your data analytic code will save you a lot of time in the future when a collaborator asks you to update your analysis with the latest data. We will discuss this more when we talk about structuring data science projects. 3.2.5 Clean the data It has been said that data science is 80% data cleaning and that continues to hold true (at least if the data scientists on Twitter are to be trusted): Have been extremely curious about this for a while now, so I decided to create a poll. “As someone titled ‘data scientist’ in 2019, I spend most of (60%+) my time:” (“Other”) also welcome, add it in the replies. — Vicki Boykis (@vboykis) January 28, 2019 Data cleaning is one of the hardest things to teach. The reason is best summed up in this quote by Hadley Wickham with apologies to Tolstoy: tidy datasets are all alike but every messy dataset is messy in its own way. Each type of data you encounter will have different steps involved in the process. But data cleaning always involves identifying and cataloging data quirks, iconsistencies, and errors and reformatting or reshaping data to the form needed for analysis. In our simple example we can find the data here: https://github.com/jtleek/ads2020/tree/master/docs/resources/week2/. In cases where the data are large or unwieldy an important principle is to make the data small enough to work with. Here we do that by random sampling: Robert Gentleman, Genentech: “make big data as small as possible as quick as is possible” to enable sharing #bigdatamed — Ellie McDonagh (@EllieMcDonagh) May 21, 2014 library(LaF) library(here) axes_json = sample_lines(here::here(&quot;docs/resources/week2/axes.ndjson&quot;),100) clouds_json = sample_lines(here::here(&quot;docs/resources/week2/clouds.ndjson&quot;),100) axes_json[[1]] ## [1] &quot;{\\&quot;word\\&quot;:\\&quot;axe\\&quot;,\\&quot;countrycode\\&quot;:\\&quot;PT\\&quot;,\\&quot;timestamp\\&quot;:\\&quot;2017-03-05 22:57:31.79679 UTC\\&quot;,\\&quot;recognized\\&quot;:true,\\&quot;key_id\\&quot;:\\&quot;5454978309160960\\&quot;,\\&quot;drawing\\&quot;:[[[2,67,151,160,190,237,241,241,214,208,165,149,138,81,29,24,0],[231,122,2,0,25,56,60,66,133,139,96,87,89,164,255,254,238]]]}&quot; The next thing I did was google “quick draw data ndjson rstats”. I found a tutorial and lifted some code for processing ndjson data into data frames. This is a common step in data cleaning - since it is so bespoke by data type you will often want to use Google to search for the data type and the type of cleaning you want to do. library(dplyr) parse_drawing = function(list) { lapply(list$drawing, function(z) {data_frame(x=z[[1]], y=z[[2]])}) %&gt;% bind_rows(.id = &quot;line&quot;) %&gt;% mutate(drawing=list$key_id, row_id=row_number()) } first_axe = rjson::fromJSON(axes_json[[1]]) %&gt;% parse_drawing() first_axe ## # A tibble: 17 x 5 ## line x y drawing row_id ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 2 231 5454978309160960 1 ## 2 1 67 122 5454978309160960 2 ## 3 1 151 2 5454978309160960 3 ## 4 1 160 0 5454978309160960 4 ## 5 1 190 25 5454978309160960 5 ## 6 1 237 56 5454978309160960 6 ## 7 1 241 60 5454978309160960 7 ## 8 1 241 66 5454978309160960 8 ## 9 1 214 133 5454978309160960 9 ## 10 1 208 139 5454978309160960 10 ## 11 1 165 96 5454978309160960 11 ## 12 1 149 87 5454978309160960 12 ## 13 1 138 89 5454978309160960 13 ## 14 1 81 164 5454978309160960 14 ## 15 1 29 255 5454978309160960 15 ## 16 1 24 254 5454978309160960 16 ## 17 1 0 238 5454978309160960 17 3.2.6 Exploratory data analysis Now that we have cleaned the data up into a format we can use we can start to explore the data. For most data analysis projects, exploratory data analysis and data cleaning will go hand-in-hand and it will often be iterative between the two steps. The key point is that you should use EDA to discover issues with the data, sources of variation, and key factors you must account for in downstream analysis. The process is very similar to the parable of the blind men and the elephant: A group of blind men heard that a strange animal, called an elephant, had been brought to the town, but none of them were aware of its shape and form. Out of curiosity, they said: “We must inspect and know it by touch, of which we are capable”. So, they sought it out, and when they found it they groped about it. The first person, whose hand landed on the trunk, said, “This being is like a thick snake”. For another one whose hand reached its ear, it seemed like a kind of fan. As for another person, whose hand was upon its leg, said, the elephant is a pillar like a tree-trunk. The blind man who placed his hand upon its side said the elephant, “is a wall”. Another who felt its tail, described it as a rope. The last felt its tusk, stating the elephant is that which is hard, smooth and like a spear. Similarly, when doing your initial data cleaning you will be looking at different parts of the data and trying to get a mental picture of the complete data set before moving on to modeling. Here we can do some simple EDA and look at a plot of one of the axes library(ggplot2) ggplot(first_axe,aes(x, y)) + geom_point() + scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() then we might fill in the lines to get a better idea of what the drawing looks like to the customer: ggplot(first_axe,aes(x, y)) + geom_path(aes(group = line), lwd=1)+ scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() then we might look at a few more axes to get a handle on the variation in the data set: rjson::fromJSON(axes_json[[2]]) %&gt;% parse_drawing() %&gt;% ggplot(aes(x, y)) + geom_path(aes(group = line), lwd=1)+ scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() then compare these drawings to drawings of clouds rjson::fromJSON(clouds_json[[1]]) %&gt;% parse_drawing() %&gt;% ggplot(aes(x, y)) + geom_path(aes(group = line), lwd=1)+ scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() A bunch of data processing has been done for us, but the data aren’t quite ready to be fed into an algorithm yet. To do that, we’d need a data frame with each row equal to one drawing and each column equal to one feature for that drawing, with an extra column for the drawing output. Can we think of how we’d do that for a data set like this? Here are two things I thought of: I need the points sampled on a regular grid I need the points to be of a manageable size Note that these are semi-arbitrary choices. A lot of data processing is based on the intuition/best guess of the analyst. While this shouldn’t be a cause for alarm, it is important that you document all of these choices as they may have an impact on the answers you ultimately obtain; these choices may need to be explored and explained when interpreting your results. # Let’s start by creating a regular grid of 256 x and y values. library(tibble) grid_dat = as.tibble(expand.grid(x = 1:256,y=1:256)) # Now we could make each x,y value be a grid point with a join - this is overkill grid_axe = left_join(grid_dat,first_axe) # Let’s add an indicator of whether a particular value is zero or not. grid_axe = grid_axe %&gt;% mutate(pixel = ifelse(is.na(line),0,1)) # subsample to a smaller size library(Hmisc) grid_axe$xgroup = cut2(grid_axe$x,g=16,levels.mean=TRUE) grid_axe$ygroup = cut2(grid_axe$y,g=16,levels.mean=TRUE) grid_axe ## # A tibble: 65,536 x 8 ## x y line drawing row_id pixel xgroup ygroup ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 2 2 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 3 3 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 4 4 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 5 5 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 6 6 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 7 7 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 8 8 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 9 9 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 10 10 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## # … with 65,526 more rows # Now I can convert these to numbers so we’ll have them later grid_axe = grid_axe %&gt;% mutate(xgroup = as.numeric(as.character(xgroup)) - 7.5) %&gt;% mutate(ygroup = as.numeric(as.character(ygroup)) - 7.5) # Then average within groups of pixels to get a smaller value small_axe = grid_axe %&gt;% group_by(xgroup,ygroup) %&gt;% summarise(pixel=mean(pixel), .groups = &quot;drop&quot;) small_axe ## # A tibble: 256 x 3 ## xgroup ygroup pixel ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 ## 2 1 17 0 ## 3 1 33 0 ## 4 1 49 0 ## 5 1 65 0 ## 6 1 81 0 ## 7 1 97 0 ## 8 1 113 0 ## 9 1 129 0 ## 10 1 145 0 ## # … with 246 more rows Remember this was our original axe ggplot(first_axe,aes(x, y)) + geom_point() + scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() Now we can look at the small version - after cleaning, sampling to a grid, downsampling to shrink the data and averaging pixels - it looks similar - whew! :) ggplot(small_axe %&gt;% filter(pixel &gt; 0),aes(xgroup, ygroup)) + geom_point() + scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() This is obviously glossing over a lot of exploration and cleaning, remember this should be 80% of the analysis! But it is a fair representation of the kinds of steps you might go through to get the data ready to model. Now let’s do this for all axes and clouds. img_dat = tibble(pixel=NA,type=NA,drawing=NA,pixel_number=NA) ## Axes for(i in 1:100){ tmp_draw = rjson::fromJSON(axes_json[[i]]) %&gt;% parse_drawing() grid_draw = left_join(grid_dat,tmp_draw) %&gt;% mutate(pixel = ifelse(is.na(line),0,1)) grid_draw$xgroup = cut2(grid_draw$x,g=16,levels.mean=TRUE) grid_draw$ygroup = cut2(grid_draw$y,g=16,levels.mean=TRUE) small_draw = grid_draw %&gt;% mutate(xgroup = as.numeric(as.character(xgroup)) - 7.5) %&gt;% mutate(ygroup = as.numeric(as.character(ygroup)) - 7.5) %&gt;% group_by(xgroup,ygroup) %&gt;% summarise(pixel=mean(pixel), .groups = &quot;drop&quot;) %&gt;% select(pixel) %&gt;% mutate(type=&quot;axe&quot;,drawing=i,pixel_number=row_number()) img_dat = img_dat %&gt;% bind_rows(small_draw) } ## Clouds for(i in 1:100){ tmp_draw = rjson::fromJSON(clouds_json[[i]]) %&gt;% parse_drawing() grid_draw = left_join(grid_dat,tmp_draw) %&gt;% mutate(pixel = ifelse(is.na(line),0,1)) grid_draw$xgroup = cut2(grid_draw$x,g=16,levels.mean=TRUE) grid_draw$ygroup = cut2(grid_draw$y,g=16,levels.mean=TRUE) small_draw = grid_draw %&gt;% mutate(xgroup = as.numeric(as.character(xgroup)) - 7.5) %&gt;% mutate(ygroup = as.numeric(as.character(ygroup)) - 7.5) %&gt;% group_by(xgroup,ygroup) %&gt;% summarise(pixel=mean(pixel), .groups = &quot;drop&quot;) %&gt;% select(pixel) %&gt;% mutate(type=&quot;cloud&quot;,drawing=i,pixel_number=row_number()) img_dat = img_dat %&gt;% bind_rows(small_draw) } library(tidyr) img_final = spread(img_dat[-1,],pixel_number,pixel) names(img_final) = c(&quot;type&quot;,&quot;drawing&quot;,paste0(&quot;pixel&quot;,1:256)) img_final ## # A tibble: 200 x 258 ## type drawing pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 axe 1 0 0 0 0 0 0 0 0 ## 2 axe 2 0 0 0 0 0 0.00391 0.00391 0 ## 3 axe 3 0 0.00368 0 0 0 0 0 0.00368 ## 4 axe 4 0 0 0 0 0 0 0 0 ## 5 axe 5 0 0.00391 0.00781 0 0.00781 0.00391 0 0 ## 6 axe 6 0 0 0 0 0 0 0 0 ## 7 axe 7 0 0.00391 0 0 0 0 0.00391 0 ## 8 axe 8 0 0 0 0 0.00391 0.00391 0 0.00391 ## 9 axe 9 0.00781 0.00391 0 0.00781 0.00391 0.00391 0.0117 0.00391 ## 10 axe 10 0 0.0117 0 0 0.00391 0 0 0 ## # … with 190 more rows, and 248 more variables: pixel9 &lt;dbl&gt;, pixel10 &lt;dbl&gt;, ## # pixel11 &lt;dbl&gt;, pixel12 &lt;dbl&gt;, pixel13 &lt;dbl&gt;, pixel14 &lt;dbl&gt;, pixel15 &lt;dbl&gt;, ## # pixel16 &lt;dbl&gt;, pixel17 &lt;dbl&gt;, pixel18 &lt;dbl&gt;, pixel19 &lt;dbl&gt;, pixel20 &lt;dbl&gt;, ## # pixel21 &lt;dbl&gt;, pixel22 &lt;dbl&gt;, pixel23 &lt;dbl&gt;, pixel24 &lt;dbl&gt;, pixel25 &lt;dbl&gt;, ## # pixel26 &lt;dbl&gt;, pixel27 &lt;dbl&gt;, pixel28 &lt;dbl&gt;, pixel29 &lt;dbl&gt;, pixel30 &lt;dbl&gt;, ## # pixel31 &lt;dbl&gt;, pixel32 &lt;dbl&gt;, pixel33 &lt;dbl&gt;, pixel34 &lt;dbl&gt;, pixel35 &lt;dbl&gt;, ## # pixel36 &lt;dbl&gt;, pixel37 &lt;dbl&gt;, pixel38 &lt;dbl&gt;, pixel39 &lt;dbl&gt;, pixel40 &lt;dbl&gt;, ## # pixel41 &lt;dbl&gt;, pixel42 &lt;dbl&gt;, pixel43 &lt;dbl&gt;, pixel44 &lt;dbl&gt;, pixel45 &lt;dbl&gt;, ## # pixel46 &lt;dbl&gt;, pixel47 &lt;dbl&gt;, pixel48 &lt;dbl&gt;, pixel49 &lt;dbl&gt;, pixel50 &lt;dbl&gt;, ## # pixel51 &lt;dbl&gt;, pixel52 &lt;dbl&gt;, pixel53 &lt;dbl&gt;, pixel54 &lt;dbl&gt;, pixel55 &lt;dbl&gt;, ## # pixel56 &lt;dbl&gt;, pixel57 &lt;dbl&gt;, pixel58 &lt;dbl&gt;, pixel59 &lt;dbl&gt;, pixel60 &lt;dbl&gt;, ## # pixel61 &lt;dbl&gt;, pixel62 &lt;dbl&gt;, pixel63 &lt;dbl&gt;, pixel64 &lt;dbl&gt;, pixel65 &lt;dbl&gt;, ## # pixel66 &lt;dbl&gt;, pixel67 &lt;dbl&gt;, pixel68 &lt;dbl&gt;, pixel69 &lt;dbl&gt;, pixel70 &lt;dbl&gt;, ## # pixel71 &lt;dbl&gt;, pixel72 &lt;dbl&gt;, pixel73 &lt;dbl&gt;, pixel74 &lt;dbl&gt;, pixel75 &lt;dbl&gt;, ## # pixel76 &lt;dbl&gt;, pixel77 &lt;dbl&gt;, pixel78 &lt;dbl&gt;, pixel79 &lt;dbl&gt;, pixel80 &lt;dbl&gt;, ## # pixel81 &lt;dbl&gt;, pixel82 &lt;dbl&gt;, pixel83 &lt;dbl&gt;, pixel84 &lt;dbl&gt;, pixel85 &lt;dbl&gt;, ## # pixel86 &lt;dbl&gt;, pixel87 &lt;dbl&gt;, pixel88 &lt;dbl&gt;, pixel89 &lt;dbl&gt;, pixel90 &lt;dbl&gt;, ## # pixel91 &lt;dbl&gt;, pixel92 &lt;dbl&gt;, pixel93 &lt;dbl&gt;, pixel94 &lt;dbl&gt;, pixel95 &lt;dbl&gt;, ## # pixel96 &lt;dbl&gt;, pixel97 &lt;dbl&gt;, pixel98 &lt;dbl&gt;, pixel99 &lt;dbl&gt;, pixel100 &lt;dbl&gt;, ## # pixel101 &lt;dbl&gt;, pixel102 &lt;dbl&gt;, pixel103 &lt;dbl&gt;, pixel104 &lt;dbl&gt;, ## # pixel105 &lt;dbl&gt;, pixel106 &lt;dbl&gt;, pixel107 &lt;dbl&gt;, pixel108 &lt;dbl&gt;, … Since this is a prediction problem, we will need to split the data into a training and a testing set. We will learn more about the structure of machine learning vs inference problems later in the course. For now, you can just take my word for it that if we build a model in the training set, we will need a held out set of independent data to validate and critique our model. library(caret) train_set = createDataPartition(img_final$type,list=FALSE) train_dat = img_final[train_set,] test_dat = img_final[-train_set,] train_dat ## # A tibble: 100 x 258 ## type drawing pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 axe 1 0 0 0 0 0 0 0 0 ## 2 axe 3 0 0.00368 0 0 0 0 0 0.00368 ## 3 axe 5 0 0.00391 0.00781 0 0.00781 0.00391 0 0 ## 4 axe 6 0 0 0 0 0 0 0 0 ## 5 axe 7 0 0.00391 0 0 0 0 0.00391 0 ## 6 axe 9 0.00781 0.00391 0 0.00781 0.00391 0.00391 0.0117 0.00391 ## 7 axe 11 0 0 0 0 0 0 0 0 ## 8 axe 15 0 0.0117 0.00391 0 0.00781 0 0.00391 0 ## 9 axe 18 0 0 0 0 0 0 0.00368 0.00368 ## 10 axe 19 0 0 0 0 0 0 0 0 ## # … with 90 more rows, and 248 more variables: pixel9 &lt;dbl&gt;, pixel10 &lt;dbl&gt;, ## # pixel11 &lt;dbl&gt;, pixel12 &lt;dbl&gt;, pixel13 &lt;dbl&gt;, pixel14 &lt;dbl&gt;, pixel15 &lt;dbl&gt;, ## # pixel16 &lt;dbl&gt;, pixel17 &lt;dbl&gt;, pixel18 &lt;dbl&gt;, pixel19 &lt;dbl&gt;, pixel20 &lt;dbl&gt;, ## # pixel21 &lt;dbl&gt;, pixel22 &lt;dbl&gt;, pixel23 &lt;dbl&gt;, pixel24 &lt;dbl&gt;, pixel25 &lt;dbl&gt;, ## # pixel26 &lt;dbl&gt;, pixel27 &lt;dbl&gt;, pixel28 &lt;dbl&gt;, pixel29 &lt;dbl&gt;, pixel30 &lt;dbl&gt;, ## # pixel31 &lt;dbl&gt;, pixel32 &lt;dbl&gt;, pixel33 &lt;dbl&gt;, pixel34 &lt;dbl&gt;, pixel35 &lt;dbl&gt;, ## # pixel36 &lt;dbl&gt;, pixel37 &lt;dbl&gt;, pixel38 &lt;dbl&gt;, pixel39 &lt;dbl&gt;, pixel40 &lt;dbl&gt;, ## # pixel41 &lt;dbl&gt;, pixel42 &lt;dbl&gt;, pixel43 &lt;dbl&gt;, pixel44 &lt;dbl&gt;, pixel45 &lt;dbl&gt;, ## # pixel46 &lt;dbl&gt;, pixel47 &lt;dbl&gt;, pixel48 &lt;dbl&gt;, pixel49 &lt;dbl&gt;, pixel50 &lt;dbl&gt;, ## # pixel51 &lt;dbl&gt;, pixel52 &lt;dbl&gt;, pixel53 &lt;dbl&gt;, pixel54 &lt;dbl&gt;, pixel55 &lt;dbl&gt;, ## # pixel56 &lt;dbl&gt;, pixel57 &lt;dbl&gt;, pixel58 &lt;dbl&gt;, pixel59 &lt;dbl&gt;, pixel60 &lt;dbl&gt;, ## # pixel61 &lt;dbl&gt;, pixel62 &lt;dbl&gt;, pixel63 &lt;dbl&gt;, pixel64 &lt;dbl&gt;, pixel65 &lt;dbl&gt;, ## # pixel66 &lt;dbl&gt;, pixel67 &lt;dbl&gt;, pixel68 &lt;dbl&gt;, pixel69 &lt;dbl&gt;, pixel70 &lt;dbl&gt;, ## # pixel71 &lt;dbl&gt;, pixel72 &lt;dbl&gt;, pixel73 &lt;dbl&gt;, pixel74 &lt;dbl&gt;, pixel75 &lt;dbl&gt;, ## # pixel76 &lt;dbl&gt;, pixel77 &lt;dbl&gt;, pixel78 &lt;dbl&gt;, pixel79 &lt;dbl&gt;, pixel80 &lt;dbl&gt;, ## # pixel81 &lt;dbl&gt;, pixel82 &lt;dbl&gt;, pixel83 &lt;dbl&gt;, pixel84 &lt;dbl&gt;, pixel85 &lt;dbl&gt;, ## # pixel86 &lt;dbl&gt;, pixel87 &lt;dbl&gt;, pixel88 &lt;dbl&gt;, pixel89 &lt;dbl&gt;, pixel90 &lt;dbl&gt;, ## # pixel91 &lt;dbl&gt;, pixel92 &lt;dbl&gt;, pixel93 &lt;dbl&gt;, pixel94 &lt;dbl&gt;, pixel95 &lt;dbl&gt;, ## # pixel96 &lt;dbl&gt;, pixel97 &lt;dbl&gt;, pixel98 &lt;dbl&gt;, pixel99 &lt;dbl&gt;, pixel100 &lt;dbl&gt;, ## # pixel101 &lt;dbl&gt;, pixel102 &lt;dbl&gt;, pixel103 &lt;dbl&gt;, pixel104 &lt;dbl&gt;, ## # pixel105 &lt;dbl&gt;, pixel106 &lt;dbl&gt;, pixel107 &lt;dbl&gt;, pixel108 &lt;dbl&gt;, … We can look at a few of the pixels to see if we see any differences between the two types of images: ggplot(train_dat,aes(x=type,y=pixel1)) + geom_boxplot() + theme_minimal() ggplot(train_dat,aes(x=type,y=pixel100)) + geom_boxplot() + theme_minimal() In general, this would kick off another phase of exploratory analysis before setting off to build a model. 3.2.7 Statistical prediction/modeling Now that we have a sufficiently organized and cleaned data set we can go about modeling. This is the part that tends to get a lot of attention. Random forests, deep neural nets, and penalized regression, oh my! But the best data analyses use the simplest model that they can to get the job done. There are always tradeoffs - typically between some measure of accuracy and either cost, time, interpretability, or convenience. We will discuss these tradeoffs later in the class, but in general, it is better to err on the side of simplicity when developing a statistical model. For this simple example we will use penalized logistic regression. In particular, we use the lasso penalty - which encourages a model that uses as few of the pixels as possible when predicting. myGrid &lt;- expand.grid( alpha = 1, lambda = seq(0.0001, 1, length = 20) ) mod = train(as.factor(type) ~ . - drawing , data=train_dat, method=&quot;glmnet&quot;, tuneGrid = myGrid) confusionMatrix(as.factor(test_dat$type),predict(mod,test_dat)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction axe cloud ## axe 45 5 ## cloud 10 40 ## ## Accuracy : 0.85 ## 95% CI : (0.7647, 0.9135) ## No Information Rate : 0.55 ## P-Value [Acc &gt; NIR] : 1.716e-10 ## ## Kappa : 0.7 ## ## Mcnemar&#39;s Test P-Value : 0.3017 ## ## Sensitivity : 0.8182 ## Specificity : 0.8889 ## Pos Pred Value : 0.9000 ## Neg Pred Value : 0.8000 ## Prevalence : 0.5500 ## Detection Rate : 0.4500 ## Detection Prevalence : 0.5000 ## Balanced Accuracy : 0.8535 ## ## &#39;Positive&#39; Class : axe ## It looks like we get a reasonable level of accuracy compared to our original question - we were shooting for 80% accuracy, and for low levels of the regularization parameter we are above 80%: plot(mod) 3.2.8 Interpret results Next we might dig in deeper and try to understand why and how our model is working. For example we might look at the pixels with non-zero coefficients. library(pheatmap) coef(mod$finalModel, mod$bestTune$lambda)[-1] %&gt;% abs() %&gt;% matrix(byrow=TRUE,nrow=16) %&gt;% pheatmap(.,cluster_cols=FALSE,cluster_rows=FALSE) We see that most of the non-zero coefficients are clustered near the bottom of the image. If you look at the pictures of axes and clouds above, you’ll see that the lower part of the image often contains the handle of the axe. So one interpretation might be that the handle of the axe may be the feature our model is using to make the predictions. In a real data analysis we would want to explore and evaluate that carefully. 3.2.9 Challenge results On the one hand we have completed a “successful” data analysis - we have identified a model that answers our original, specific data science question up to our originally proposed metric for success. But there are also a number of outstanding questions like: Would this work if we had to classify many types of images? Would this level of accuracy hold up in the real world? Did our approach to downsampling produce bias? Reduce accuracy? Is our sample size sufficiently large to have confidence in our answer? Does this analysis even really address our more general question of creating a classifier for a hardware website? Some of these questions are easy to answer, some less so. But a critical component of any data analysis is identifying these issues and including them in the analysis you ultimately deliver. 3.2.10 Synthesize/write up results We have gone through a simple analysis of images from Google QuickDraw. This represents what we would call a “raw” analysis of the data. A raw analysis is performed as a stream of consciousness, trying out each idea as you go without discarding or attempting to synthesize. If this were a real data science project, the next step would be synthesis and interpretation. We will discuss the keys to a successful data analysis later in the course. But a primary purpose of an analysis is to communicate the answer to the question. So our write-up might include an executive summary of our conclusions, remove extraneous and unnecessary processing, include additional explanations for a non-technical audience, clean up figures, and provide a clearer “arc” to the analysis. We might recommend next steps for both analysis and implementation and highlight key strengths or weaknesses of our approach. 3.2.11 Create reproducible code The most common query from a collaborator, manager, or executive when you perform an analysis is: Could you just re-run that code with the [latest/different/best] parameters? Even if you are doing a data science project solely for yourself you should keep in mind that: Your closest collaborator is you six months ago, but you don’t reply to emails Both of these quotes are modified from this excellent lecture by Karl Broman as part of his Tools for Reproducible Research Course. The point of these quotes is that it is an almost certainty that you will need to run your analysis more than once. For that reason alone, it is worth it to build out code that can be used to perform the analysis automatically - saving your future self hours of frustrating investigation. 3.3 Raw, informal, and formal data science In this lecture we have performed a “raw” data analysis. We did not attempt to organize, synthesize, or format our analysis for a particular audience. Every step, piece of code, and plot inthe analysis was included whether it was relevant or not. Raw data analysis is almost always the place to start. As with the parable of the blind men and the elephant, the priority should be placed on a full exploration to identify all of the strengths and weaknesses of both the data and the methods used on the data. An “informal” data analysis takes the first steps toward polishing a data analysis for a broader audience. Common steps between a raw data analysis and an informal data analysis are: An arc of the analysis will be defined Unimportant analyses may be removed Figures may be improved Code may be removed or simplified Conclusions will be more specifically outlined But the analysis might not be subject to the formal structure of a memo, report, or journal article. Some examples of informal data analyses are: http://varianceexplained.org/r/trump-tweets/ https://hilaryparker.com/2013/01/30/hilary-the-most-poisoned-baby-name-in-us-history/ http://alyssafrazee.com/2014/06/04/gender-and-github-code.html A formal data analysis is one that may appear in a formalized report, memo, or journal. Some steps for moving from an informal to a formal data analysis are: An arc of the analysis will be streamlined Unimportant analyses will be removed Supporting analyses will be moved to supporting documentation Figures may be production quality Code will be moved to supporting documentation Methods will be referenced and supported Conclusions will be referenced and supported Moreover, the analysis will conform to a specific set of formatting rules, structure, and style that are specific to the outlet. Each of these types of data science can play an important role in scoping projects and moving them forward - depending on the relative need for speed or completeness. 3.4 Additional Resources Art of Data Science Tools for Reproducible Research Opinion: Reproducible research can still be wrong: Adopting a prevention approach 10 simple rules for structuring papers 3.5 Homework Template Repo: https://github.com/advdatasci/homework2 Repo Name: homework2-ind-yourgithubusername Pull Date: 2020/09/14 9:00AM Baltimore Time "],["week-3.html", "4 Week 3 4.1 Week 3 Learning Objectives 4.2 Organizing a data analysis 4.3 Project Organization 4.4 File naming 4.5 Absolute vs. relative paths 4.6 Coding Variables 4.7 Project management software 4.8 Coding style 4.9 Chain of Custody for Data 4.10 Version Control 4.11 Additional Resources 4.12 Homework", " 4 Week 3 4.1 Week 3 Learning Objectives At the end of this lesson you will be able to: Organize a data analysis Use appropriate file naming Develop and apply a coding style Distinguish raw from processed data Apply the chain of custody model for data 4.2 Organizing a data analysis This week we will focus on organizing a data analysis from high level to low level. An important thing to keep in mind is that this is one system for doing this, it is not the only system for data analysis organization. The key idea here is just to be consistent in everything you do - from folder structure, to file names, to data columns, and more. This will slow you down a bit as you do your analysis! That’s ok! You will more than make up for the time somewhere down the line when you have to repeat or reproduce an analysis. As Karl Broman puts it the key steps are: Step 1: slow down and document. Step 2: have sympathy for your future self. Step 3: have a system 4.2.1 Motivation - the stick This is an advanced data science class. You’d think that the most important parts of advanced data science would focus on complicated methods like deep learning, or managing massive scale data sets. But the reality is that the primary measure of how well or poorly a data analysis goes comes down to organization and management of data, code, and writing. This is going to seem pretty simple for an advanced course, but is remarkably one of the things most commonly left out of courses. If you focus on setting your analysis well up from the start you’ll have a much better time! The classic example of what can go wrong with a poorly organized, documented data analysis is the Duke Saga. In this case, a group of scientists at Duke developed a statistical model that appeared to precisely predict patient response to chemotherapy based on gene expression measurements. This seemed like a huge precision medicine success and resulted in several high profile publications: Unfortunately, it turns out there were a lot of problems with the analysis. Some of them were very obvious problems - things like the analysts used probability incorrectly in their calculations. But many of the most important problems were things like row-names shifting due to Excel and mis-labeled values for variables (things like coding 0 = responded, 1= not responded, when it was actually the reverse). These mistakes were ultimately detailed in a statistical paper: This paper was one of the fastest reviewed in the history of statistics! The reason was that by the time this paper was published, there were already clinical trials ongoing where patients were being assigned the wrong therapy on the basis of this flawed data analysis! How could clinical trials have gotten started so quickly?! The reason is that it actually took almost 3 years (!) for the statisticians reproducing the paper to get all of the code and data together so that they could reverse engineer the problems with the analysis. It turns out there were a lot of problems with this case, from scientific, to cultural, to statistical - that led to the eventual bad clinical trial. This case was so bad that the scientists and administrators involved even faced lawsuits. Obviously most data analyses aren’t this high risk! But the real problems here were due to a lack of organization and sharing of the original research materials for the data analysis that identified the chemotherapy predictors. Had the analysts used an organized data analysis many of these problems could be avoided. If you want a highly entertaining review of the entire Duke Saga by one of the statisticians involved in identifying the errors (and one of the most entertaining statistical speakers out there) you can watch this video of a talk he gave. 4.2.2 Motivation - the carrot When you perform a data analysis the audience is either someone else or you at some time in the future. As we mentioned last week the most common query from a collaborator, manager, or executive when you perform an analysis is: Could you just re-run that code with the [latest/different/best] parameters? So even when you think you are doing an analysis for someone else, your code is actually most often just for you at some point in the future. When you first get started analyzing data you may have one project or two that you are working on. You will know where all the files are and what they all mean. But as you go on in your career, you will accumulate more analyses and each one will have a large number of code, data, and writing files. This isn’t a big problem at first, but later when someone asks you to find, reproduce, or share a piece of analysis from a paper 10 years ago, you’ll be so grateful to your past self for being organized! 4.3 Project Organization At a high level when you are organizing a data analysis project you need to manage data, code, writing, and analysis products (figures, tables, graphics). When I set up a new project I usually use a single folder with a structure like this. README.md .gitignore project.Rproj data/ raw_data/ tidy_data/ codebook.md code/ raw_code/ final_code/ figures/ exploratory_figures/ explanatory_figures/ products/ writing/ 4.3.1 README.md This is maybe the most critical piece of any data analysis that you are working on. You absolutely will forget which files were the original data, which code you used to clean the data with, that one really weird quik that means you have to run clean_data.R before preprocess_data.R and many other small details. If you keep a steady record of these changes as you go it will be much easier to reproduce, share, or manage your analysis. This could save future you hundreds or thousands of hours of repetitive and frustrating work. 4.3.2 .gitignore This file is critical. It tells you which files for Github to ignore when you are pushing to the web. The files that you should make sure you ignore are: Any data files that contain PII or HIPAA protected data Any files that contain access keys or tokens to databases or services Any big data or image files (&gt; 25 MB) It is particularly important that you make sure you don’t push access keys or private data to Github where they can be accessed publicly. Generally when I have data or keys like these I need to protect I create an additional folder project/private that contains these files. I then add _private/*_ to my .gitignore folder so that all the files in this folder are ignored. I push the .gitignore file to Github before I put anything in the private folder. Then, because I’m always paranoid about these things, I generally put a more innocuous file in the private folder and push to test to make sure it is ignored. 4.3.3 data/ 4.3.3.1 The components of a data set The work of converting the data from raw form to directly analyzable form is the first step of any data analysis. It is important to see the raw data, understand the steps in the processing pipeline, and be able to incorporate hidden sources of variability in one’s data analysis. On the other hand, for many data types, the processing steps are well documented and standardized. These are the components of a processed data set: The raw data. A tidy data set. A code book describing each variable and its values in the tidy data set. An explicit and exact recipe you used to go from 1 -&gt; 2,3 4.3.3.2 data/raw_data/ It is critical that you include the rawest form of the data that you have access to. Here are some examples of the raw form of data: The strange binary file your measurement machine spits out The unformatted Excel file with 10 worksheets the company you contracted with sent you The complicated JSON data you got from scraping the Twitter API The hand-entered numbers you collected looking through a microscope You know the raw data is in the right format if you: Ran no software on the data Did not manipulate any of the numbers in the data You did not remove any data from the data set You did not summarize the data in any way If you did any manipulation of the data at all it is not the raw form of the data. Reporting manipulated data as raw data is a very common way to slow down the analysis process, since the analyst will often have to do a forensic study of your data to figure out why the raw data looks weird. 4.3.3.3 data/processed_data The general principles of tidy data are laid out by Hadley Wickham in this paper and this video. The paper and the video are both focused on the R package, which you may or may not know how to use. Regardless the four general principles you should pay attention to are: Each variable you measure should be in one column Each different observation of that variable should be in a different row There should be one table for each “kind” of variable If you have multiple tables, they should include a column in the table that allows them to be linked While these are the hard and fast rules, there are a number of other things that will make your data set much easier to handle. First is to include a row at the top of each data table/spreadsheet that contains full row names. So if you measured age at diagnosis for patients, you would head that column with the name AgeAtDiagnosis instead of something like ADx or another abbreviation that may be hard for another person to understand. Here is an example of how this would work from genomics. Suppose that for 20 people you have collected gene expression measurements with RNA-sequencing. You have also collected demographic and clinical information about the patients including their age, treatment, and diagnosis. You would have one table/spreadsheet that contains the clinical/demographic information. It would have four columns (patient id, age, treatment, diagnosis) and 21 rows (a row with variable names, then one row for every patient). You would also have one spreadsheet for the summarized genomic data. Usually this type of data is summarized at the level of the number of counts per exon. Suppose you have 100,000 exons, then you would have a table/spreadsheet that had 21 rows (a row for gene names, and one row for each patient) and 100,001 columns (one row for patient ids and one row for each data type). If you are sharing your data with the collaborator in Excel, the tidy data should be in one Excel file per table. They should not have multiple worksheets, no macros should be applied to the data, and no columns/cells should be highlighted. Alternatively share the data in a CSV or TAB-delimited text file. 4.3.3.4 data/codebook.md For almost any data set, the measurements you calculate will need to be described in more detail than you will sneak into the spreadsheet. The code book contains this information. At minimum it should contain: Information about the variables (including units!) in the data set not contained in the tidy data Information about the summary choices you made Information about the experimental study design you used In our genomics example, the analyst would want to know what the unit of measurement for each clinical/demographic variable is (age in years, treatment by name/dose, level of diagnosis and how heterogeneous). They would also want to know how you picked the exons you used for summarizing the genomic data (UCSC/Ensembl, etc.). They would also want to know any other information about how you did the data collection/study design. For example, are these the first 20 patients that walked into the clinic? Are they 20 highly selected patients by some characteristic like age? Are they randomized to treatments? A common format for this document is a Word file. There should be a section called “Study design” that has a thorough description of how you collected the data. There is a section called “Code book” that describes each variable and its units. 4.3.3.5 The instruction list/script You may have heard this before, but reproducibility is kind of a big deal in computational science. That means, when you submit your paper, the reviewers and the rest of the world should be able to exactly replicate the analyses from raw data all the way to final results. If you are trying to be efficient, you will likely perform some summarization/data analysis steps before the data can be considered tidy. The ideal thing for you to do when performing summarization is to create a computer script (in R, Python, or something else) that takes the raw data as input and produces the tidy data you are sharing as output. You can try running your script a couple of times and see if the code produces the same output. In many cases, the person who collected the data has incentive to make it tidy for a statistician to speed the process of collaboration. They may not know how to code in a scripting language. In that case, what you should provide the statistician is something called pseudocode. It should look something like: Step 1 - take the raw file, run version 3.1.2 of summarize software with parameters a=1, b=2, c=3 Step 2 - run the software separately for each sample Step 3 - take column three of outputfile.out for each sample and that is the corresponding row in the output data set You should also include information about which system (Mac/Windows/Linux) you used the software on and whether you tried it more than once to confirm it gave the same results. Ideally, you will run this by a fellow student/labmate to confirm that they can obtain the same output file you did. 4.3.4 code/ 4.3.4.1 code/raw_code When you start out doing an analysis recall that you will be doing exploration and data cleaning. Unless you have worked with this type of data before, you will be acting like the blind men and the elephant - just trying to figure out the different aspects of the data set. The important characteristics of this stage of an analysis are speed and comprehensiveness. You will want to make many plots, tables, and summaries of the data. You will generally not be trying to make these clean or organize the results. This is ok! You can put these (often slightly unorganized) R scripts in the code/raw_code folder. While these scripts are generally unorganized two things will save you a lot of time later: Make sure you comment your code, even if it is really fast/simple comments. Especially highlight any important discoveries/plots you think might be useful later with labeled comments. If you find anything really important that you definitely will include in downstream analysis add a note to your README.md file about the nature of the discovery and which file you discovered it in, so you can find it later. 4.3.4.2 code/final_code Your raw code is mostly for you - to help you figure out a data set, start to scope your analysis, and organize your thoughts. Your final code is for other people or future you. Final code should be organized, easy to follow, and reproducible. Final code is like a curation of your original raw code. You shouldn’t keep every plot, figure, summary or table from your raw code in your final code. Only the ones that are important and are part of the story. Some important principles of final code are: Files should be ordered so it is easy to follow the flow of an analysis (see file naming section for more) You should use relative, rather than absolute, paths for all coding within your final code (see section on relative vs absolute paths below) If you randomly sample data, or use methods like k means clustering or random forests that are inherently stochastic, you should set a seed so that calculations can be reproduced exactly. Final code files should either be in R markdown format or heavily commented so it is easy to follow all the steps in ana analysis. 4.3.5 figures We will have an entire week focused on just figures, so for now we will briefly explain these two folders, but more will be coming soon! 4.3.5.1 figures/exploratory_figures Exploratory figures are figures that are made rapidly to determine the structure, quirks, and summaries of the data that will be important for downstream analysis. The important thing about exploratory figures is that you should make a lot of them! Exploratory figures are like raw code, they are mostly for you. You should save any figure you make that you think could be useful for you later. You should still maintain good file naming practices and include comments in your raw code that point out which code produces which exploratory figure. 4.3.5.2 figures/explanatory_figures Explanatory figures, like final code, is for sharing with others. You will generally make many fewer explantory figures than exploratory figures, they will generally be much more polished, and you will want to make sure that they clearly follow the “arc” of the data analysis you are performing. Since this is a folder you will be sharing with others, it is a good idea to include documentation in your README.md explaining what these figures are and which code in the code/final_code folder makes these figures. 4.3.6 products/ This folder is for any complete data analytic products you will be sharing with your collaborators, manager, or audience. For our class, we will be primarily focusing on written data analyses so the main folder we include is products/writing but you may also include folders such as products/presentations, products/shinyapps, or products/dashboards if they are more appropriate for the type of sharing you are performing. 4.3.6.1 products/writing This folder will contain written documents you are sharing with collaborators. They may be Word documents, links to Google Docs, or Latex documents. Sometimes it will be possible for one of your .Rmd files to compile directly to these documents, but it is more common that these will be passed back and forth with collaborators. It is good to separate these writing files from those that you are managing and organizing for the broader data analysis. 4.4 File naming Jenny Bryan, one of the world’s best data scientists says that: File organization and naming are powerful weapons against chaos. If this looks familiar, you may want to update your file naming approach! She has outlined some key principles for naming files. While these principles are especially useful for final code, they are also useful for naming data files, for naming raw code, and for naming figures and writing. The principles are that files should be: Machine readable Human readable Be nicely ordered Machine readable means that file names shouldn’t contain spaces, special characters, and should include important pieces of information about the file contents (sometimes called slugs) separated by underscores or dashes so that it is easy to run computer code to search for specific files. It is often easier if file names are entirely lowercase letters to avoid simple coding errors. Human readable means that files should be labeled with names that make it easy for a person to follow along. So err on the side of long file names rather than abbreviations, numeric only file names, or obscure file names. When you open a folder - especially the final code folder - it is useful to see the files in the order you want them to be run. One way to do this is to name files alpha-numerically so that they will appear in the right order on your computer. One way to do this is to order the files and then append a number to the beginning of each file name (like 01_data_cleaning.R, 02_exploratory_analysis.R etc. so that the files will be ordered correctly). Here are some examples of good and bad names for files - can you figure out why the bad file names are bad? As an example, here are two data files from an analysis I worked on. processed_pvalue_data_from_pubmed_oct24.rData raw_pvalue_data_from_pubmed_oct24.rData The first part of the file name tells you what I did to the data: processed_pvalue_data_from_pubmed_oct24.rData raw_pvalue_data_from_pubmed_oct24.rData the second part of the file name tells you what kind of data it is: processed_pvalue_data_from_pubmed_oct24.rData raw_pvalue_data_from_pubmed_oct24.rData and the last part tells you the date I pulled the data: processed_pvalue_data_from_pubmed_oct24.rData raw_pvalue_data_from_pubmed_oct24.rData These file names are human readable, machine readable (you can sort by processing type, etc.), and they are nicely ordered if you care about the data processing as the main ordering variable. 4.5 Absolute vs. relative paths An important reason you are organizing your data analysis files is that you will often have to share the files with others. One thing to keep in mind is that when you share files with others they are no longer on your computer. This means that if you give your code to someone else, they will need to be able to find the files, data, and code within the folders you have created inside your project. Imagine for example that you have set up your folder structure like this: The reality is that while you will share only this one folder with someone else; it is on your computer somewhere and might be buried deep in your folder structure. The important thing to realize is you have to specify files you call in your analysis so they can be found by others. So if you source a file like this: source('C:\\Documents\\ds_projects\\project\\code\\final_code\\myfile.R') source('/root/analyses/ds_projects/project/code/final_code/myfile.R') Then it won’t work on someone else’s computer since they don’t have that same file structure! To avoid this, you should use relative paths, which specify the file path to a folder relative to the project folder. You can do that with the here package in R. The here package looks for a .Rproj file and points to that directory: The nice thing about the here function finds the base folder regardless of which folder you are in inside the project directory: You can use the here function to define paths in your code to code or data files: So for example if your working directory is /root/analyses/ds_projects/project/ and you run the code: here('code/final_code/myfile.R') it will produce the file path: /root/analyses/ds_projects/project/code/final_code/myfile.R and if you are on a different computer and your working directory is C:\\Documents\\ds_projects\\ and you run the code: here('code/final_code/myfile.R') it will produce the file path: C:\\Documents\\ds_projects\\project\\code\\final_code\\myfile.R In other words, here will produce the complete path to your myfile.R file, relative to the directory where you have stored .Rproj. This means that regardless of where a person puts the folder on their computer, the here function will be able to create the path to the data. The optimal way to take advantage of this package is to use Rstudio projects but you can use the set_here() function to create a .here file that will function as if it was the .Rproj file for targeting the here function. 4.6 Coding Variables When you put variables into a spreadsheet there are several main categories you will run into depending on their data type: Continuous Ordinal Categorical Missing Censored Continuous variables are anything measured on a quantitative scale that could be any fractional number. An example would be something like weight measured in kg. Ordinal data are data that have a fixed, small (&lt; 100) number of levels but are ordered. This could be for example survey responses where the choices are: poor, fair, good. Categorical data are data where there are multiple categories, but they aren’t ordered. One example would be sex: male or female. Missing data are data that are missing and you don’t know the mechanism. You should code missing values as NA. Censored data/make up/ throw away missing observations. In general, try to avoid coding categorical or ordinal variables as numbers. When you enter the value for sex in the tidy data, it should be “male” or “female”. The ordinal values in the data set should be “poor”, “fair”, and “good” not 1, 2 ,3. This will avoid potential mixups about which direction effects go and will help identify coding errors. Always encode every piece of information about your observations using text. For example, if you are storing data in Excel and use a form of colored text or cell background formatting to indicate information about an observation (“red variable entries were observed in experiment 1.”) then this information will not be exported (and will be lost!) when the data is exported as raw text. Every piece of data should be encoded as actual text that can be exported. One very good approach to naming columns in a data analysis are with multiple levels as described in this great blog post by Emily Riederer. For example your variable names might include: A slug for the type of variable (ID for ids, IND for indicators, N for counts, etc.) A slug for subjects being measured (DRIVER for drivers, RIDER for riders etc.) A slug for what is being measured (CITY, ZIPCODE, etc) which turns into data column names like: ID_DRIVER N_TRIP_PASSENGER_ORIGIN_DESTINATION The blog post contains much more information on this approach. 4.7 Project management software The approach we discussed here to managing a project can be done by hand or by copy pasting each time you create a new project. It has some advantages - it is lightweight and focused on organization - so you don’t depend on any outside software. That being said there are some important pieces of software that have been developed for project management in R. These packages can help automate some of the processes that make data project management difficult. drake- is software for automating pipelines in R, it is smart about knowing which file to run in which order and can be really useful if you have a complicated set of processing scripts that must be run in just the right order to produce your final analysis. workflowr - is a package for setting up and automating some of the file organization/folder structure described in this lecture. It also contains workflow management functions and can be combined with drake to make workflows automated. snakemake is more general purpose workflow automation software. If you are doing lots of processing outside of R, this is probably more useful than drake. 4.8 Coding style While it isn’t as critical as some of the other organizing principles we have discussed here, having clean code is really helpful for making it easier for others to follow your analysis. As with many of the other suggestions in this week’s lesson, the key principle here is consistency. There are a variety of different coding style definitions you can go with - if you use a popular framework your code will be more easily readable by individuals from that community. Google’s R Style Guide Tidyverse Style Guide Bioconductor Style Guide 4.9 Chain of Custody for Data “Raw data” is one of those terms that everyone in statistics and data science uses but no one defines. For example, we all agree that we should be able to recreate results in scientific papers from the raw data and the code for that paper. But what do we mean when we say raw data? When working with collaborators or students I often find myself saying - could you just give me the raw data so I can do the normalization or processing myself. To give a concrete example, I work in the analysis of data from high-throughput genomic sequencing experiments. These experiments produce data by breaking up genomic molecules into short fragements of DNA - then reading off parts of those fragments to generate “reads” - usually 100 to 200 letters long per read. But the reads are just puzzle pieces that need to be fit back together and then quantified to produce measurements on DNA variation or gene expression abundances. High throughput sequencing Image from Hector Corrata Bravo’s lecture notes When I say “raw data” when talking to a collaborator I mean the reads that are reported from the sequencing machine. To me that is the rawest form of the data I will look at. But to generate those reads the sequencing machine first (1) created a set of images for each letter in the sequence of reads, (2) measured the color at the spots on that image to get the quantitative measurement of which letter, and (3) calculated which letter was there with a confidence measure. The raw data I ask for only includes the confidence measure and the sequence of letters itself, but ignores the images and the colors extracted from them (steps 1 and 2). So to me the “raw data” is the files of reads. But to the people who produce the machine for sequencing the raw data may be the images or the color data. To my collaborator the raw data may be the quantitative measurements I calculate from the reads. When thinking about this I realized an important characteristics of raw data. Raw data is relative to your reference frame. In other words the raw data is raw to you if you have done no processing, manipulation, coding, or analysis of the data. In other words, the file you received from the person before you is untouched. But it may not be the rawest version of the data. The person who gave you the raw data may have done some computations. They have a different “raw data set”. The implication for reproducibility and replicability is that we need a “chain of custody”. The chain of custody model states that each person must keep a copy of the raw data and a complete record of the operations they performed to arrive at their version of processed data. As long as each person follows these steps, then the data will be As long as each person keeps a copy and record of the “raw data” to them you can trace the provencance of the data back to the original source. 4.10 Version Control Typically when you are working on data analysis files, you will have to update them over and over again. One way to handle this is to save a new file each time like in this comic from PhD Comics This is a ….bad idea. You should instead use a version control system that will keep track of the changes to your files. The unfortunate truth about version control though is that most version control systems can be frustrating: “Version control is a truly vital concept that has unfortunately been implemented by madmen.” Amen. https://t.co/wSI7r9Epm7 — mike cook (@mtrc) July 3, 2015 It is as an assumed pre-requisite for this class that you know how to use a version control system like Github. That being said there are a number of good tutorials out there for using Git/Github both with R and more generally. I highly recommend, for example, that you check out Happy Git and Github for the UseR. 4.11 Additional Resources Organizing Data Science Projects Happy Git and Github for the UseR How to name files Coordinating with collaborators 4.12 Homework Template Repo: https://github.com/advdatasci/homework3 Repo Name: homework3-ind-yourgithubusername Pull Date: 2020/09/21 9:00AM Baltimore Time "],["week-4.html", "5 Week 4 5.1 Week 4 Learning Objectives 5.2 The optimality paradox 5.3 An example of the art of data science 5.4 Early definitions of success 5.5 A theory of data science success 5.6 A successful data analysis 5.7 Constraints and expectation matching 5.8 What about being correct? 5.9 Additional Resources 5.10 Homework", " 5 Week 4 5.1 Week 4 Learning Objectives At the end of this lesson you will be able to: Describe the optimality paradox Define reproducibility and replicability Identify the elements of a data analysis Identify the principles of a data analysis Match elements to principles in a data analysis 5.2 The optimality paradox Over the last several decades statistical and machine learning methods have been steadily improving. It is common to see statements like this in statistical methods papers: At the same time, machine learning methods are also dramatically improving and with larger training sets, clever training algorithms, and sophisticated optimization can now perform tasks - like mastering the game of GO - that were previously thought to be impossible. Google’s AlphaZero even taught itself to beat the best chess programs in the world! So this seems to suggest that we are converging on a point where all data analysis will be a solved problem - optimally performed by machines and always arriving at the correct answer. So I guess maybe not the best time to be learning about data science, right? However, at the same time we see these incredible technical successes, there are also some more troubling trends. For example, you may have heard of the ‘reproducibility crisis’. Or you might have heard the suggestion that most published research findings are false More insidiously you have probably heard of the ways that machine learning and data analysis have been used to encode sexist and racist behavior in algorithms. So maybe the robots haven’t come for all of our data analysis and data science jobs just yet? The optimality paradox at the heart of data science is that data science tools, statistical methods, and machine learning techniques are increasingly optimal and sophisticated, but the data analyses created with those tools, methods and techniques are increasingly suspect. The heart of the optimality paradox is that data analysis, for better or worse, is still as much an art as it is a science. In this week’s lecture we will discuss different dimensions of a successful data analysis - from reproducibility, to communication, to a definition of “correct”. 5.3 An example of the art of data science To illustrate the art of data science I’m going to select a data scientist at random from all the data scientists in the world - my co-instructor Roger Peng. Roger and his co-authors wrote a really important paper on the relationship between PM 2.5 and particulates and hospital admissions. In that paper they describe their main statistical model as follows. “In the first stage, single lag and distributed lag overdispersed Poisson regression models were used for estimating county-specific RRs of hospital admissions associated with ambient levels of PM2.5.” This makes sense as generalized linear models are considered to be one of the most robust statistical tools for measuring statistical associations with a whole book written about their useful and powerful statistical properties. Diving a bit further into the methods we observe the following more detailed description of the statistical choices behind this model (colors added by me). You’ll notice if you read this carefully that each choice coded in orange or blue, was a judgement call made by the authors in writing this paper - from the number of outcomes, to the type of outcomes, to the geographies and context considered. These decisions are likely not backed by tomes of statistical or methodological theory. While I said I was picking on someone, there is absolutely nothing wrong with these choices! In fact, they represent an effort to carefully define the scope of the statistical model and deal with all of the pecularities that come up in any real data set. Every single data analysis requires tens to thousands of small decisions like this, each of which may have an impact on the results. To bring this a little closer to home, here is a stylized and extended version of the data analysis cycle originally proposed by Grolemund and Wickham labeled with the tools and choices I often make when performing data analysis… …and if I’m being perfectly honest these are the reasons that I often make these decisions. You’ll notice that most of the time the reason isn’t some deep scientific or statistical reason, but rather due to convenience, training, or the sub-culture of the field I work in. The result is that any data analysis I produce will be the product of the tools I use with the decisions I make. Producing accurate data analysis will depend critically on both. 5.3.1 An important subtlety - analyst decisions change the question If you haven’t already read the amazing piece by Christie Aschwanden on why Science isn’t Broken you should do so immediately. It does an amazing job of capturing the nuance of statistics as applied to real data sets and how that can be misconstrued as science being “broken” without falling for the easy “everything is wrong” meme. One thing that caught my eye was how the piece highlighted a crowd-sourced data analysis of soccer red cards. They asked dozens of independent groups to analyze the relationship between skin color and red-cards. They got answers that varied quite a bit based on the different modeling choices the analysts made. The figure and underlying data for this figure are fascinating in that they really highlight the human behavioral variation in data analysis and you can even see some data analysis subcultures emerging from the descriptions of how people did the analysis and justified the use or removal of certain covariates. One way to read this figure is that even the best data analysts working on the same data won’t come to the same conclusions. But one subtlety of the figure - and of analyst decisions in data science in general - is that not all of the estimates being reported are measuring the same thing. For example, if some groups adjusted for the country of origin of the referees and some did not, then the estimates for those two groups are measuring different things (the association conditional on country of origin or not, respectively). In this case the estimates may be different, but entirely consistent with each other, since they are just measuring different things. If you ask two people to do the analysis and you only ask them the simple question: Are referees more likely to givered cards to dark skinned players? You may get a different answer based on those two estimates. But the reality is the answers the analysts are reporting are actually to the questions: Are referees more likely to give red cards to dark skinned players holding country of origin fixed? Are referees more likely to give red cards to dark skinned players averaging over country of origin (and everything else)? The subtlety lies in the fact that changes to covariates in the analysis are actually changing the hypothesis you are studying. So in fact the conclusions in that figure may all be entirely consistent after you condition on asking the same question. It is always worth thinking carefully about what covariates you include, what processing you do, and what methods you apply - because they may in fact change the question you are answering! 5.4 Early definitions of success There are a range of potential definitions of a successful data analysis. Motivated by cases like the reproducibility failure in precision medicine and later massive scale replication studies in psychology as well as the claims that most published research are false; the original definitions of a successful data analysis focused on three key terms - reproduciblity, replicability and false discoveries. These terms have different definitions in different fields and so can be a bit hard to pin down. To make our discussion concrete consider the steps in a scientific study. Using these steps we can now define some of the key early ways for defining the success of a data analysis. 5.4.1 Reproducibility We will define reproduciblity as the ability of an independent data analyst to use the data and code from an original analysis and get the same results. Under our simplified model for the scientific steps - this means that a reproducibility study involves keeping every step the same with the exception of the data analyst. In other words, using the original code and data lead to the same numbers as the original analysis. Reproducibility is the easiest of these problems to both define and assess. Assessing reproducibility involves checking the published manuscript, looking for published data and code, then comparing the results of that data and code to the published results. If they are the same the study is reproducible, if they are not, then the study is not. However, reproducible research can still be wrong. For example, the original study behind the precision medicine scandal is now fully reproducible! But the analysis still produces results that do not correctly match patients to the most useful chemotherapies. 5.4.2 Replicability We will define replicability as the ability of an independent scientific team to repeat an experiment, collect, new data, and arrive at equivalent statistical results. Replicability is a more challenging concept to both define and measure. A study replicates if the same experiment can be performed a second time with consistent results. If the data collected during the study are subject to sampling variability then even in the best case scenario the results of a replication will not be identical to the original study. However, we would expect that the results would be within the range of values predicted by the parameter estimates and variability estimates from the original study. The difficulties in assessing replicability are compounded by potential for publication bias, regression to the mean, fragility of scientific results to a particular context, and imperfect replication. There is now a whole field dedicated to estimating replicability of experiments within scientific subfields from psychology, to economics, to cancer biology. This is an important line of research, but isn’t without its difficulties - in particular it is expensive and difficult to replicate research, it is difficult to define what a true replication means, and the authors of these studies may also be motivated to find that studies don’t replicate to increase the impact of their results. The bottom line is that for evaluating any individual data analysis it is rare that you will have the motivation, funding, and ability to fully replicate a study to see if the results hold up. 5.4.3 False discovery A false discovery is the most challenging of these three problems to assess. A false discovery means that the reported parameter or answer to a scientific question is not consistent with the underlying natural truth being studied. A false discovery is the most difficult to assess because we rarely know the true state of nature for any particular scientific study. Single replications are not sufficient to separate true discoveries from false discoveries since both the original study and the replication are subject to sampling error and other potential difficulties with replication studies. Repeated replications or near replications that all point to a similar conclusion are the best way to measure false discoveries in the medical literature. However, repeated replication or near replication of identical hypotheses is very expensive and tends to only occur for highly controversial ideas - such as the claim that vaccines cause autism - which has been repeatedly disproven. 5.5 A theory of data science success As we pointed out in Week 1 it is easier to think of the theory of data analysis more like the theory of music. It provides guidance and opinions on what is “good” versus what is “bad”. An important component of this idea of theory is that it is not prescriptive. It is designed to give some rules that will be useful when considering a new data analysis and evaluating whether it is successful or not. However a huge challenge is that there hasn’t been a formal theory for what is a good data analysis or not. For example, a very successful data scientist, Daryl Pregibon said: “Throughout American or even global industry, there is much advocacy of statistical process control and of understanding processes. Statisticians have a process they espouse but do not know anything about. It is the process of putting together many tiny pieces, the process called data analysis, and is not really understood.” However, to be able to have a conversation around the quality of an individual data analysis without resorting to multiple, expensive, time-consuming replications we need to come up with a framework for guide-posting what is a “successful” and an “unsuccessful” data analysis. The core idea of modern thinking around data analysis is that any data science project has at least two participants - the analyst and the audience. The analyst is the person collecting, organizing, exploring, modeling, and reporting on the data. The audience may be the analysts themselves, or could be a collaborator, manager, or the general public. A successful data analysis is one where the analyst and the audience agree on the most important components of the analysis and the analyst performs those components to the expectations of the audience. One important component of this definition is that it is relative to the audience in question. This is an uncomfortable thing for people who are naturally trained to be quantitative to process at first. But advanced data science is to a very large extent about identifying both the spoken and unspoken goals of your audience and then designing analytical techniques to meet those goals. To make this idea more concrete, we need to identify some key components of a data analysis - the elements and principles of data science. 5.5.1 Elements of Data Science According to Hicks and Peng: The elements of a data analysis are the fundamental components of a data analysis used by the data analyst: code, code comments, data visualization, non-data visualization, narrative text, summary statistics, tables, and statistical models or computational algorithms They include 8 examples of broad categories of data science elements in their paper including narrative text, code, figures, and models. These elements may be combined together into three basic data analytic outputs: Analytic containers - which might be a set of files including a Jupyter notebook or R Markdown document, a dataset, and a set of ancillary code files. The analytic container is essentially the “source code” of the analysis and it is the basis for making modifications to the analysis and for reproducing its findings. Analytic products - which is the executed version of the analysis in the analytic container, containing the executed code producing the results and output that the analyst chooses to include, which might be a PDF document or HTML file. Analytic presentation which might be a slide deck, PDF document, or other presentation format, which is the primary means by which the data analysis is communicated to the audience. 5.5.2 Principles of Data Science According to Hicks and Peng: The principles illustrated by a data analysis are prioritized qualities or characteristics that are relevant to the analysis, as a whole or individual components, and that can be objectively observed or measured. Their presence (or absence) in the analysis is not dependent on the characteristics of the audience viewing the analysis, but rather the relative weight assigned to each principle by the analyst can be highly dependent on the audience’s needs. In addition, the weighting of the principles by the analyst can be influenced by outside constraints or resources, such as time, budget, or access to individuals to ask context-specific questions, that can impose restrictions on the analysis. In their paper they define six key principles of a data analysis. 5.5.3 Data matching Data analyses with high data matching have data readily measured or available to the analyst that directly matches the data needed to investigate a question with data analytic elements 5.5.4 Exhaustive An analysis is exhaustive if specific questions are addressed using multiple, complementary elements 5.5.5 Skeptical An analysis is skeptical if multiple, related questions are considered using the same data 5.5.6 Second order An analysis is second-order if it includes elements that do not directly address the primary question, but give important context or supporting information to the analysis 5.5.7 Transparent Transparent analyses present an element or subset of elements summarizing or visualizing data that are influential in explaining how the underlying data phenomena or datageneration process connects to any key output, results, or conclusions 5.5.8 Reproducible An analysis is reproducible if someone who is not the original analyst can take the published code and data and compute the same results as the original analyst. This is a great start on a set of principles to consider, but is not necessarily exhaustive. One thing to think about is whether there are any principles you think are missing from this list - important general concepts you look for in a data analysis. You may add them to your list of principles when designing your approach. 5.6 A successful data analysis We say that a data analysis is successful if the analyst uses elements that appropriately weight each of the principles to match audience expectations. However, if the elements chosen by the analyst do not appropriately weight the principles according to the audience expectation then the analysis may not be successful. So for example, consider the two analyses of the mortality data post hurricane Maria. In the case of a New England Journal of Medicine paper there are many second-order analyses to consider sub-questions and secondary questions other than the direct impact of Maria on mortality. However, in the case of a blog post there is a more direct single track to the analysis. The NEJM article is also more exhaustive. Each analysis could be considered “successful” if the choices made by the analyst met expectations. In our course discussions, many people mentioned that they enjoyed the blog post more than the paper since they were able to follow the line of reasoning more direclty. In other words, as an audience member, you down-weighted second order analyses. But others suggested that they preferred the NEJM article because they knew exactly where to look for particularly influential figures or tables - in other words they upweighted transparency. When undertaking a new analysis you should consider the principles (either those here, or others you defines for your self) and consider how each component of your analysis contributes to upweight or downweight the principles. Then consider your audience and whether your choices match expectations. 5.7 Constraints and expectation matching One key consideration when using the principle weighting model for data analysis success is that there are often constraints on your analysis. These constraints may be due to your budget, your available time, choices made by people who manage you, or constraints due to the outlet where you plan to communicate. This will necessarily mean that you will always have tradeoffs you will need to consider as a data scientist. For example, it would be ideal if every analysis was fully reproducible down to the compiler level. This may even be a principle that your audience weights very highly. However, that level of reproducibility requires significant investment of time and resources. You will need to carefully consider whether you can afford to spend that time on reproducibility or whether you will need to budget your resources across principles. 5.8 What about being correct? One thing that is tricky about the principle weighting approach to defining data analysis success is that we don’t require an analysis to be “correct” to be successful. This is both due to difficulties in defining what we mean by “correct” and in knowing that the rewards of a data analysis may not always be proportional to measures of accuracy or replicabilitiy. Successful analyses may be different from valid, honest, or complete analyses.For example, in academia it is common for some journals to upweight significance of results over transparency, reproducibility, or exhaustiveness. Honesty and correctness are important in applying your methods and choosing your data. Data analysis often requires the analyst to be brave; precisely because the success of an analysis may be independent of its honesty. The really successful analysts are able to both match audience expectations and uphold honesty and integrity in their analysis. 5.9 Additional Resources Elements and Principles for Characterizing Variation between Data Analyses Evaluating the success of a data analysis Lecture on Elements and Principles Is most published research really false? 5.10 Homework Template Repo: https://github.com/advdatasci/homework4 Repo Name: homework4-ind-yourgithubusername Pull Date: 2020/09/28 9:00AM Baltimore Time "],["week-5.html", "6 Week 5 6.1 Week 5 Learning Objectives 6.2 Skepticism vs. Discovery 6.3 Figures 6.4 Common data analytic fallacies 6.5 Over-skepticism 6.6 Additional Resources 6.7 Homework", " 6 Week 5 6.1 Week 5 Learning Objectives At the end of this lesson you will be able to: Appropriately understand and weight skepticism and discovery and data science Identify and fix key problems with data analytic figures Identify common data analytic mistakes and their characteristics Identify common over-skepticism pitfalls and think carefully about the skepticism/discovery tradeoff 6.2 Skepticism vs. Discovery The first principle is that you must not fool yourself – and you are the easiest person to fool. - Richard Feynman Statisticians often think of themselves as referees - we have the habit of standing just outside of a scientific field and poking holes in the choices that the analysts or scientists made when analyzing their data. This isn’t entirely bad, as skepticism is a key component of performing thoughtful data analysis. But being a data scientist or doing data science involves a more delicate tradeoff. You have both be searching for new discoveries and simultaneously skeptical of what you find. Getting to the right place in the discovery vs. skepticism balance will depend somewhat on the role you are playing - statisticians at the FDA should be more skeptical, while data scientists at new startups made need to focus on discovery. However, all working data scientists should both be aware of the tradeoff and think carefully about where they want to be in terms of balance. If you tilt too far toward skepticism, you will inevitably not be involved in the biggest and most exciting discoveries we can make with data. But if you strive entirely for discovery you will almost certainly fool yourself and others. Perhaps the greatest complement you can pay to a data scientist is that they are thoughtful about the way they consider a data set. This has actually been a huge challenge for the field of statistics - which has traditionally heavily weighted skepticism. In many of the early (and ongoing!) data science initiatives you won’t find many statisticians. This is both bad for data science and bad for statisticians. As Roger put it in his post Statistics and the Science Club (from way back in 2012!): [Data Science] presents an enormous opportunity for statisticians to play a new leadership role in scientific investigations because we have the skills to extract information from the data that no one else has (at least for the moment). But now we have to choose between being “in the club” by leading the science or remaining outside the club to be unbiased arbiters. I think as an individual it’s very difficult to be both simply because there are only 24 hours in the day. It takes an enormous amount of time to learn the scientific background required to lead scientific investigations and this is piled on top of whatever statistical training you receive. However, I think as a field, we desperately need to promote both kinds of people, if only because we are the best people for the job. We need to expand the tent of statistics and include people who are using their statistical training to lead the new science. They may not be publishing papers in the Annals of Statistics or in JASA, but they are statisticians. If we do not move more in this direction, we risk missing out on one of the most exciting developments of our lifetime. Fortunately since Roger wrote this statistics as a field has slowly become more supportive of data science and open to different weights on skepticism vs. discovery. But it still is an important tradeoff that each field must consider. As Rafa Irizarry put it: Few will deny that our current system, with all its flaws, still produces important discoveries. Many of the pessimists’ proposals for reducing false positives seem to be, in one way or another, a call for being more conservative in reporting findings. Example of recommendations include that we require larger effect sizes or smaller p-values, that we correct for the “researcher degrees of freedom”, and that we use Bayesian analyses with pessimistic priors. I tend to agree with many of these recommendations but I have yet to see a specific proposal on exactly how conservative we should be. Note that we could easily bring the false positives all the way down to 0 by simply taking this recommendation to its extreme and stop publishing biomedical research results all together. He goes on to produce this hypothetical ROC curve for the scientific enterprise. An ROC curve plots the rate of true discoveries on the y-axis (in this case, hypothetical true discoveries per decade) versus the number of false discoveries (in this case, hypothetical false discoveries per decade). He highlights two fields - physics and biology. Physics as a more mature, and possibly more well understood field, makes many discoveries even with a strict cutoff for false discoveries. Biomedical sciences is much less well understood and so offers the potential for more discoveries, however, if we are too strict with our approach we may miss many important discoveries in our effort to root out all potential false discoveries. This week we will focus on how to identify potential issues with a data analysis - while we do this we should be aware that though these techniques of skepticism are important, we should also be careful to not over apply them lest we miss the opportunity to find important discoveries. 6.3 Figures We will cover the creation of figures for papers in a separate lecture. But there are a few principles to consider when evaluating a figure in a data analysis. Many of these ideas are borrowed from Karl Broman. There are a variety of reasons to create statistical graphics, infographics, and data visualizations. They depend on the audience and community. However, for data analyses intended to support a scientific claim, there are a few common categories of issues you should look out for. 6.3.1 Just modeling results -&gt; show the data! In many data analytic reports you will find a series of tables and regression modeling results, but no graphical displays of the data whatsoever. It is well known that regression models can obscure important data characteristics. One of the earliest example is Anscombe’s quartet - a set of four data sets that have exactly identical regression coefficients, statistics, and inference, but have wildly different behavior. A more hilarious recent version is the Datasaurus dozen, a set of twelve data sets that all have identical regression summaries (same mean in x, same mean in y, same correlation, same standard deviation, etc) If an analysis produces only statistical summaries and no visualizations of the data, then you should be skeptical that something could be going on “under the hood”. Similarly, when performing your own data analyses you should be sure to show the data. 6.3.2 Dynamite plots -&gt; scatterplots Another very popular type of plot is the “dynamite plot” (shown on the left here in this great paper on data visualization in biology) Dynamite plots are very popular in molecular biology but often appear in other forums as well. As you can see though, the reporting of just a mean and variance can obscure wildly different data distributions. Be wary of dynamite plots in published analyses, especially with small sample sizes. In your own analyses you can use beeswarm plots or boxplots with points overlayed. 6.3.3 Ridiculograms -&gt; clustering diagrams Ridiculograms have been defined as Visually stunning, scientifically worthless, and published on the cover of Science or Nature These graphs take the form of a tangled hairball network diagram like these. It is often better to show more of the data and create clustering diagrams as an alternative to ridiculograms as described in this paper. A good package for creating nicely labeled heatmaps in R is the pheatmap package. 6.3.4 Venn diagrams -&gt; upset plots Venn diagrams are often used in data analyses to represent intersections between groups. However, if the number of intersections starts to grow even in a bit you can end up with some fairly difficult to read and often borderline silly graphs like this. An alternative approach for complicated intersections that is often much easier to read (and less likely to end up bananas) is the UpSet plot which you can make with the unhelpfully capitalized UpSetR package. These plots represent the most common intersections and also can be used to illustrate both marginal and conditional relationships. 6.3.5 Extreme variation in scales -&gt; log scales For many types of measurement devices, data can be measured on a very broad range of scales. Here is an example from two microarrays measuring the same 20,000 genes on two different samples from Karl Broman’s lecture. But what you can’t see in this plot is the data is highly concentrated at smaller scales - with 50 percent of the data below the pink line and 99 percent of the data below the blue line. When you see plots like this that span a very large range with possible outliers, you might dig in deeper to see if a lot of the variation in the data is being obscured by the larger measurements. One solution is to plot the data on the log scale - so the data are more visible. 6.3.6 Correlated measurements -&gt; MA plots It is very common when comparing two measurements of the same thing to plot the measurements on the x and y axis and measure the correlation. But this can be deceiving, especially if the measurements are on very different scales. When you see comparisons of measurements in an x-y scatterplot, you should ask yourself what is the difference between those measurements. An alternative way to show that comparison (using the same example as above) is to plot the difference between the values on the y-axis and the sum of the values on the x-axis. This plot has been discovered multiple times, so is called different things in differnet fields. In medicine it is a Bland-Altman plot and in genomics it is an MA-plot among other names. 6.3.7 Axes cutoff -&gt; plot to zero When reading barplots, it is important to check the y-axis. Often a plot can seem to show big differences - for example the plot on the right. However, when you set the axis all the way to zero it may be clear that the numbers are only separated by a small amount. This is one of the most commonly used techniques for deceiving with graphs. But both ways can be appropriate - it depends on what difference actually matters. The main thing here is to check the y-axis and in your own plots, to point out if you don’t start the axis at zero prominently. 6.3.8 3d graphics -&gt; just don’t 3-D graphs are common in some charting software. They distract, they make comparisons harder. If you see them, you should be skeptical about what is being hidden. In your own practice, just don’t. 6.3.9 Pie charts -&gt; adjacent bar plots The statistician Bill Cleveland did a famous experiment where he showed people two types of graphs - pie charts and line charts. Then he asked them to estimate the size of the dotted portion of the graph. People had much lower error when juding length in side-by-side barplots than in pie charts. This experiment is why statisticians don’t like pie charts! If you see one, especially with many slices - you might want to look for the data in tabular form. If you are doing the analysis yourself put the data in bars. 6.3.10 Difficult comparisons -&gt; put things close on linear axes From Karl’s lecture consider comparing the blue element to the pink element. Which case is easiest? If you see a very complicated comparison asking you to compare small slices of two separate pie charts, be skeptical that you will be able to make that comparison well. In general putting things on a linear scale and close together when making a comparison makes it easier for the reader. 6.3.11 Comparisons hard to make between plots -&gt; use common axes If you see a comparison between two plots in different panels, it is important to check to make sure the axes are on the same scale as it can be really deceiving if they aren’t. library(tidyverse) library(patchwork) dat = tibble(x=rnorm(100,mean=2),y=rnorm(100,mean=25)) p1 = dat %&gt;% ggplot(aes(y=x)) + geom_boxplot() + theme_minimal() p2 = dat %&gt;% ggplot(aes(y=y)) + geom_boxplot() + theme_minimal() pcombined = p1 + p2 pcombined In general if you see comparisons like this where the axes aren’t on the same scale, you might be concerned that the author doesn’t realize the measurements aren’t compararable. In your own analysis, you will want to make sure comparisons across plots have aligned axes and axes on the same scale. pcombined &amp; ylim(-1,28) ## Warning: Removed 1 rows containing non-finite values (stat_boxplot). 6.3.12 Legends hard to follow -&gt; use labels Legends in graphs often make you do two steps at once: (1) figure out what a color or size means and (2) make the comparison between levels of that color or size. For example on the left the color is labeled in a legend. You have to glance up to the legend, then back to the graph to follow what is going on. It is much easier if the labels appear directly on the graph. This is not to say that legends on graphs are bad! But moving toward labels encourages simplicity in the labels and reduces cognitive overhead on the reader. 6.3.13 Confounders aren’t clear -&gt; color by confounder When you are looking at a scatterplot in a paper for a primary comparison or correlation where important confounders have been discussed - always look for a graph where the points have been colored by the values of the confounder. For example here is a plot of the top two principal components of a gene expression data set from Rafa Irizarry’s genomics book. The PCs have been colored by the year the samples were processed. You can see that there is a clear relationship between the PCs and the date the samples were processed - making it an important potential confounder! 6.4 Common data analytic fallacies This section based on the fantastic lecture by Stephanie Hicks and derived from material in Rafa Irizarry’s data science book - this section is licensed CC BY-NC-SA 4.0 based on Rafa’s license. As we learned last week, Skepticism is one of the key principles of data analysis. Perhaps the best known piece of skepticism we teach to statistics students is that: &gt; “Correlation is not causation” In our course we will consider tools useful for quantifying associations between variables. However, we must be careful not to overinterpret these associations. There are many reasons that a variable \\(X\\) can be correlated with a variable \\(Y\\) without either being a cause for the other. Here we examine three common ways that can lead to misinterpreting data. Spurious correlation Outliers Reversing cause and effect Confounders Next, we will discuss in detail what each of these are and give an example. 6.4.1 Spurious correlation First we load some packages we will use library(tidyverse) library(broom) # installed, but not loaded with library(tidyverse) library(dslabs) # needs to be installed library(HistData) # needs to be installed The following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption. the_title &lt;- paste(&quot;Correlation =&quot;, round(with(divorce_margarine, cor(margarine_consumption_per_capita, divorce_rate_maine)),2)) data(divorce_margarine) divorce_margarine %&gt;% ggplot(aes(margarine_consumption_per_capita, divorce_rate_maine)) + geom_point(cex=3) + geom_smooth(method = &quot;lm&quot;) + ggtitle(the_title) + xlab(&quot;Margarine Consumption per Capita (lbs)&quot;) + ylab(&quot;Divorce rate in Maine (per 1000)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Does this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a spurious correlation. You can see many more absurd examples of this wesbsite completely dedicated to spurious correlations. The cases presented in the spurious correlation site are all examples of what is generally called data dredging, data fishing, or data snooping. It’s basically a form of what in the US they call cherry picking. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend. A Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble: N &lt;- 25 G &lt;- 10000 set.seed(1000) sim_data &lt;- tibble(group = rep(1:G, each = N), X = rnorm(N*G), Y = rnorm(N*G)) sim_data ## # A tibble: 250,000 x 3 ## group X Y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.446 -0.312 ## 2 1 -1.21 0.0613 ## 3 1 0.0411 1.17 ## 4 1 0.639 -1.32 ## 5 1 -0.787 1.89 ## 6 1 -0.385 -1.04 ## 7 1 -0.476 0.446 ## 8 1 0.720 0.396 ## 9 1 -0.0185 0.864 ## 10 1 -1.37 1.10 ## # … with 249,990 more rows The first columns denotes group and we simulated of groups each with 25 observations. For each group we generate 25 observations, which are stored in the second and third columns. These are just random independent normally distributed data. So we know, because we constructed the simulation, that \\(X\\) and \\(Y\\) are not correlated. Next, we compute the correlation between X and Y for each group and look at the max: res &lt;- sim_data %&gt;% group_by(group) %&gt;% dplyr::summarize(r = cor(X, Y)) %&gt;% arrange(desc(r)) ## `summarise()` ungrouping output (override with `.groups` argument) res ## # A tibble: 10,000 x 2 ## group r ## &lt;int&gt; &lt;dbl&gt; ## 1 2519 0.641 ## 2 7746 0.620 ## 3 5914 0.598 ## 4 5516 0.589 ## 5 3488 0.582 ## 6 7531 0.581 ## 7 2549 0.581 ## 8 1362 0.573 ## 9 7072 0.573 ## 10 7544 0.571 ## # … with 9,990 more rows We see a correlation of 0.641 and if you just plot the data from that group it shows a convincing plot that \\(X\\) and \\(Y\\) are in fact correlated: sim_data %&gt;% filter(group == res$group[which.max(res$r)]) %&gt;% ggplot(aes(X, Y)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Remember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation: res %&gt;% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = &quot;black&quot;) It is just a mathematical fact that if we observe 10000 random correlations that are expected to be 0 but have a standard error of 0.204, the largest one will be close 1. Note: If we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation: sim_data %&gt;% filter(group == res$group[which.max(res$r)]) %&gt;% do(tidy(lm(Y ~ X, data = .))) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.0923 0.151 0.613 0.546 ## 2 X 0.469 0.117 4.01 0.000548 This particular form of data dredging is referred to as p-hacking. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to adjust for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather to statistical ignorance or wishful thinking. In advanced statistics courses you learn methods to adjust for these multiple comparisons. 6.4.2 Outliers Suppose we take measurements from two independent outcomes, \\(X\\) and \\(Y\\), and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using: set.seed(1) x &lt;- rnorm(100,100,1) y &lt;- rnorm(100,84,1) x[-23] &lt;- scale(x[-23]) y[-23] &lt;- scale(y[-23]) The data look like this: tibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_point(alpha = 0.5) Not surprisingly, the correlation is very high: cor(x,y) ## [1] 0.9881391 But this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, what it should be: cor(x[-23], y[-23]) ## [1] -0.001066464 There is an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called Spearman correlation. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other: tibble(x,y) %&gt;% ggplot(aes(rank(x),rank(y))) + geom_point(alpha = 0.5) The outlier is no longer associated with a very large value and the correlation comes way down: cor(rank(x), rank(y)) ## [1] 0.06583858 Spearman correlation can also be calculated like this: cor(x, y, method = &quot;spearman&quot;) ## [1] 0.06583858 There are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 Peter J. Huber Elvezio M. Ronchetti 6.4.3 Reversing Cause and Effect Another way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around. A form of this claim was actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated. Consider this quote from the article: When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse. A very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school. We can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model: \\[X_i = \\beta_0 + \\beta_1 y_i + \\varepsilon_i, i=1, \\dots, N\\] to the father and son height data, with \\(X_i\\) the father height and \\(y_i\\) the son height, we do get a statistically significant result: library(HistData) data(&quot;GaltonFamilies&quot;) GaltonFamilies %&gt;% filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;% select(father, childHeight) %&gt;% rename(son = childHeight) %&gt;% do(tidy(lm(father ~ son, data = .))) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 34.0 4.57 7.44 4.31e-12 ## 2 son 0.499 0.0648 7.70 9.47e-13 The model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation. 6.4.4 Confounders Confounders are perhaps the most common reason that leads to associations being misinterpreted. If \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if changes in \\(Z\\) cause changes in both \\(X\\) and \\(Y\\). Incorrect interpretation due to confounders is ubiquitous in the lay press. It is sometimes hard to detect. Here we present two examples, both related to gender discrimination. 6.4.4.1 Case Study: UC Berkeley admissions Admission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). Here are the data: data(admissions) admissions ## major gender admitted applicants ## 1 A men 62 825 ## 2 B men 63 560 ## 3 C men 37 325 ## 4 D men 33 417 ## 5 E men 28 191 ## 6 F men 6 373 ## 7 A women 82 108 ## 8 B women 68 25 ## 9 C women 34 593 ## 10 D women 35 375 ## 11 E women 24 393 ## 12 F women 7 341 We see the percent of men and women that were accepted was: admissions %&gt;% group_by(gender) %&gt;% summarize(percentage = round(sum(admitted*applicants)/sum(applicants),1)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## gender percentage ## &lt;chr&gt; &lt;dbl&gt; ## 1 men 44.5 ## 2 women 30.3 A statistical test clearly rejects the hypothesis that gender and admission are independent: dat &lt;- admissions %&gt;% group_by(gender) %&gt;% summarize(total_admitted = round(sum((admitted/100)*applicants)), not_admitted = sum(applicants) - sum(total_admitted)) %&gt;% select(-gender) ## `summarise()` ungrouping output (override with `.groups` argument) dat %&gt;% do(tidy(chisq.test(.))) ## # A tibble: 1 x 4 ## statistic p.value parameter method ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 91.6 1.06e-21 1 Pearson&#39;s Chi-squared test with Yates&#39; continuit… But closer inspection shows a paradoxical result. Here are the percent admissions by major: admissions %&gt;% select(major, gender, admitted) %&gt;% spread(gender, admitted) %&gt;% mutate(women_minus_men = women - men) ## major men women women_minus_men ## 1 A 62 82 20 ## 2 B 63 68 5 ## 3 C 37 34 -3 ## 4 D 33 35 2 ## 5 E 28 24 -4 ## 6 F 6 7 1 Four out of the six majors favor women. More importantly all the differences are much smaller than the 14.2 difference that we see when examining the totals. The paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability. So let’s define three variables: \\(X=1\\) for men and \\(X=0\\) for women \\(Y=1\\) for those admitted and \\(Y=0\\) otherwise \\(Z\\) quantifies how selective is the major A gender bias claim would be based on the fact that \\(\\mbox{Pr}(Y=1 | X = x)\\) is higher for \\(X=1\\) then \\(X=0\\). But \\(Z\\) is an important confounder to consider. Clearly \\(Z\\) is associated with \\(Y\\), as the more selective a major, the lower \\(\\mbox{Pr}(Y=1 | Z = z)\\). But is major selectivity \\(Z\\) associated with gender \\(X\\)? One way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants: admissions %&gt;% group_by(major) %&gt;% summarize(major_selectivity = sum(admitted*applicants)/sum(applicants), percent_women_applicants = sum(applicants*(gender==&quot;women&quot;)/sum(applicants))*100) %&gt;% ggplot(aes(major_selectivity, percent_women_applicants, label = major)) + geom_text() ## `summarise()` ungrouping output (override with `.groups` argument) There seems to be an association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women while less than 10% of the applicants of major B were women. 6.4.4.2 Confounding explained graphically The following plot shows the percent of applicants that were accepted by gender. admissions %&gt;% mutate(percent_admitted = admitted*applicants/sum(applicants)) %&gt;% ggplot(aes(gender, y = percent_admitted, fill = major)) + geom_bar(stat = &quot;identity&quot;, position = &quot;stack&quot;) It also breaks down the acceptance rates by major: the size of the colored bars represents the percent of each major students were admitted to. This breakdown allows us to see that the majority of accepted men come from two majors: A and B. It also lets us see that few women were accepted to these majors. What the plot does not show us is the number of applicants for each major. 6.4.4.3 Average after stratifying In this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away. admissions %&gt;% ggplot(aes(major, admitted, col = gender, size = applicants)) + geom_point() + xlab(&quot;Major&quot;) + ylab(&quot;Admission percentage by major for each gender&quot;) Now we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B. If we average the difference by major we find that the percent is actually 3.5% higher for women. admissions %&gt;% group_by(gender) %&gt;% summarize(average = mean(admitted)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## gender average ## &lt;chr&gt; &lt;dbl&gt; ## 1 men 38.2 ## 2 women 41.7 6.4.5 Simpson’s Paradox The case we have just covered is an example of Simpson’s Paradox. It is called a paradox because we see the sign of the correlation of flip when comparing the entire dataset and specific strata. The following is an illustrative example. Suppose you have three variables \\(X\\), \\(Y\\) and \\(Z\\). Here is a scatterplot of \\(Y\\) versus \\(X\\): N &lt;- 100 Sigma &lt;- matrix(c(1,0.75,0.75, 1), 2, 2)*1.5 means &lt;- list(c(11,3), c(9,5), c(7,7), c(5,9), c(3,11)) dat &lt;- lapply(means, function(mu) MASS::mvrnorm(N, mu, Sigma)) dat &lt;- Reduce(rbind, dat) colnames(dat) &lt;- c(&quot;X&quot;, &quot;Y&quot;) dat &lt;- tbl_df(dat) %&gt;% mutate(Z = as.character(rep(seq_along(means), each = N))) ## Warning: `tbl_df()` is deprecated as of dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. dat %&gt;% ggplot(aes(X,Y)) + geom_point(alpha = .5) + ggtitle(paste(&quot;correlation = &quot;, round(cor(dat$X, dat$Y), 2))) You can see that \\(X\\) and \\(Y\\) are negatively correlated. However, once we stratify by \\(Z\\), shown in different colors below, another pattern emerges. means &lt;- as_tibble(Reduce(rbind, means)) %&gt;% rename(x = V1, y = V2) %&gt;% mutate(z = as.character(seq_along(means))) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. corrs &lt;- dat %&gt;% group_by(Z) %&gt;% summarize(cor = cor(X,Y)) %&gt;% pull(cor) ## `summarise()` ungrouping output (override with `.groups` argument) dat %&gt;% ggplot(aes(X, Y, color = Z)) + geom_point(show.legend = FALSE, alpha = 0.5) + ggtitle(paste(&quot;correlations =&quot;, paste(signif(corrs,2), collapse=&quot; &quot;))) + annotate(&quot;text&quot;, x = means$x, y = means$y, label = paste(&quot;Z=&quot;, means$z), cex = 5) It is really \\(Z\\) that is negatively correlated with \\(X\\). If we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated. 6.4.5.1 Case Study: Research funding success Here we examine a similar case to the UC Berkeley admissions example, but much more subtle. A 2014 PNAS paper analyzed success rates from funding agencies in the Netherlands and concluded that their: results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language used in instructional and evaluation materials. or that gender contributes to personal research funding success in The Netherlands. The main evidence for this conclusion comes down to a comparison of the percentages. Table S1 in the paper includes the information we need: data(&quot;research_funding_rates&quot;) research_funding_rates ## discipline applications_total applications_men applications_women ## 1 Chemical sciences 122 83 39 ## 2 Physical sciences 174 135 39 ## 3 Physics 76 67 9 ## 4 Humanities 396 230 166 ## 5 Technical sciences 251 189 62 ## 6 Interdisciplinary 183 105 78 ## 7 Earth/life sciences 282 156 126 ## 8 Social sciences 834 425 409 ## 9 Medical sciences 505 245 260 ## awards_total awards_men awards_women success_rates_total success_rates_men ## 1 32 22 10 26.2 26.5 ## 2 35 26 9 20.1 19.3 ## 3 20 18 2 26.3 26.9 ## 4 65 33 32 16.4 14.3 ## 5 43 30 13 17.1 15.9 ## 6 29 12 17 15.8 11.4 ## 7 56 38 18 19.9 24.4 ## 8 112 65 47 13.4 15.3 ## 9 75 46 29 14.9 18.8 ## success_rates_women ## 1 25.6 ## 2 23.1 ## 3 22.2 ## 4 19.3 ## 5 21.0 ## 6 21.8 ## 7 14.3 ## 8 11.5 ## 9 11.2 We can construct the two-by-two table used for the conclusion above: two_by_two &lt;- research_funding_rates %&gt;% select(-discipline) %&gt;% summarize_all(sum) %&gt;% summarize(yes_men = awards_men, no_men = applications_men - awards_men, yes_women = awards_women, no_women = applications_women - awards_women) %&gt;% gather %&gt;% separate(key, c(&quot;awarded&quot;, &quot;gender&quot;)) %&gt;% spread(gender, value) two_by_two ## awarded men women ## 1 no 1345 1011 ## 2 yes 290 177 Compute the difference in percentage: two_by_two %&gt;% mutate(men = round(men/sum(men)*100, 1), women = round(women/sum(women)*100, 1)) %&gt;% filter(awarded == &quot;yes&quot;) ## awarded men women ## 1 yes 17.7 14.9 Note: It’s lower for women, and find that it is almost statistically significant at the \\(\\alpha=0.05\\) level: two_by_two %&gt;% select(-awarded) %&gt;% chisq.test() %&gt;% tidy ## # A tibble: 1 x 4 ## statistic p.value parameter method ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 3.81 0.0509 1 Pearson&#39;s Chi-squared test with Yates&#39; continuity… So there appears to be some evidence of an association. But can we infer causation here? Is gender bias causing this observed difference? A response was published a few months later titled No evidence that gender contributes to personal research funding success in The Netherlands: A reaction to Van der Lee and Ellemers which concluded: However, the overall gender effect borders on statistical significance, despite the large sample. Moreover, their conclusion could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines (i.e., with low application success rates for both men and women), then an analysis across all disciplines could incorrectly show “evidence” of gender inequality. In the UC Berkeley admissions example, the overall differences were explained by difference across disciplines. We use the same approach on the research funding data and look at comparisons by discipline: dat &lt;- research_funding_rates %&gt;% rename(success_total = success_rates_total, success_men = success_rates_men, success_women = success_rates_women) %&gt;% gather(key, value, -discipline) %&gt;% separate(key, c(&quot;type&quot;, &quot;gender&quot;)) %&gt;% spread(type, value) %&gt;% filter(gender != &quot;total&quot;) %&gt;% mutate(discipline = reorder(discipline, applications, sum)) dat %&gt;% ggplot(aes(discipline, success, size = applications, color = gender)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + geom_point() Here we see that some fields favor men and other women. We see that the two fields with the largest difference favoring men, are also the fields with the most applications. However, are any of these differences statistically significant? Keep in mind that even when there is no bias, we will see differences due to random variability in the review process as well as random variability across candidates. If we perform a Fisher test in each discipline, we see that most differences result in p-values larger than \\(\\alpha = 0.05\\). do_fisher_test &lt;- function(m, x, n, y){ tab &lt;- tibble(men = c(x, m-x), women = c(y, n-y)) tidy(fisher.test(tab)) %&gt;% rename(odds = estimate) %&gt;% mutate(difference = y/n - x/m) } res &lt;- research_funding_rates %&gt;% group_by(discipline) %&gt;% do(do_fisher_test(.$applications_men, .$awards_men, .$applications_women, .$awards_women)) %&gt;% ungroup() %&gt;% select(discipline, difference, p.value) %&gt;% arrange(difference) res ## # A tibble: 9 x 3 ## discipline difference p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Earth/life sciences -0.101 0.0367 ## 2 Medical sciences -0.0762 0.0175 ## 3 Physics -0.0464 1 ## 4 Social sciences -0.0380 0.127 ## 5 Chemical sciences -0.00865 1 ## 6 Physical sciences 0.0382 0.651 ## 7 Humanities 0.0493 0.217 ## 8 Technical sciences 0.0509 0.437 ## 9 Interdisciplinary 0.104 0.0671 We see that for Earth/Life Sciences, there is a difference of 10% favoring men and this has a p-value of 0.04. But is this a spurious correlation? We performed 9 tests. Reporting only the one case with a p-value less than 0.05 would be cherry picking. The overall average of the difference is only -0.3%, which is much smaller than the standard error: res %&gt;% summarize(overall_avg = mean(difference), se = sd(difference)/sqrt(n())) ## # A tibble: 1 x 2 ## overall_avg se ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.00311 0.0226 Furthermore, note that the differences appear to follow a normal distribution: res %&gt;% ggplot(aes(sample = scale(difference))) + stat_qq() + geom_abline() which suggests the possibility that the observed differences are just due to chance. 6.5 Over-skepticism There is a famous paper called Why most published research findings are false. There are also claims that billions are wasted due to lack of reproducibility and that most psychological studies won’t replicate. These represent very flashy and field defining examples of over skepticism. But there are more subtle forms that often come up when applying statistics within the context of statistics - where sometimes the role of referee is taken too far. 6.5.1 Everything must be causal Recall from the types of questions that we might be interested in whether the manipulation of one variable causes a change in another variable. However, sometimes it is sufficiently interesting to simply see a correlation between variables. If an analysis is not over-interpreted, it is perfectly reasonable to present an association - sometimes the desire for causality will cause people to throw out interesting correlations. 6.5.2 Nothing can be causal Causal inference as a subfield of statistics places extreme weight on skepticism versus discovery. This is often a very good thing. I want the FDA to be skeptical when they are evaluating what medicines I can take and if they might kill me! But there are also cases where observational data, carefully analyzed, can suggest causal relationships. This is tricky to do well in practice, but as a field there is a tendency to criticize any analysis attempting to discover causal relationshps unless is an idealized randomized trial - which hardly ever happens. 6.5.3 You should answer my question, not yours When you read a data analysis there is a question the authors were trying to answer. Sometimes this corresponds to the question that you care about and sometimes it doesn’t. A common tendency is to criticize a data analysis for not answering the question you wanted answered, even if it does a perfectly reasonable job of answering the question they set out to answer. 6.5.4 You used (insert method I don’t use) therefore the analysis is wrong Statistical modeling choices are made both for scientific and well-justified theoretical reasons; they are also made due to personal preference, ease of use, tradition in a field, and many other reasons. It is common to criticize analysts for using a different method than the one you would have used. Sometimes this is justified and the chosen method produces incorrect results, but often it is simply a personal preference and it makes more sense to understand the analysis in the context of the methdods used. 6.6 Additional Resources Karl Broman’s Data Visualization Lecture Rafa Irizarry’s Data Science Book Rafa Irizarry’s Genomic Data Science Book Stephanie Hick’s Lecture on Skepticism in Data Science 6.7 Homework Template Repo: https://github.com/advdatasci/homework5 Repo Name: homework5-ind-yourgithubusername Pull Date: 2020/10/05 9:00AM Baltimore Time "],["week-6.html", "7 Week 6 7.1 Week 6 Learning objectives 7.2 What you wish data looked like 7.3 What it actually looks like 7.4 Background on getting data 7.5 Relative versus absolute paths 7.6 Reading files 7.7 Google Sheets 7.8 Databases 7.9 APIs 7.10 Webscraping 7.11 Google-ing 7.12 Additional Resources 7.13 Homework", " 7 Week 6 7.1 Week 6 Learning objectives At the end of this lesson you will be able to: Define a tidy data set Name the parts of a shareable data set Download data from multiple sources Import data from multiple sources This lecture includes material from Stephanie Hicks lecture on getting data 7.2 What you wish data looked like The first step in almost every data practical data science project is to collect the data that you will need to perform your analysis, clean the data, and explore the key characteristics of the data set. It has been said in many different ways that 80% of data science is data cleaning - and the other 20% is spent complaining about it… In Data Science, 80% of time spent prepare data, 20% of time spent complain about need for prepare data. — Big Data Borat (@BigDataBorat) February 27, 2013 This is unfortunately a truism that hasn’t changed too much despite the best efforts to develop a set of tools that make it easier than ever to collect, manipulate, clean, and explore data. One important reason is that data collection is often done without data analysis in mind. For example, electronic health record data may be initially collected for the purpose of billing, rather than research. Social media data may be collected to allow communication, but will not be structured for analysis. This means that a fair amount of work will be transforming data from whatever format you find it in the wild into data that you can use to do analysis. Depending on the type of software you use, you will ultimately need to format the data in different ways. However, there has been a major effort to standardize a lot of statistical analysis software - particularly in the R programming language - around the idea of “tidy data”. Tidy data was originally defined by Hadley Wickham in a now classic paper. According to Wickham, data is “tidy” if it has the following properties: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. If you have multiple tables, an additional assumption of tidy data that we often use is that: Each table contains a set of unique ids that allow observations to be linked This formalism is extremely useful for a wide range of data types and thanks to significant investment by Rstudio and their extended network, there are now a large number of tools - called the tidyverse - that either facilitate the creation of tidy data or accept tidy data as a default input format. This is often how you wish data would be formatted. But not always! For example, genomic data is often best analyzed using the Bioconductor suite of software that often assumes three (not-tidy by the above definition) data sets: Regardless, the first steps in any data science project after the question and audience have been settled are usually around collecting, reformatting, and exploring data. These steps are hyper critical to the success of a data analysis and - much to the chagrin of statisticians - are often very influential on the ultimate results of a data analysis. So it usually makes sense to consider several alternative processing approaches particularly when dealing with complicated or sensitive data collection technolgoies. 7.3 What it actually looks like So what does data actually look like? As Wickham points out in his Tidy Data paper: tidy datasets are all alike but every messy dataset is messy in its own way. It would take entire courses to cover all the ways messy data exists in the wild and the massive number of tools that have been developed to manipulate these data. Here we will simply give an overview of some of the most widely used/common denominator tools. However, your mileage will vary considerably depending on the field you work in. The most common type of “messy” data that you will encounter, nearly independent of which field you choose to work in, are data in spreadsheets. These are nearly infinite in how ugly they can get. You may also encounter free text files, sometimes called “flat” files, in a variety of formats. For example a health record may be stored as a plain text file (or in one of several proprietary formats): Increasingly, data from the web is stored in JSON format. If you collect data directly from application programming interfaces, this is almost certainly how your data will arrive. Finally, if you work in a subfield with specialized measurement tools you will encounter raw data in a variety of very specific formats. For example, much of the data from high-throughput short-read sequencing appears in the FASTQ format, which may be a 3 gigabyte or more flat or zipped file formatted in a very specific way to report short “reads” and their quality from the genome. Unfortunately, unless you have your own experimental research lab, start your own company, or lead the data team for your organization, you will be stuck with the raw data that arrives. This can be extremely frustrating, but it is important to get good at managing data that is realistically complex if you want to be a practicing data scientist. If you need to vent, you can always check out the hashtag #otherpeoplesdata on Twitter for commiseration and humor. squints at the files I was sent #otherpeoplesdata pic.twitter.com/cB3PC6Y0Yk — Dave Hemprich-Bennett (@hammerheadbat) November 11, 2017 7.4 Background on getting data Regardless of how the data is formatted, the first step in your data analysis will be to collect the data, organize it, read it into R and format it so that you can perform your analysis. We are going to cover some of the most common ways of getting data here, and this will necessarily be pretty R specific, but this is the step that is most likely to be field specific so you may need to follow tutorials within your field. A good place to start is always “file format extension rstats package” on Google. 7.4.1 Where do data live? Data lives anywhere and everywhere. Data might be stored simply in a .csv or .txt file. Data might be stored in an Excel or Google Spreadsheet. Data might be stored in large databases that require users to write special functions to interact with to extract the data they are interested in. For example, you may have heard of the terms mySQL or MongoDB. From Wikipedia, MySQL is defined as an open-source relational database management system (RDBMS). Its name is a combination of “My”, the name of co-founder Michael Widenius’s daughter,[7] and “SQL”, the abbreviation for Structured Query Language.. From Wikipeda, MongoDB is defined as “a free and open-source cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with schemata.” So after reading that, we get the sense that there are multiple ways large databases can be structured, data can be formatted and interacted with. In addition, we see that database programs (e.g. MySQL and MongoDB) can also interact with each other. We will learn more about SQL and JSON in a bit. 7.4.2 Best practices on sharing data When you are getting data, you should be thinking about how you will organize it, both for your self and for sharing with others. We wrote a paper called: How to share data for collaboration where we provide some guidelines for sharing data: We highlight the need to provide raw data to the statistician, the importance of consistent formatting, and the necessity of including all essential experimental information and pre-processing steps carried out to the statistician. With these guidelines we hope to avoid errors and delays in data analysis. the importance of consistent formatting, and the necessity of including all essential experimental information and pre-processing steps carried out to the statistician. The easiest data analyses start with data shared in this format and you will make a lot of friends among your collaborators if you organize data collect in this way. Specifically: The raw data (or the rawest form of the data to which you have access) Should not have modified, removed or summarized any data; Ran no software on data e.g. strange binary file your measurement machine spits out e.g. complicated JSON file you scrapped from Twitter Application Programming Interfaces (API) e.g. hand-entered numbers you collected looking through a microscope A clean data set This may or may not be transforming data into a tidy dataset, but possibly yes A code book describing each variable and its values in the clean or tidy data set. More detailed information about the measurements in the data set (e.g. units, experimental design, summary choices made) Doesn’t quite fit into the column names in the spreadsheet Often reported in a .md, .txt or Word file. An explicit and exact recipe you used to go from 1 -&gt; 2,3 7.5 Relative versus absolute paths When you are starting a data analysis, you have already learned about the use of .Rproj files. When you open up a .Rproj file, RStudio changes the path (location on your computer) to the .Rproj location. After opening up a .Rproj file, you can test this by getwd() When you open up someone else’s R code or analysis, you might also see the setwd() function being used which explicitly tells R to change the absolute path or absolute location of which directory to move into. For example, say I want to clone a GitHub repo from Roger, which has 100 R script files, and in every one of those files at the top is: setwd(&quot;C:\\Users\\Roger\\path\\only\\that\\Roger\\has&quot;) The problem is, if I want to use his code, I will need to go and hand-edit every single one of those paths (C:\\Users\\Roger\\path\\only\\that\\Roger\\has) to the path that I want to use on my computer or wherever I saved the folder on my computer (e.g. /Users/Stephanie/Documents/path/only/I/have). This is an unsustainable practice. I can go in and manually edit the path, but this assumes I know how to set a working directory. Not everyone does. So instead of absolute paths: setwd(&quot;/Users/jtleek/data&quot;) setwd(&quot;~/Desktop/files/data&quot;) setwd(&quot;C:\\\\Users\\\\Andrew\\\\Downloads&quot;) A better idea is to use relative paths: setwd(&quot;../data&quot;) setwd(&quot;../files&quot;) setwd(&quot;..\\tmp&quot;) Within R, an even better idea is to use the here R package will recognize the top-level directory of a Git repo and supports building all paths relative to that. For more on project-oriented workflow suggestions, read this post from Jenny Bryan. 7.5.1 The here package In her post, she writes “I suggest organizing each data analysis into a project: a folder on your computer that holds all the files relevant to that particular piece of work.” Instead of using setwd() at the top your .R or .Rmd file, she suggests: Organize each logical project into a folder on your computer. Make sure the top-level folder advertises itself as such. This can be as simple as having an empty file named .here. Or, if you use RStudio and/or Git, those both leave characteristic files behind that will get the job done. Use the here() function from the here package to build the path when you read or write a file. Create paths relative to the top-level directory. Whenever you work on this project, launch the R process from the project’s top-level directory. If you launch R from the shell, cd to the correct folder first. Let’s test this out. We can use getwd() to see our current working directory path and the files available using list.file() getwd() ## [1] &quot;/Users/rdpeng/projects/ads2020&quot; list.files() ## [1] &quot;_book&quot; &quot;_bookdown_files&quot; &quot;_bookdown.yml&quot; ## [4] &quot;_output.yml&quot; &quot;01-week.md&quot; &quot;01-week.Rmd&quot; ## [7] &quot;01-week.utf8.md&quot; &quot;02-week_cache&quot; &quot;02-week_files&quot; ## [10] &quot;02-week.md&quot; &quot;02-week.Rmd&quot; &quot;02-week.utf8.md&quot; ## [13] &quot;03-week.md&quot; &quot;03-week.Rmd&quot; &quot;03-week.utf8.md&quot; ## [16] &quot;04-week.md&quot; &quot;04-week.Rmd&quot; &quot;04-week.utf8.md&quot; ## [19] &quot;05-week_files&quot; &quot;05-week.md&quot; &quot;05-week.Rmd&quot; ## [22] &quot;05-week.utf8.md&quot; &quot;06-week_files&quot; &quot;06-week.Rmd&quot; ## [25] &quot;07-week.Rmd&quot; &quot;08-week_cache&quot; &quot;08-week_files&quot; ## [28] &quot;08-week.Rmd&quot; &quot;09-week.Rmd&quot; &quot;10-week_files&quot; ## [31] &quot;10-week.Rmd&quot; &quot;11-week.Rmd&quot; &quot;12-week.Rmd&quot; ## [34] &quot;13-week.Rmd&quot; &quot;ads2020.rds&quot; &quot;ads2020.Rproj&quot; ## [37] &quot;basicsystem.png&quot; &quot;data&quot; &quot;docs&quot; ## [40] &quot;fyi.png&quot; &quot;index.md&quot; &quot;index.Rmd&quot; ## [43] &quot;index.utf8.md&quot; &quot;jhsph ads.png&quot; &quot;key.png&quot; ## [46] &quot;Lightbulb.png&quot; &quot;Live-code.png&quot; &quot;logo&quot; ## [49] &quot;modelSLR.png&quot; &quot;README.md&quot; &quot;render90e23fe240a6.rds&quot; ## [52] &quot;style.css&quot; &quot;test.png&quot; &quot;tmpstuff.Rmd&quot; ## [55] &quot;Your-turn.png&quot; OK so our current location is in the /cloud/project directory. Using the here package we can see that here points to this base directory. library(here) ## here() starts at /Users/rdpeng/projects/ads2020 here() ## [1] &quot;/Users/rdpeng/projects/ads2020&quot; list.files(here::here()) ## [1] &quot;_book&quot; &quot;_bookdown_files&quot; &quot;_bookdown.yml&quot; ## [4] &quot;_output.yml&quot; &quot;01-week.md&quot; &quot;01-week.Rmd&quot; ## [7] &quot;01-week.utf8.md&quot; &quot;02-week_cache&quot; &quot;02-week_files&quot; ## [10] &quot;02-week.md&quot; &quot;02-week.Rmd&quot; &quot;02-week.utf8.md&quot; ## [13] &quot;03-week.md&quot; &quot;03-week.Rmd&quot; &quot;03-week.utf8.md&quot; ## [16] &quot;04-week.md&quot; &quot;04-week.Rmd&quot; &quot;04-week.utf8.md&quot; ## [19] &quot;05-week_files&quot; &quot;05-week.md&quot; &quot;05-week.Rmd&quot; ## [22] &quot;05-week.utf8.md&quot; &quot;06-week_files&quot; &quot;06-week.Rmd&quot; ## [25] &quot;07-week.Rmd&quot; &quot;08-week_cache&quot; &quot;08-week_files&quot; ## [28] &quot;08-week.Rmd&quot; &quot;09-week.Rmd&quot; &quot;10-week_files&quot; ## [31] &quot;10-week.Rmd&quot; &quot;11-week.Rmd&quot; &quot;12-week.Rmd&quot; ## [34] &quot;13-week.Rmd&quot; &quot;ads2020.rds&quot; &quot;ads2020.Rproj&quot; ## [37] &quot;basicsystem.png&quot; &quot;data&quot; &quot;docs&quot; ## [40] &quot;fyi.png&quot; &quot;index.md&quot; &quot;index.Rmd&quot; ## [43] &quot;index.utf8.md&quot; &quot;jhsph ads.png&quot; &quot;key.png&quot; ## [46] &quot;Lightbulb.png&quot; &quot;Live-code.png&quot; &quot;logo&quot; ## [49] &quot;modelSLR.png&quot; &quot;README.md&quot; &quot;render90e23fe240a6.rds&quot; ## [52] &quot;style.css&quot; &quot;test.png&quot; &quot;tmpstuff.Rmd&quot; ## [55] &quot;Your-turn.png&quot; We can now create a data folder if it doesn’t already exist and see how to create a link to the data directory using the here package: if(!file.exists(&quot;data&quot;)){dir.create(&quot;data&quot;)} list.files(here(&quot;data&quot;)) ## [1] &quot;2020-10-05-cameras.csv&quot; &quot;2020-10-05-cameras.xlsx&quot; ## [3] &quot;2020-10-11-cameras.csv&quot; &quot;2020-10-12-cameras.csv&quot; ## [5] &quot;2020-10-21-cameras.csv&quot; &quot;2020-10-26-cameras.csv&quot; ## [7] &quot;2020-11-02-cameras.csv&quot; &quot;2020-11-09-cameras.csv&quot; ## [9] &quot;2020-11-15-cameras.csv&quot; &quot;2020-11-16-cameras.csv&quot; ## [11] &quot;2020-11-27-cameras.csv&quot; &quot;2020-11-29-cameras.csv&quot; ## [13] &quot;cameras.csv&quot; &quot;Chinook.sqlite&quot; ## [15] &quot;repos.json&quot; Now we see that using the here::here() function is a relative path (relative to the .Rproj file in our home directory. We also see there is a cameras.csv file in the data folder. Let’s read it into R with the readr package. df &lt;- readr::read_csv(here(&quot;data&quot;, &quot;cameras.csv&quot;)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## address = col_character(), ## direction = col_character(), ## street = col_character(), ## crossStreet = col_character(), ## intersection = col_character(), ## `Location 1` = col_character(), ## `2010 Census Neighborhoods` = col_double(), ## `2010 Census Wards Precincts` = col_double(), ## `Zip Codes` = col_double() ## ) df ## # A tibble: 80 x 9 ## address direction street crossStreet intersection `Location 1` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 GARRIS… E/B &quot;Garr… Wabash Ave &quot;Garrison \\… (39.341209,… ## 2 HILLEN… W/B &quot;Hill… Forrest St &quot;Hillen \\n … (39.29686, … ## 3 EDMOND… E/B &quot;Edmo… Woodbridge… &quot;Edmonson\\n… (39.293453,… ## 4 YORK R… S/B &quot;York… Gitting Ave &quot;York Rd \\n… (39.370493,… ## 5 RUSSEL… S/B &quot;Russ… Hamburg St &quot;Russell\\n … (39.279819,… ## 6 S MART… S/B &quot;MLK … Pratt St &quot;MLK Jr. Bl… (39.286027,… ## 7 ORLEAN… E/B &quot;Orle… Linwood Ave &quot;Orleans … (39.295866,… ## 8 E NORT… W/B &quot;Nort… Springlake… &quot;Northern P… (39.364311,… ## 9 W COLD… E/B &quot;Cold… Roland Ave &quot;Cold Sprin… (39.343906,… ## 10 E NORT… W/B &quot;Nort… York Road &quot;Northern P… (39.365146,… ## # … with 70 more rows, and 3 more variables: `2010 Census Neighborhoods` &lt;dbl&gt;, ## # `2010 Census Wards Precincts` &lt;dbl&gt;, `Zip Codes` &lt;dbl&gt; We can also ask for the full paths for specific files here(&quot;data&quot;, &quot;cameras.csv&quot;) ## [1] &quot;/Users/rdpeng/projects/ads2020/data/cameras.csv&quot; The nice thing about the here package is that the above code creates the “correct” path relative to the home directory, regardless of whether the folder is on your computer or not. 7.5.2 Finding and creating files locally If you want to download a file, one way to use the file.exists(), dir.create() and list.files() functions. file.exists(here(\"my\", \"relative\", \"path\")) = logical test if the file exists dir.create(here(\"my\", \"relative\", \"path\")) = create a folder list.files(here(\"my\", \"relative\", \"path\")) = list contents of folder if(!file.exists(here(&quot;my&quot;, &quot;relative&quot;, &quot;path&quot;))){ dir.create(here(&quot;my&quot;, &quot;relative&quot;, &quot;path&quot;)) } list.files(here(&quot;my&quot;, &quot;relative&quot;, &quot;path&quot;)) 7.5.3 Downloading files Let’s say we wanted to find out where are all the Fixed Speed Cameras in Baltimore? To do this, we can use the Open Baltimore API which has information on the locations of fixed speed cameras in Baltimore. In case you aren’t familiar with fixed speed cameras, the website states: Motorists who drive aggressively and exceed the posted speed limit by at least 12 miles per hour will receive $40 citations in the mail. These citations are not reported to insurance companies and no license points are assigned. Notification signs will be placed at all speed enforcement locations so that motorists will be aware that they are approaching a speed check zone. The goal of the program is to make the streets of Baltimore safer for everyone by changing aggressive driving behavior. In addition to the eight portable speed enforcement units, the city has retrofitted 50 red light camera locations with the automated speed enforcement technology. When we go to the website, we see that the data can be provided to us as a .csv file. To download in this data, we can do the following: file_url &lt;- paste0(&quot;https://data.baltimorecity.gov/api/&quot;, &quot;views/dz54-2aru/rows.csv?accessType=DOWNLOAD&quot;) download.file(file_url, destfile=here(&quot;data&quot;, &quot;cameras.csv&quot;)) list.files(here(&quot;data&quot;)) Alternatively, if we want to only download the file once each time we knit our reproducible report or homework or project, we can us wrap the code above into a !file.exists() function. filename = paste0(Sys.Date(),&quot;-cameras.csv&quot;) if(!file.exists(here(&quot;data&quot;, filename))){ file_url &lt;- paste0(&quot;https://data.baltimorecity.gov/api/&quot;, &quot;views/dz54-2aru/rows.csv?accessType=DOWNLOAD&quot;) todays_date = Sys.Date() download.file(file_url, destfile=here(&quot;data&quot;,filename)) } date_downloaded = Sys.Date() date_downloaded ## [1] &quot;2020-11-29&quot; list.files(here(&quot;data&quot;)) ## [1] &quot;2020-10-05-cameras.csv&quot; &quot;2020-10-05-cameras.xlsx&quot; ## [3] &quot;2020-10-11-cameras.csv&quot; &quot;2020-10-12-cameras.csv&quot; ## [5] &quot;2020-10-21-cameras.csv&quot; &quot;2020-10-26-cameras.csv&quot; ## [7] &quot;2020-11-02-cameras.csv&quot; &quot;2020-11-09-cameras.csv&quot; ## [9] &quot;2020-11-15-cameras.csv&quot; &quot;2020-11-16-cameras.csv&quot; ## [11] &quot;2020-11-27-cameras.csv&quot; &quot;2020-11-29-cameras.csv&quot; ## [13] &quot;cameras.csv&quot; &quot;Chinook.sqlite&quot; ## [15] &quot;repos.json&quot; Here you will notice I also named the file with the date and/or saved another variable with the downloaded date. The reason is that if you are downloading data directly from the internet, it is likely to update and your results may change. It is a good idea to keep track of the data each time you download. Always remember to save the date when you download a file from the internet - usually by naming the file with the date. This can prevent reproducibility errors later if the data are updated between when you collect the data and when you 7.6 Reading files Once you have downloaded a file from the internet the next step is reading the data in so you can explore it. In R there are a number of packages that have been developed for most common data types. We will go over a few of them here. 7.6.1 Reading in CSV files The easiest type of files to read in R are delimited files (for example comma separated values, csv; or tab separated values, tsv). The cameras file we downloaded is an example of a comma separated file. We can read cameras.csv like we have already learned how to do using the readr::read_csv() function: cameras &lt;- readr::read_csv(here(&quot;data&quot;, &quot;cameras.csv&quot;)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## address = col_character(), ## direction = col_character(), ## street = col_character(), ## crossStreet = col_character(), ## intersection = col_character(), ## `Location 1` = col_character(), ## `2010 Census Neighborhoods` = col_double(), ## `2010 Census Wards Precincts` = col_double(), ## `Zip Codes` = col_double() ## ) cameras ## # A tibble: 80 x 9 ## address direction street crossStreet intersection `Location 1` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 GARRIS… E/B &quot;Garr… Wabash Ave &quot;Garrison \\… (39.341209,… ## 2 HILLEN… W/B &quot;Hill… Forrest St &quot;Hillen \\n … (39.29686, … ## 3 EDMOND… E/B &quot;Edmo… Woodbridge… &quot;Edmonson\\n… (39.293453,… ## 4 YORK R… S/B &quot;York… Gitting Ave &quot;York Rd \\n… (39.370493,… ## 5 RUSSEL… S/B &quot;Russ… Hamburg St &quot;Russell\\n … (39.279819,… ## 6 S MART… S/B &quot;MLK … Pratt St &quot;MLK Jr. Bl… (39.286027,… ## 7 ORLEAN… E/B &quot;Orle… Linwood Ave &quot;Orleans … (39.295866,… ## 8 E NORT… W/B &quot;Nort… Springlake… &quot;Northern P… (39.364311,… ## 9 W COLD… E/B &quot;Cold… Roland Ave &quot;Cold Sprin… (39.343906,… ## 10 E NORT… W/B &quot;Nort… York Road &quot;Northern P… (39.365146,… ## # … with 70 more rows, and 3 more variables: `2010 Census Neighborhoods` &lt;dbl&gt;, ## # `2010 Census Wards Precincts` &lt;dbl&gt;, `Zip Codes` &lt;dbl&gt; A couple of important things to check for with these type of “flat” files are: What are the indicators of NA - is it NA? NULL? a space? 99999 (gasp!)? Are there any ill-formatted fields, where a whole row accidentally gets read into one cell? For text fields, are there any strings that should be factors (or vice-versa)? 7.6.2 Reading in Excel files In an ideal world everyone would read the outstanding paper on Data Organization in Spreadsheets by Broman and Woo - or at the least their abstract where they lay out the most important formatting rules! The basic principles are: be consistent, write dates like YYYY-MM-DD, do not leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, do not include calculations in the raw data files, do not use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text files. Unfortunately this is rarely the case and spreadsheets are deceptively difficult to import into R when they have formulae, colored fields, hidden sheets and other things. We can download the cameras data in Excel format and read it with the readxl package: library(readxl) sheets = readxl::excel_sheets(here::here(&quot;data&quot;,&quot;2020-10-05-cameras.xlsx&quot;)) sheets ## [1] &quot;2020-10-05-cameras&quot; cameras &lt;- readxl::read_excel(here::here(&quot;data&quot;,&quot;2020-10-05-cameras.xlsx&quot;),sheet=sheets[1]) cameras ## # A tibble: 80 x 9 ## address direction street crossStreet intersection `Location 1` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 GARRIS… E/B &quot;Garr… Wabash Ave &quot;Garrison \\… (39.341209,… ## 2 HILLEN… W/B &quot;Hill… Forrest St &quot;Hillen \\n … (39.29686, … ## 3 EDMOND… E/B &quot;Edmo… Woodbridge… &quot;Edmonson\\n… (39.293453,… ## 4 YORK R… S/B &quot;York… Gitting Ave &quot;York Rd \\n… (39.370493,… ## 5 RUSSEL… S/B &quot;Russ… Hamburg St &quot;Russell\\n … (39.279819,… ## 6 S MART… S/B &quot;MLK … Pratt St &quot;MLK Jr. Bl… (39.286027,… ## 7 ORLEAN… E/B &quot;Orle… Linwood Ave &quot;Orleans … (39.295866,… ## 8 E NORT… W/B &quot;Nort… Springlake… &quot;Northern P… (39.364311,… ## 9 W COLD… E/B &quot;Cold… Roland Ave &quot;Cold Sprin… (39.343906,… ## 10 E NORT… W/B &quot;Nort… York Road &quot;Northern P… (39.365146,… ## # … with 70 more rows, and 3 more variables: `2010 Census Neighborhoods` &lt;dbl&gt;, ## # `2010 Census Wards Precincts` &lt;dbl&gt;, `Zip Codes` &lt;dbl&gt; However, in practice you might need to look out for: Values that are colored - you may need to use something like tidyxl Values that appear in only a subset of the spreadsheet - you will need to set sell ranges Hidden sheets - you will want to use the excel_sheets function to check for sheet names before you read. Formulae - you may need to use tidyxl to discover what they are Hidden/calculated values - you may again need to use tidyxl In practice, I have seen the code to tidy a single Excel file run into thousands of lines of R code. 7.6.3 Reading in JSON Files JSON (or JavaScript Object Notation) is a file format that stores information in human-readable, organized, logical, easy-to-access manner. For example, here is what a JSON file looks like: var stephanie = { &quot;age&quot; : &quot;33&quot;, &quot;hometown&quot; : &quot;Baltimore, MD&quot;, &quot;gender&quot; : &quot;female&quot;, &quot;cars&quot; : { &quot;car1&quot; : &quot;Hyundai Elantra&quot;, &quot;car2&quot; : &quot;Toyota Rav4&quot;, &quot;car3&quot; : &quot;Honda CR-V&quot; } } Some features about JSON object: JSON objects are surrounded by curly braces {} JSON objects are written in key/value pairs Keys must be strings, and values must be a valid JSON data type (string, number, object, array, boolean) Keys and values are separated by a colon Each key/value pair is separated by a comma 7.6.4 Using GitHub API Let’s say we want to use the GitHub API to find out how many of my GitHub repositories have open issues? (we will learn more about using APIs in a minute) We will use the jsonlite R package and the fromJSON() function to convert from a JSON object to a data frame. We will read in a JSON file located at https://api.github.com/users/jtleek/repos github_url = &quot;https://api.github.com/users/jtleek/repos&quot; library(jsonlite) jsonData &lt;- fromJSON(github_url) The function fromJSON() has now converted the JSON file into a data frame with the names: names(jsonData) ## [1] &quot;id&quot; &quot;node_id&quot; &quot;name&quot; ## [4] &quot;full_name&quot; &quot;private&quot; &quot;owner&quot; ## [7] &quot;html_url&quot; &quot;description&quot; &quot;fork&quot; ## [10] &quot;url&quot; &quot;forks_url&quot; &quot;keys_url&quot; ## [13] &quot;collaborators_url&quot; &quot;teams_url&quot; &quot;hooks_url&quot; ## [16] &quot;issue_events_url&quot; &quot;events_url&quot; &quot;assignees_url&quot; ## [19] &quot;branches_url&quot; &quot;tags_url&quot; &quot;blobs_url&quot; ## [22] &quot;git_tags_url&quot; &quot;git_refs_url&quot; &quot;trees_url&quot; ## [25] &quot;statuses_url&quot; &quot;languages_url&quot; &quot;stargazers_url&quot; ## [28] &quot;contributors_url&quot; &quot;subscribers_url&quot; &quot;subscription_url&quot; ## [31] &quot;commits_url&quot; &quot;git_commits_url&quot; &quot;comments_url&quot; ## [34] &quot;issue_comment_url&quot; &quot;contents_url&quot; &quot;compare_url&quot; ## [37] &quot;merges_url&quot; &quot;archive_url&quot; &quot;downloads_url&quot; ## [40] &quot;issues_url&quot; &quot;pulls_url&quot; &quot;milestones_url&quot; ## [43] &quot;notifications_url&quot; &quot;labels_url&quot; &quot;releases_url&quot; ## [46] &quot;deployments_url&quot; &quot;created_at&quot; &quot;updated_at&quot; ## [49] &quot;pushed_at&quot; &quot;git_url&quot; &quot;ssh_url&quot; ## [52] &quot;clone_url&quot; &quot;svn_url&quot; &quot;homepage&quot; ## [55] &quot;size&quot; &quot;stargazers_count&quot; &quot;watchers_count&quot; ## [58] &quot;language&quot; &quot;has_issues&quot; &quot;has_projects&quot; ## [61] &quot;has_downloads&quot; &quot;has_wiki&quot; &quot;has_pages&quot; ## [64] &quot;forks_count&quot; &quot;mirror_url&quot; &quot;archived&quot; ## [67] &quot;disabled&quot; &quot;open_issues_count&quot; &quot;license&quot; ## [70] &quot;forks&quot; &quot;open_issues&quot; &quot;watchers&quot; ## [73] &quot;default_branch&quot; How many are private repos? How many have forks? table(jsonData$private) ## ## FALSE ## 30 table(jsonData$forks) ## ## 0 1 2 3 5 6 7 8 9 11 23 ## 7 4 2 3 2 1 1 1 1 1 1 ## 25 41 61 126 713 228195 ## 1 1 1 1 1 1 What’s the most popular language? table(jsonData$language) ## ## C++ CSS HTML JavaScript R TeX ## 1 1 9 2 6 1 To find out how many repos that I have with open issues, we can just create a table: # how many repos have open issues? table(jsonData$open_issues_count) ## ## 0 1 2 5 6 731 ## 22 4 1 1 1 1 Whew! Not as many as I thought. How many do you have? One important thing to note about data read in JSON format is that it is often “nested”. The way that R handles this is by forcing an entire data frame into a column! class(jsonData$owner) ## [1] &quot;data.frame&quot; jsonData$owner ## login id node_id ## 1 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 2 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 3 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 4 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 5 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 6 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 7 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 8 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 9 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 10 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 11 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 12 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 13 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 14 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 15 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 16 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 17 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 18 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 19 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 20 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 21 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 22 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 23 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 24 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 25 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 26 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 27 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 28 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 29 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## 30 jtleek 1571674 MDQ6VXNlcjE1NzE2NzQ= ## avatar_url gravatar_id ## 1 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 2 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 3 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 4 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 5 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 6 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 7 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 8 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 9 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 10 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 11 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 12 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 13 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 14 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 15 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 16 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 17 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 18 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 19 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 20 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 21 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 22 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 23 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 24 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 25 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 26 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 27 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 28 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 29 https://avatars2.githubusercontent.com/u/1571674?v=4 ## 30 https://avatars2.githubusercontent.com/u/1571674?v=4 ## url html_url ## 1 https://api.github.com/users/jtleek https://github.com/jtleek ## 2 https://api.github.com/users/jtleek https://github.com/jtleek ## 3 https://api.github.com/users/jtleek https://github.com/jtleek ## 4 https://api.github.com/users/jtleek https://github.com/jtleek ## 5 https://api.github.com/users/jtleek https://github.com/jtleek ## 6 https://api.github.com/users/jtleek https://github.com/jtleek ## 7 https://api.github.com/users/jtleek https://github.com/jtleek ## 8 https://api.github.com/users/jtleek https://github.com/jtleek ## 9 https://api.github.com/users/jtleek https://github.com/jtleek ## 10 https://api.github.com/users/jtleek https://github.com/jtleek ## 11 https://api.github.com/users/jtleek https://github.com/jtleek ## 12 https://api.github.com/users/jtleek https://github.com/jtleek ## 13 https://api.github.com/users/jtleek https://github.com/jtleek ## 14 https://api.github.com/users/jtleek https://github.com/jtleek ## 15 https://api.github.com/users/jtleek https://github.com/jtleek ## 16 https://api.github.com/users/jtleek https://github.com/jtleek ## 17 https://api.github.com/users/jtleek https://github.com/jtleek ## 18 https://api.github.com/users/jtleek https://github.com/jtleek ## 19 https://api.github.com/users/jtleek https://github.com/jtleek ## 20 https://api.github.com/users/jtleek https://github.com/jtleek ## 21 https://api.github.com/users/jtleek https://github.com/jtleek ## 22 https://api.github.com/users/jtleek https://github.com/jtleek ## 23 https://api.github.com/users/jtleek https://github.com/jtleek ## 24 https://api.github.com/users/jtleek https://github.com/jtleek ## 25 https://api.github.com/users/jtleek https://github.com/jtleek ## 26 https://api.github.com/users/jtleek https://github.com/jtleek ## 27 https://api.github.com/users/jtleek https://github.com/jtleek ## 28 https://api.github.com/users/jtleek https://github.com/jtleek ## 29 https://api.github.com/users/jtleek https://github.com/jtleek ## 30 https://api.github.com/users/jtleek https://github.com/jtleek ## followers_url ## 1 https://api.github.com/users/jtleek/followers ## 2 https://api.github.com/users/jtleek/followers ## 3 https://api.github.com/users/jtleek/followers ## 4 https://api.github.com/users/jtleek/followers ## 5 https://api.github.com/users/jtleek/followers ## 6 https://api.github.com/users/jtleek/followers ## 7 https://api.github.com/users/jtleek/followers ## 8 https://api.github.com/users/jtleek/followers ## 9 https://api.github.com/users/jtleek/followers ## 10 https://api.github.com/users/jtleek/followers ## 11 https://api.github.com/users/jtleek/followers ## 12 https://api.github.com/users/jtleek/followers ## 13 https://api.github.com/users/jtleek/followers ## 14 https://api.github.com/users/jtleek/followers ## 15 https://api.github.com/users/jtleek/followers ## 16 https://api.github.com/users/jtleek/followers ## 17 https://api.github.com/users/jtleek/followers ## 18 https://api.github.com/users/jtleek/followers ## 19 https://api.github.com/users/jtleek/followers ## 20 https://api.github.com/users/jtleek/followers ## 21 https://api.github.com/users/jtleek/followers ## 22 https://api.github.com/users/jtleek/followers ## 23 https://api.github.com/users/jtleek/followers ## 24 https://api.github.com/users/jtleek/followers ## 25 https://api.github.com/users/jtleek/followers ## 26 https://api.github.com/users/jtleek/followers ## 27 https://api.github.com/users/jtleek/followers ## 28 https://api.github.com/users/jtleek/followers ## 29 https://api.github.com/users/jtleek/followers ## 30 https://api.github.com/users/jtleek/followers ## following_url ## 1 https://api.github.com/users/jtleek/following{/other_user} ## 2 https://api.github.com/users/jtleek/following{/other_user} ## 3 https://api.github.com/users/jtleek/following{/other_user} ## 4 https://api.github.com/users/jtleek/following{/other_user} ## 5 https://api.github.com/users/jtleek/following{/other_user} ## 6 https://api.github.com/users/jtleek/following{/other_user} ## 7 https://api.github.com/users/jtleek/following{/other_user} ## 8 https://api.github.com/users/jtleek/following{/other_user} ## 9 https://api.github.com/users/jtleek/following{/other_user} ## 10 https://api.github.com/users/jtleek/following{/other_user} ## 11 https://api.github.com/users/jtleek/following{/other_user} ## 12 https://api.github.com/users/jtleek/following{/other_user} ## 13 https://api.github.com/users/jtleek/following{/other_user} ## 14 https://api.github.com/users/jtleek/following{/other_user} ## 15 https://api.github.com/users/jtleek/following{/other_user} ## 16 https://api.github.com/users/jtleek/following{/other_user} ## 17 https://api.github.com/users/jtleek/following{/other_user} ## 18 https://api.github.com/users/jtleek/following{/other_user} ## 19 https://api.github.com/users/jtleek/following{/other_user} ## 20 https://api.github.com/users/jtleek/following{/other_user} ## 21 https://api.github.com/users/jtleek/following{/other_user} ## 22 https://api.github.com/users/jtleek/following{/other_user} ## 23 https://api.github.com/users/jtleek/following{/other_user} ## 24 https://api.github.com/users/jtleek/following{/other_user} ## 25 https://api.github.com/users/jtleek/following{/other_user} ## 26 https://api.github.com/users/jtleek/following{/other_user} ## 27 https://api.github.com/users/jtleek/following{/other_user} ## 28 https://api.github.com/users/jtleek/following{/other_user} ## 29 https://api.github.com/users/jtleek/following{/other_user} ## 30 https://api.github.com/users/jtleek/following{/other_user} ## gists_url ## 1 https://api.github.com/users/jtleek/gists{/gist_id} ## 2 https://api.github.com/users/jtleek/gists{/gist_id} ## 3 https://api.github.com/users/jtleek/gists{/gist_id} ## 4 https://api.github.com/users/jtleek/gists{/gist_id} ## 5 https://api.github.com/users/jtleek/gists{/gist_id} ## 6 https://api.github.com/users/jtleek/gists{/gist_id} ## 7 https://api.github.com/users/jtleek/gists{/gist_id} ## 8 https://api.github.com/users/jtleek/gists{/gist_id} ## 9 https://api.github.com/users/jtleek/gists{/gist_id} ## 10 https://api.github.com/users/jtleek/gists{/gist_id} ## 11 https://api.github.com/users/jtleek/gists{/gist_id} ## 12 https://api.github.com/users/jtleek/gists{/gist_id} ## 13 https://api.github.com/users/jtleek/gists{/gist_id} ## 14 https://api.github.com/users/jtleek/gists{/gist_id} ## 15 https://api.github.com/users/jtleek/gists{/gist_id} ## 16 https://api.github.com/users/jtleek/gists{/gist_id} ## 17 https://api.github.com/users/jtleek/gists{/gist_id} ## 18 https://api.github.com/users/jtleek/gists{/gist_id} ## 19 https://api.github.com/users/jtleek/gists{/gist_id} ## 20 https://api.github.com/users/jtleek/gists{/gist_id} ## 21 https://api.github.com/users/jtleek/gists{/gist_id} ## 22 https://api.github.com/users/jtleek/gists{/gist_id} ## 23 https://api.github.com/users/jtleek/gists{/gist_id} ## 24 https://api.github.com/users/jtleek/gists{/gist_id} ## 25 https://api.github.com/users/jtleek/gists{/gist_id} ## 26 https://api.github.com/users/jtleek/gists{/gist_id} ## 27 https://api.github.com/users/jtleek/gists{/gist_id} ## 28 https://api.github.com/users/jtleek/gists{/gist_id} ## 29 https://api.github.com/users/jtleek/gists{/gist_id} ## 30 https://api.github.com/users/jtleek/gists{/gist_id} ## starred_url ## 1 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 2 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 3 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 4 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 5 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 6 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 7 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 8 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 9 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 10 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 11 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 12 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 13 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 14 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 15 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 16 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 17 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 18 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 19 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 20 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 21 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 22 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 23 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 24 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 25 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 26 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 27 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 28 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 29 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## 30 https://api.github.com/users/jtleek/starred{/owner}{/repo} ## subscriptions_url ## 1 https://api.github.com/users/jtleek/subscriptions ## 2 https://api.github.com/users/jtleek/subscriptions ## 3 https://api.github.com/users/jtleek/subscriptions ## 4 https://api.github.com/users/jtleek/subscriptions ## 5 https://api.github.com/users/jtleek/subscriptions ## 6 https://api.github.com/users/jtleek/subscriptions ## 7 https://api.github.com/users/jtleek/subscriptions ## 8 https://api.github.com/users/jtleek/subscriptions ## 9 https://api.github.com/users/jtleek/subscriptions ## 10 https://api.github.com/users/jtleek/subscriptions ## 11 https://api.github.com/users/jtleek/subscriptions ## 12 https://api.github.com/users/jtleek/subscriptions ## 13 https://api.github.com/users/jtleek/subscriptions ## 14 https://api.github.com/users/jtleek/subscriptions ## 15 https://api.github.com/users/jtleek/subscriptions ## 16 https://api.github.com/users/jtleek/subscriptions ## 17 https://api.github.com/users/jtleek/subscriptions ## 18 https://api.github.com/users/jtleek/subscriptions ## 19 https://api.github.com/users/jtleek/subscriptions ## 20 https://api.github.com/users/jtleek/subscriptions ## 21 https://api.github.com/users/jtleek/subscriptions ## 22 https://api.github.com/users/jtleek/subscriptions ## 23 https://api.github.com/users/jtleek/subscriptions ## 24 https://api.github.com/users/jtleek/subscriptions ## 25 https://api.github.com/users/jtleek/subscriptions ## 26 https://api.github.com/users/jtleek/subscriptions ## 27 https://api.github.com/users/jtleek/subscriptions ## 28 https://api.github.com/users/jtleek/subscriptions ## 29 https://api.github.com/users/jtleek/subscriptions ## 30 https://api.github.com/users/jtleek/subscriptions ## organizations_url ## 1 https://api.github.com/users/jtleek/orgs ## 2 https://api.github.com/users/jtleek/orgs ## 3 https://api.github.com/users/jtleek/orgs ## 4 https://api.github.com/users/jtleek/orgs ## 5 https://api.github.com/users/jtleek/orgs ## 6 https://api.github.com/users/jtleek/orgs ## 7 https://api.github.com/users/jtleek/orgs ## 8 https://api.github.com/users/jtleek/orgs ## 9 https://api.github.com/users/jtleek/orgs ## 10 https://api.github.com/users/jtleek/orgs ## 11 https://api.github.com/users/jtleek/orgs ## 12 https://api.github.com/users/jtleek/orgs ## 13 https://api.github.com/users/jtleek/orgs ## 14 https://api.github.com/users/jtleek/orgs ## 15 https://api.github.com/users/jtleek/orgs ## 16 https://api.github.com/users/jtleek/orgs ## 17 https://api.github.com/users/jtleek/orgs ## 18 https://api.github.com/users/jtleek/orgs ## 19 https://api.github.com/users/jtleek/orgs ## 20 https://api.github.com/users/jtleek/orgs ## 21 https://api.github.com/users/jtleek/orgs ## 22 https://api.github.com/users/jtleek/orgs ## 23 https://api.github.com/users/jtleek/orgs ## 24 https://api.github.com/users/jtleek/orgs ## 25 https://api.github.com/users/jtleek/orgs ## 26 https://api.github.com/users/jtleek/orgs ## 27 https://api.github.com/users/jtleek/orgs ## 28 https://api.github.com/users/jtleek/orgs ## 29 https://api.github.com/users/jtleek/orgs ## 30 https://api.github.com/users/jtleek/orgs ## repos_url ## 1 https://api.github.com/users/jtleek/repos ## 2 https://api.github.com/users/jtleek/repos ## 3 https://api.github.com/users/jtleek/repos ## 4 https://api.github.com/users/jtleek/repos ## 5 https://api.github.com/users/jtleek/repos ## 6 https://api.github.com/users/jtleek/repos ## 7 https://api.github.com/users/jtleek/repos ## 8 https://api.github.com/users/jtleek/repos ## 9 https://api.github.com/users/jtleek/repos ## 10 https://api.github.com/users/jtleek/repos ## 11 https://api.github.com/users/jtleek/repos ## 12 https://api.github.com/users/jtleek/repos ## 13 https://api.github.com/users/jtleek/repos ## 14 https://api.github.com/users/jtleek/repos ## 15 https://api.github.com/users/jtleek/repos ## 16 https://api.github.com/users/jtleek/repos ## 17 https://api.github.com/users/jtleek/repos ## 18 https://api.github.com/users/jtleek/repos ## 19 https://api.github.com/users/jtleek/repos ## 20 https://api.github.com/users/jtleek/repos ## 21 https://api.github.com/users/jtleek/repos ## 22 https://api.github.com/users/jtleek/repos ## 23 https://api.github.com/users/jtleek/repos ## 24 https://api.github.com/users/jtleek/repos ## 25 https://api.github.com/users/jtleek/repos ## 26 https://api.github.com/users/jtleek/repos ## 27 https://api.github.com/users/jtleek/repos ## 28 https://api.github.com/users/jtleek/repos ## 29 https://api.github.com/users/jtleek/repos ## 30 https://api.github.com/users/jtleek/repos ## events_url ## 1 https://api.github.com/users/jtleek/events{/privacy} ## 2 https://api.github.com/users/jtleek/events{/privacy} ## 3 https://api.github.com/users/jtleek/events{/privacy} ## 4 https://api.github.com/users/jtleek/events{/privacy} ## 5 https://api.github.com/users/jtleek/events{/privacy} ## 6 https://api.github.com/users/jtleek/events{/privacy} ## 7 https://api.github.com/users/jtleek/events{/privacy} ## 8 https://api.github.com/users/jtleek/events{/privacy} ## 9 https://api.github.com/users/jtleek/events{/privacy} ## 10 https://api.github.com/users/jtleek/events{/privacy} ## 11 https://api.github.com/users/jtleek/events{/privacy} ## 12 https://api.github.com/users/jtleek/events{/privacy} ## 13 https://api.github.com/users/jtleek/events{/privacy} ## 14 https://api.github.com/users/jtleek/events{/privacy} ## 15 https://api.github.com/users/jtleek/events{/privacy} ## 16 https://api.github.com/users/jtleek/events{/privacy} ## 17 https://api.github.com/users/jtleek/events{/privacy} ## 18 https://api.github.com/users/jtleek/events{/privacy} ## 19 https://api.github.com/users/jtleek/events{/privacy} ## 20 https://api.github.com/users/jtleek/events{/privacy} ## 21 https://api.github.com/users/jtleek/events{/privacy} ## 22 https://api.github.com/users/jtleek/events{/privacy} ## 23 https://api.github.com/users/jtleek/events{/privacy} ## 24 https://api.github.com/users/jtleek/events{/privacy} ## 25 https://api.github.com/users/jtleek/events{/privacy} ## 26 https://api.github.com/users/jtleek/events{/privacy} ## 27 https://api.github.com/users/jtleek/events{/privacy} ## 28 https://api.github.com/users/jtleek/events{/privacy} ## 29 https://api.github.com/users/jtleek/events{/privacy} ## 30 https://api.github.com/users/jtleek/events{/privacy} ## received_events_url type site_admin ## 1 https://api.github.com/users/jtleek/received_events User FALSE ## 2 https://api.github.com/users/jtleek/received_events User FALSE ## 3 https://api.github.com/users/jtleek/received_events User FALSE ## 4 https://api.github.com/users/jtleek/received_events User FALSE ## 5 https://api.github.com/users/jtleek/received_events User FALSE ## 6 https://api.github.com/users/jtleek/received_events User FALSE ## 7 https://api.github.com/users/jtleek/received_events User FALSE ## 8 https://api.github.com/users/jtleek/received_events User FALSE ## 9 https://api.github.com/users/jtleek/received_events User FALSE ## 10 https://api.github.com/users/jtleek/received_events User FALSE ## 11 https://api.github.com/users/jtleek/received_events User FALSE ## 12 https://api.github.com/users/jtleek/received_events User FALSE ## 13 https://api.github.com/users/jtleek/received_events User FALSE ## 14 https://api.github.com/users/jtleek/received_events User FALSE ## 15 https://api.github.com/users/jtleek/received_events User FALSE ## 16 https://api.github.com/users/jtleek/received_events User FALSE ## 17 https://api.github.com/users/jtleek/received_events User FALSE ## 18 https://api.github.com/users/jtleek/received_events User FALSE ## 19 https://api.github.com/users/jtleek/received_events User FALSE ## 20 https://api.github.com/users/jtleek/received_events User FALSE ## 21 https://api.github.com/users/jtleek/received_events User FALSE ## 22 https://api.github.com/users/jtleek/received_events User FALSE ## 23 https://api.github.com/users/jtleek/received_events User FALSE ## 24 https://api.github.com/users/jtleek/received_events User FALSE ## 25 https://api.github.com/users/jtleek/received_events User FALSE ## 26 https://api.github.com/users/jtleek/received_events User FALSE ## 27 https://api.github.com/users/jtleek/received_events User FALSE ## 28 https://api.github.com/users/jtleek/received_events User FALSE ## 29 https://api.github.com/users/jtleek/received_events User FALSE ## 30 https://api.github.com/users/jtleek/received_events User FALSE Finally, I will leave you with a few other examples of using GitHub API: How long does it take to close a GitHub Issue in the dplyr package? How to retrieve all commits for a branch Getting my GitHub Activity 7.7 Google Sheets Google Sheets is increasingly used to create and store data. It is one of the most useful distributed data collection services. Not surprisingly, there are a number of tools that have been developed for reading data from Google Sheets. If the data are not protected, then you can make them public on the web and read them without authentication: library(googlesheets4) gs4_deauth() cameras = googlesheets4::read_sheet(&quot;https://docs.google.com/spreadsheets/d/16gHHSHCIg7r4NRu_8dbCaZzjkEChh799nyujXbiDVPg/edit?usp=sharing&quot;) ## Reading from &quot;2020-10-05-cameras&quot; ## Range &quot;2020-10-05-cameras&quot; cameras ## # A tibble: 80 x 9 ## address direction street crossStreet intersection `Location 1` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 GARRIS… E/B Garri… Wabash Ave &quot;Garrison \\… (39.341209,… ## 2 HILLEN… W/B Hillen Forrest St &quot;Hillen \\n … (39.29686, … ## 3 EDMOND… E/B Edmon… Woodbridge… &quot;Edmonson\\n… (39.293453,… ## 4 YORK R… S/B York … Gitting Ave &quot;York Rd \\n… (39.370493,… ## 5 RUSSEL… S/B Russe… Hamburg St &quot;Russell\\n … (39.279819,… ## 6 S MART… S/B MLK J… Pratt St &quot;MLK Jr. Bl… (39.286027,… ## 7 ORLEAN… E/B Orlea… Linwood Ave &quot;Orleans … (39.295866,… ## 8 E NORT… W/B North… Springlake… &quot;Northern P… (39.364311,… ## 9 W COLD… E/B Cold … Roland Ave &quot;Cold Sprin… (39.343906,… ## 10 E NORT… W/B North… York Road &quot;Northern P… (39.365146,… ## # … with 70 more rows, and 3 more variables: `2010 Census Neighborhoods` &lt;dbl&gt;, ## # `2010 Census Wards Precincts` &lt;dbl&gt;, `Zip Codes` &lt;dbl&gt; You can also read private Google Sheets if you have permission. You will first need to authenticate with your Google account: library(googlesheets4) gs4_auth() cameras = googlesheets4::read_sheet(&quot;https://docs.google.com/spreadsheets/d/16gHHSHCIg7r4NRu_8dbCaZzjkEChh799nyujXbiDVPg/edit?usp=sharing&quot;) cameras There is a lot more you can do with the googlesheets4 package including navigating the sheets you have access to, identifying them by id, and much more. All of the same caveats apply as with an Excel spreadsheet. These sheets can be just as complicated and hard to manage. 7.8 Databases If you plan to do data science in industry, one of the most useful things you can learn about is how to acces, manipulate, and use data that is stored in databases. We don’t have time to cover all of the varieties of databases in this course. So we will focus on relational databases. These databases include tables that have pre-defined relationships. There are several ways to query databases in R. Here we will use sqllite as an example of the type of database you can access in R. First, we will download a .sqlite database. This is a portable version of a SQL database. For our purposes, we will use the chinook sqlite database here. The database represents a “digital media store, including tables for artists, albums, media tracks, invoices and customers”. From the Readme.md file: Sample Data Media related data was created using real data from an iTunes Library. It is possible for you to use your own iTunes Library to generate the SQL scripts, see instructions below. Customer and employee information was manually created using fictitious names, addresses that can be located on Google maps, and other well formatted data (phone, fax, email, etc.). Sales information is auto generated using random data for a four year period. if(!file.exists(here(&quot;data&quot;, &quot;Chinook.sqlite&quot;))){ file_url &lt;- paste0(&quot;https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite&quot;) download.file(file_url, destfile=here(&quot;data&quot;, &quot;Chinook.sqlite&quot;)) } list.files(here(&quot;data&quot;)) ## [1] &quot;2020-10-05-cameras.csv&quot; &quot;2020-10-05-cameras.xlsx&quot; ## [3] &quot;2020-10-11-cameras.csv&quot; &quot;2020-10-12-cameras.csv&quot; ## [5] &quot;2020-10-21-cameras.csv&quot; &quot;2020-10-26-cameras.csv&quot; ## [7] &quot;2020-11-02-cameras.csv&quot; &quot;2020-11-09-cameras.csv&quot; ## [9] &quot;2020-11-15-cameras.csv&quot; &quot;2020-11-16-cameras.csv&quot; ## [11] &quot;2020-11-27-cameras.csv&quot; &quot;2020-11-29-cameras.csv&quot; ## [13] &quot;cameras.csv&quot; &quot;Chinook.sqlite&quot; ## [15] &quot;repos.json&quot; The main workhorse packages that we will use are the DBI and dplyr packages. Let’s look at the DBI::dbConnect() help file ?DBI::dbConnect So we need a driver and one example is RSQLite::SQLite(). Let’s look at the help file ?RSQLite::SQLite Ok so with RSQLite::SQLite() and DBI::dbConnect() we can connect to a SQLite database. Let’s try that with our Chinook.sqlite file that we downloaded. Chinook.sqlite library(DBI) conn &lt;- DBI::dbConnect(RSQLite::SQLite(), here(&quot;data&quot;, &quot;Chinook.sqlite&quot;)) conn ## &lt;SQLiteConnection&gt; ## Path: /Users/rdpeng/projects/ads2020/data/Chinook.sqlite ## Extensions: TRUE So we have opened up a connection with the SQLite database. You can use a similar process with most common database backends, both locally on your computer and by connecting to databases on the cloud like BigQuery or RedShift. Next, we can see what tables are available in the database using the dbListTables() function: dbListTables(conn) ## [1] &quot;Album&quot; &quot;Artist&quot; &quot;Customer&quot; &quot;Employee&quot; ## [5] &quot;Genre&quot; &quot;Invoice&quot; &quot;InvoiceLine&quot; &quot;MediaType&quot; ## [9] &quot;Playlist&quot; &quot;PlaylistTrack&quot; &quot;Track&quot; From RStudio’s website, there are several ways to interact with SQL Databases. One of the simplest ways that we will use here is to leverage the dplyr framework. \"The dplyr package now has a generalized SQL backend for talking to databases, and the new dbplyr package translates R code into database-specific variants. As of this writing, SQL variants are supported for the following databases: Oracle, Microsoft SQL Server, PostgreSQL, Amazon Redshift, Apache Hive, and Apache Impala. More will follow over time. So if we want to query a SQL databse with dplyr, the benefit of usingdbplyris: \"You can write your code indplyrsyntax, anddplyrwill translate your code into SQL. There are several benefits to writing queries indplyrsyntax: you can keep the same consistent language both for R objects and database tables, no knowledge of SQL or the specific SQL variant is required, and you can take advantage of the fact thatdplyruses lazy evaluation. Let's take a closer look at theconn` database that we just connected to: library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(dbplyr) ## ## Attaching package: &#39;dbplyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## ident, sql src_dbi(conn) ## src: sqlite 3.33.0 [/Users/rdpeng/projects/ads2020/data/Chinook.sqlite] ## tbls: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, ## MediaType, Playlist, PlaylistTrack, Track You can think of the multiple tables similar to having multiple worksheets in a spreadsheet. Let’s try interacting with one. 7.8.1 Querying with dplyr syntax First, let’s look at the first ten rows in the Album table. tbl(conn, &quot;Album&quot;) %&gt;% head(n=10) ## # Source: lazy query [?? x 3] ## # Database: sqlite 3.33.0 [/Users/rdpeng/projects/ads2020/data/Chinook.sqlite] ## AlbumId Title ArtistId ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 For Those About To Rock We Salute You 1 ## 2 2 Balls to the Wall 2 ## 3 3 Restless and Wild 2 ## 4 4 Let There Be Rock 1 ## 5 5 Big Ones 3 ## 6 6 Jagged Little Pill 4 ## 7 7 Facelift 5 ## 8 8 Warner 25 Anos 6 ## 9 9 Plays Metallica By Four Cellos 7 ## 10 10 Audioslave 8 The output looks just like a data.frame that we are familiar with. But it’s important to know that it’s not really a dataframe. For example, what about if we use the dim() function? tbl(conn, &quot;Album&quot;) %&gt;% dim() ## [1] NA 3 Interesting! We see that the number of rows returned is NA. This is because these functions are different than operating on datasets in memory (e.g. loading data into memory using read_csv()). Instead, dplyr communicates differently with a SQLite database. Let’s consider our example. If we were to use straight SQL, the following SQL query returns the first 10 rows from the Album table: SELECT * FROM `Album` LIMIT 10 In the background, dplyr does the following: translates your R code into SQL submits it to the database translates the database’s response into an R data frame To better understand the dplyr code, we can use the show_query() function: Album &lt;- tbl(conn, &quot;Album&quot;) show_query(head(Album, n = 10)) ## &lt;SQL&gt; ## SELECT * ## FROM `Album` ## LIMIT 10 This is nice because instead of having to write the SQL query ourself, we can just use the dplyr and R syntax that we are used to. However, the downside is that dplyr never gets to see the full Album table. It only sends our query to the database, waits for a response and returns the query. However, in this way we can interact with large datasets! Many of the usual dplyr functions are available too: select() filter() summarize() and many join functions. Ok let’s try some of the functions out. First, let’s count how many albums each artist has made. tbl(conn, &quot;Album&quot;) %&gt;% group_by(ArtistId) %&gt;% summarize(n = count(ArtistId)) %&gt;% head(n=10) ## # Source: lazy query [?? x 2] ## # Database: sqlite 3.33.0 [/Users/rdpeng/projects/ads2020/data/Chinook.sqlite] ## ArtistId n ## &lt;int&gt; &lt;int&gt; ## 1 1 2 ## 2 2 2 ## 3 3 1 ## 4 4 1 ## 5 5 1 ## 6 6 2 ## 7 7 1 ## 8 8 3 ## 9 9 1 ## 10 10 1 Next, let’s plot it. library(ggplot2) tbl(conn, &quot;Album&quot;) %&gt;% group_by(ArtistId) %&gt;% summarize(n = count(ArtistId)) %&gt;% arrange(desc(n)) %&gt;% ggplot(aes(x = ArtistId, y = n)) + geom_bar(stat = &quot;identity&quot;) Let’s also extract the first letter from each album and plot the frequency of each letter. tbl(conn, &quot;Album&quot;) %&gt;% mutate(first_letter = str_sub(Title, end = 1)) %&gt;% ggplot(aes(first_letter)) + geom_bar() 7.8.2 Delayed execution One important feature of dbplyr for large data sets is delayed execution. From the dbplyr documentation: It never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step. This means that when you perform a command like this test = tbl(conn, &quot;Album&quot;) Then the database hasn’t been touched! It won’t be until you run the command. Even then it will only pull the first 10 rows of the result once you ask to see the data test ## # Source: table&lt;Album&gt; [?? x 3] ## # Database: sqlite 3.33.0 [/Users/rdpeng/projects/ads2020/data/Chinook.sqlite] ## AlbumId Title ArtistId ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 For Those About To Rock We Salute You 1 ## 2 2 Balls to the Wall 2 ## 3 3 Restless and Wild 2 ## 4 4 Let There Be Rock 1 ## 5 5 Big Ones 3 ## 6 6 Jagged Little Pill 4 ## 7 7 Facelift 5 ## 8 8 Warner 25 Anos 6 ## 9 9 Plays Metallica By Four Cellos 7 ## 10 10 Audioslave 8 ## # … with more rows If you want to pull all of the data into R, a good idea is to first check and see how many rows it has. You can do this with the count function test %&gt;% count() ## # Source: lazy query [?? x 1] ## # Database: sqlite 3.33.0 [/Users/rdpeng/projects/ads2020/data/Chinook.sqlite] ## n ## &lt;int&gt; ## 1 347 In this case it is a small number, so we can pull all of the data into memory in R using the collect function - the resulting data frame has the same number of rows as we discovered using the count function. test %&gt;% collect() ## # A tibble: 347 x 3 ## AlbumId Title ArtistId ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 For Those About To Rock We Salute You 1 ## 2 2 Balls to the Wall 2 ## 3 3 Restless and Wild 2 ## 4 4 Let There Be Rock 1 ## 5 5 Big Ones 3 ## 6 6 Jagged Little Pill 4 ## 7 7 Facelift 5 ## 8 8 Warner 25 Anos 6 ## 9 9 Plays Metallica By Four Cellos 7 ## 10 10 Audioslave 8 ## # … with 337 more rows When working with databases it is a good idea to think about ways you can make the data smaller before pulling it into memory. One is through grouping and aggregating as we showed above test %&gt;% group_by(ArtistId) %&gt;% summarize(n = count(ArtistId)) %&gt;% collect() ## # A tibble: 204 x 2 ## ArtistId n ## &lt;int&gt; &lt;int&gt; ## 1 1 2 ## 2 2 2 ## 3 3 1 ## 4 4 1 ## 5 5 1 ## 6 6 2 ## 7 7 1 ## 8 8 3 ## 9 9 1 ## 10 10 1 ## # … with 194 more rows Thinking cleverly about how to make data small in database takes a certain way of thinking, but with practice can make your data analysis much easier. Some packages have been developed that automatically push certain comptuations to the database for you including: dbplot for making plots in database modeldb for running certain types of models (like regression) in database Some important things to consider when dealing with data in databases are: Making sure you count your data before you pull it down into a local environment. Listing and looking through each table. Sometimes called database spelunking this is an important step before using any database Understanding whether it costs more computationally to do an analysis mostly in the database or mostly on your computer. This will involve tradeoffs depending on the size of the data and your local computing power. Ensuring you have proper authentication to access the databases and tables that you care about. 7.9 APIs Application Programming Interfaces (or APIs) are a way to access data through a structured URL over the web. Most social media companies provide their data to the public through this format. But there are a wide range of other government entities, companies, and non-profits that make their data available via an API. The reason companies distribute data this way is because they can control what and how much data you download. Most have rate limits - the number of calls or amount of data you can pull per unit time. You need to respect these limits or your data collection will be blocked. When you want to collect data from an API, the first thing you should check and see is if someone built a package for that API. For most common APIs a package will exist (for example rtweet or Rfacebook). If they don’t exist, you can use the httr R package to build your own access to an API. Regardless of whether you are using an R package or building your own, you should: Read the developer docs - look for rate limits, licenses, and other information about how to use the API appropriately. Look at the worked examples, to figure out how to use the API. Each one is different, but all have a similar, structured URL approach. Let’s dissect an example of an API url. If you go to this url: https://api.github.com/search/repositories?q=created:2014-08-13+language:r+-user:cran&amp;type you will get a bunch of JSON - this is data from the Github API. Let’s break down each part of this url: https://api.github.com/search/repositories - This is the base url that tells us we will be asking for repository data ?q= - This defines the “query”, or the search, we will be doing created:2014-08-13 - This tells us we will search for repos created on 2014-08-13 - This is like an &quot;and&quot; for the search language:r - This tells us we are searching for only R repos -user:cran&amp;type - This tells us we don’t want any repos created by cran (to avoid explosion of repos - cran has a lot!) Using the httr package we can access these data directly using the GET command: library(httr) query_url = &quot;https://api.github.com/search/repositories?q=created:2014-08-13+language:r+-user:cran&quot; req = GET(query_url) req ## Response [https://api.github.com/search/repositories?q=created:2014-08-13+language:r+-user:cran] ## Date: 2020-11-29 17:58 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 179 kB ## { ## &quot;total_count&quot;: 77, ## &quot;incomplete_results&quot;: false, ## &quot;items&quot;: [ ## { ## &quot;id&quot;: 22917249, ## &quot;node_id&quot;: &quot;MDEwOlJlcG9zaXRvcnkyMjkxNzI0OQ==&quot;, ## &quot;name&quot;: &quot;mulTree&quot;, ## &quot;full_name&quot;: &quot;TGuillerme/mulTree&quot;, ## &quot;private&quot;: false, ## ... The resulting request will give us the status, the date, and information about the content. We can use the content function to extract the data- in this case a nested list: names(content(req)) ## [1] &quot;total_count&quot; &quot;incomplete_results&quot; &quot;items&quot; Not all APIs are open like this Github API. You may have to create a developer account and authenticate before accessing the data. You can usually figure this out by following the developer docs on each individual site. Some things to keep in mind when you are downloading data from APIs are the following: Pay attention to the terms of service and developer docs Respect rate limits so you won’t be blocked Think carefully about what data you pull and share, it is very easy to collect data people wouldn’t want shared Remember the data are constantly updating since they are from the web, so you might want to save versions of the data Remember that the data are only the version that the company/government/entity wants to expose, so may have errors or issues due to translation. 7.10 Webscraping Do we want to purchase a book on Amazon? Next we are going to learn about what to do if your data is on a website (XML or HTML) formatted to be read by humans instead of R. We will use the (really powerful) rvest R package to do what is often called “scraping data from the web”. Before we do that, we need to set up a few things: SelectorGadget tool rvest and SelectorGadget guide Awesome tutorial for CSS Selectors Introduction to stringr Regular Expressions/stringr tutorial Regular Expression online tester- explains a regular expression as it is built, and confirms live whether and how it matches particular text. We’re going to be scraping this page: it just contains the (first page of) reviews of the ggplot2 book by Hadley Wickham. url &lt;- &quot;http://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/product-reviews/0387981403/ref=cm_cr_dp_qt_see_all_top?ie=UTF8&amp;showViewpoints=1&amp;sortBy=helpful&quot; We use the rvest package to download this page. library(rvest) ## Loading required package: xml2 h &lt;- read_html(url) Now h is an xml_document that contains the contents of the page: h ## {html_document} ## &lt;html lang=&quot;en-us&quot; class=&quot;a-no-js&quot; data-19ax5a9jf=&quot;dingo&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body&gt;\\n&lt;span id=&quot;cr-state-object&quot; data-state=&#39;{&quot;asin&quot;:&quot;0387981403&quot;,&quot;devi ... How can you actually pull the interesting information out? That’s where CSS selectors come in. 7.10.1 CSS Selectors CSS selectors are a way to specify a subset of nodes (that is, units of content) on a web page (e.g., just getting the titles of reviews). CSS selectors are very powerful and not too challenging to master- here’s a great tutorial But honestly you can get a lot done even with very little understanding, by using a tool called SelectorGadget. Install the SelectorGadget on your web browser. (If you use Chrome you can use the Chrome extension, otherwise drag the provided link into your bookmarks bar). Here’s a guide for how to use it with rvest to “point-and-click” your way to a working selector. For example, if you just wanted the titles, you’ll end up with a selector that looks something like .a-text-bold span. You can pipe your HTML object along with that selector into the html_nodes function, to select just those nodes: h %&gt;% html_nodes(&quot;.a-text-bold span&quot;) ## {xml_nodeset (10)} ## [1] &lt;span&gt;Must-have reference for R graphics&lt;/span&gt; ## [2] &lt;span&gt;Still a great package and highly worth learning - but the text is ... ## [3] &lt;span&gt;Excellent&lt;/span&gt; ## [4] &lt;span&gt;Nice resource, but already out of date&lt;/span&gt; ## [5] &lt;span&gt;The best guide to the best graphics (I think) out there.&lt;/span&gt; ## [6] &lt;span&gt;Graphing in R&lt;/span&gt; ## [7] &lt;span&gt;Excellent content, poor adaptation to kindle&lt;/span&gt; ## [8] &lt;span&gt;Excellent R resource for the Kindle&lt;/span&gt; ## [9] &lt;span&gt;Great book, outdated&lt;/span&gt; ## [10] &lt;span&gt;Indispensable resource for ggplot2 users&lt;/span&gt; But you need the text from each of these, not the full tags. Pipe to the html_text function to pull these out: review_titles &lt;- h %&gt;% html_nodes(&quot;.a-text-bold span&quot;) %&gt;% html_text() review_titles ## [1] &quot;Must-have reference for R graphics&quot; ## [2] &quot;Still a great package and highly worth learning - but the text is getting quite out of date.&quot; ## [3] &quot;Excellent&quot; ## [4] &quot;Nice resource, but already out of date&quot; ## [5] &quot;The best guide to the best graphics (I think) out there.&quot; ## [6] &quot;Graphing in R&quot; ## [7] &quot;Excellent content, poor adaptation to kindle&quot; ## [8] &quot;Excellent R resource for the Kindle&quot; ## [9] &quot;Great book, outdated&quot; ## [10] &quot;Indispensable resource for ggplot2 users&quot; Now we’ve extracted something useful! Similarly, let’s grab the format (hardcover or paperback). Some experimentation with SelectorGadget shows it’s: h %&gt;% html_nodes(&quot;.a-size-mini.a-color-secondary&quot;) %&gt;% html_text() ## character(0) Now, we may be annoyed that it always starts with Format:. Let’s introduce the stringr package. formats &lt;- h %&gt;% html_nodes(&quot;.a-size-mini.a-color-secondary&quot;) %&gt;% html_text() %&gt;% stringr::str_replace(&quot;Format: &quot;, &quot;&quot;) formats ## character(0) We could do similar exercise for extracting the number of stars and whether or not someone found a review useful. This would help us decide if we were interested in purchasing the book! Webscraping is the opposite of APIs in some sense. Anything that is public on a website can technically be webscraped using things like the rvest package. However, that doesn’t mean it is always ethical or a good idea to do so. One extreme example is a student who published the private OkCupid data of 70,000 people he had scraped from the web. This included private information, including sexual preferences, pictures, and intimate details from the profiles. At the time the way he scraped the data did not violate the terms of service of the website, but the way the data were collected and shared were ethically disasterous. So when scraping data think very carefully about the ethics and purpose of your data collection. Some other things to be aware of with data scraping are 1.Most websites have a file called robots.txt. You can see the one for Google here this file tells you what it is ok to scrape and not. Some companies will block you if you try to scrape their website. In one case a student got his whole university blocked from accessing certain journals by webscraping! The data will certainly update frequently if you scrape it from a website. Some companies consider data on their websites proprietary and you can find yourself in legal battles if you collect and use them for commercial purposes. 7.11 Google-ing It seems silly to talk about using Google in an “advanced” course. But for things like getting data you will be spending a lot of time Googling. As packages evolve super rapidly, you will often want to check your workflows and make sure they haven’t gone out of date. For example, since the last time I taught this course, the googlesheets package has been replaced by googlesheets4, among other changes! A good default move when embarking on a new data collection exercise is to Google for workflows for the thing you want. I generally use variations on: “rstats reading data type x” “rstats tutorial on data type x” “stack overflow data type x” As a place to get started, but I also use Google by copying and pasting exact error messages I run into with any new package. I point this out primarily to make sure you know that it is not only acceptable, but encouraged to use the internet and Google as a resource when figuring out how to handle new data sets. 7.12 Additional Resources An outstanding JSM tutorial on webscraping The databases using Rstudio website Relational data section of R for Data Science Some of my lecture slides on webscraping and APIs Some of my lecture slides on databases 7.13 Homework Template Repo: https://github.com/advdatasci/homework6 Repo Name: homework6-ind-yourgithubusername Pull Date: 2020/10/12 9:00AM Baltimore Time "],["week-7.html", "8 Week 7 8.1 Week 7 Learning objectives 8.2 A framework for exploratory analysis 8.3 Some general EDA principles 8.4 Organizing an EDA 8.5 Additional Resources 8.6 Homework", " 8 Week 7 8.1 Week 7 Learning objectives At the end of this lesson you will be able to: Apply an exploratory data analysis framework to refine questions Apply an exploratory data anaylsis framework to identify data issues Organize your exploratory data analysis files Know and apply general exploratory data analysis principles This lecture is based on Roger Peng’s nice lecture on exploratory data analysis 8.2 A framework for exploratory analysis “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” - John Tukey Last week we talked about different ways of collecting data. Sometimes this will be an exercise you perform, and sometimes you will just be handed data from someone else. Regardless of how the data arrives the first step you will pursue in any data analysis is to explore the data to understand several things: Do you have the right question? Do you have the right data? can you sketch a solution to the question you care about? 8.2.1 Do you have the right question? As we discussed back in Week 1 the first and most important step in a data analysis is defining the question you want to answer. That sounds great in theory, but in practice, the question you set out to answer will almost inevitably change during the course of a data analysis. In his blog post on design thinking in statistics Roger lays out a phase diagram for data analysis. On the x-axis is the quality of the question. On the y-axis is the strength of evidence to answer that question. It is a common assumption that the data analysis question we are asking is “good” right from the start. So a data analysis is simply the process of moving along the phase diagram until you have strong evidence to answer the question. But in reality the question is often more dubious, vague, or not sufficiently clearly specified. This will be true both for questions you elicit from collaborators and for questions you define for yourself! The reason is it is really hard to define good data analytic questions that are concrete enough to answer, specific enough to support analytically, and still sufficiently interesting to merit attention. Statistics and machine learning - as they are usually defined - typically focus on building tools and techniques to improve the quality of evidence used to answer a question. Data science, on the other hand, usually involves both improving the question and providing evidence to answer that question. Not infrequently, improving the question will absorb more data analytic time than amassing evidence. The typical workflow for a data analysis involves one or more periods of exploration followed by a narrowing and problem definition for further analysis. In the first phase of an analysis you may start with a relatively specific question you want to answer. Exploring the data involves considering a range of possibilities - whether the data is good enough, whether it can answer the question you wanted to in the first place, whether there are potential issues that you didn’t know about in advance. After a full exploration of the data you will again refine the question and sketch an answer. Then, the modeling phase of analysis will involve a second stage of exploration and consideration of different potential modeling choices resulting in a concrete answer to the question specified on the basis of your exploration that you can work to communicate. The middle phase (Phase 2 in the diagram above) represents an important moment in any analysis. Often a good exploratory analysis will be sufficient to give you a strong indication of whether a more complete analysis will answer the question you care about. This is a sort of Pareto principle of data analysis - which many have recognized. “In my experience when a moderately good solution to a problem has been found, it is seldom worth while to spend much time trying to convert this to the ‘best’ solution. The time is much better spent in real research.” –George Kimball, “A critique of operations research,” J. Wash. Acad. Sci, 1958 Once you refine your question and come up with your first sketch of a solution, it is worth “trusting your gut” before devoting significant extra resources to detailed and complicated statistical modeling. Typically after an exploratory analysis you will have one of three feelings about the potential for answering your question. You are unlikely to find a satisfactory answer with a formal analysis this is the case when there is limited signal, an obvious confounder, major data issues, or some other feature that makes the signal very difficult to find. In this case, it is worth considering whether fitting a complicated statistical model won’t lead to over-interpretation of your data. Proceed with caution if your exploration leads you to this conclusion. You aren’t certain what the formal analysis will show this is the case when there appears to be some signal, the data are relatively well behaved, and you aren’t entirely cretain what the answer will be after full statistical modeling. This represents a major fraction of data analyses and suggests that you should proceed as planned with your new question defined through your exploration. The signal is obvious and clear this is the case when your original study is well designed, there is a lot of very clear data, or the signal is so abundantly clear it is hard to imagine an alternative explanation for the signal you see. Here the primary purpose of formal statistical modeling is both to summarize this signal and to spend some time thinking carefully about potential biases. If the signal is expected, and especially if it is surprising, it is worth being careful. As Daniel MacArthur put it: &gt; Few principles are more depressingly familiar to the veteran scientist: the more surprising a result seems to be, the less likely it is to be true. The inputs to the data exploration are the question, context, resources and audience. When you complete your exploratory analysis the goals should be to have refined your existing questions, defined and settled on any new questions, and have a sketch for your ultimate statistical analysis. While this may seem like a single pass through the data often it is an iterative process between getting data, exploring your data, checking the data, refining hypotheses, and going back to the original data collection paradigm to validate. Ultimately you want to settle on a question that is Specific - so your analysis will not be unwieldy Not overly narrow - so that your data is sufficient to answer the question both in size and target. Relevant - interesting either to you or your collaborators As an example you may be interested in the question of if you run do you lose more weight. But you might only have data collected on yourself and you might be missing certain key variables. You will want to consider what type of question you can answer and what the limitations might be. 8.2.2 Do you have the right data? Perhaps the key question in an exploratory analysis is to determine if you have the right data to answer the question. One of the key callenges is that the data you get in classrooms is much cleaner and easier to manipulate than the data you typically have to deal with in the real world. As Jenny Bryan put it in her JSM 2015 talk: “Classroom data are like teddy bears and real data are like a grizzly bear with salmon blood dripping out its mouth.” When doing any exploratory analysis you want to confirm at minimum these five things. Are the data good proxies for what you want to measure? Sometimes the data you collected don’t measure what you wanted to in order to answer your question. You may be interested in income, but you only measured reported income. You may be interested in time to death, but you only measured intermediate outcomes. To assess whether you have the right measures: Understand the source of each of the measurements in your data set Compare them to the expectations you had when defining your question Identify any gaps between the source and expecations - there will always be a gap! However your job is to determine how far is too far and then document any potential issues for including as modifications of your later analysis. Are there sufficient data, and the right type of data, to answer the question? This is the most common post-mortem on experimental data - you don’t have enough or the right type of data to answer the question. This is a very common situation to have to navigate with collaborators and we will dedicate more discussion to how to handle that conversation in future classes. But be sympathetic, some day you might collect your own data (like I did) and find when you were analyzing the data in the middle of the night you didn’t quite have enough: You should think through carefully not only if you have enough data, but enough of the kind of data you need. For example you may have a huge data set when performing survival analysis - but if you don’t have enough observed events you won’t be able to reliably detect statistical signals. Are there obviously missing pieces of information that could be confounders or modifiers? This is probably the trickiest one to evaluate. It generally involves considering the context of the problem very carefully. For example, if you’ve collected information on the relationship between drinking and incidence of cancer - it might be important to consider smoking as a confounder. You might only know that from the context of the problem. In terms of exploratory analysis of data the presence of a missing covariate is most often observed by an unexplained pattern or clustering in the data. For example this plot of Covid-19 cases Shows a regular, repeating spike. If all you had was the counts, you might think there must be an underlying variable you haven’t collected that might explain such an important signal (in this case it is likely the weekend effect in counting). This might be the only type of indicator you have a missing variable so worth keeping an eye out for. Are there any patterns of missing data among the measured variables that might cause problems for an analysis? This type of missingness is somewhat easier to detect. Mostly because you can count whether there are NA or missing values. The tricky thing is to determine why those data are missing and whether it is related to the variables you care about. This is a relatively nice explainer of the types of missingness. The ideal (and often untestable) case is where the data are “missing completely at random” - in other words they aren’t missing in a way that correlates with any of the variables you care about. Slightly less good, but still ok, is if the data are “missing at random” where the observed variables can explain the systematic missingness patterns. Finally, “missing not at random” could spell significant trouble for your analysis since there are systematic patterns to the missing data, but they can’t be explained by the variables you have. Again, as with the missing variables, you may need to explore the data carefully to try to observe patterns of variation in missigness that may be important for your analysis. Do the data have obvious errors that could increase bias or uncertainty? There are a very large number of ways data can be wrong. Sometimes these errors will be blazingly obvious. Sometimes they are exceedingly subtle. The key is to understand thoroughly and completely not only the code book for the variables but also the data themselves. Counting, visualization, and comparison to quantities you know must be true can help identify problems in a data set. But it is almost impossible to be 100% sure you have caught everything. 8.2.3 Can you sketch a solution to the question you care about? Once your data have been organized and you’ve explored them for the obvious issues above, the last component of any exploratory data analysis is to sketch a solution to your primary questions. This sketch may be as simple as a table or a plot - possibly with a statistical model in mind. It will often tell you 80% of the answer - even if that basic sketch is rarely shown to others or included in your final analysis. Think the lego bridge, not the real one. When you are making this sketch you might identify new issues with the data, or modifications to your question. So you may have to cycle between these steps. As John Tukey said: The value of a plot is that it allows us to see what we never expected to see.” So while you are performing an exploratory analysis you should be constantly resetting your expectations to compare them to the summaries and plots of the data you are making. You can use statistical models to help explore data as well. Typically these are stripped down versions of the model you will ultimately fit. Keep in mind that models quantify the expectations you have for the data, but won’t necessarily reveal what is unexpected. When exploring with models it is critical to overlay the data on any model fits you may calculate. 8.3 Some general EDA principles 8.3.1 Check the packaging What can you learn about the dataset before looking directly at the data? - Check codebooks - Ask questions - Read up on the data collection procedure Check on structure and format - Is the data a rectangle? If not look at the files/check the DB • Check metadata; are all variables there that you expected? - Compare the codebook to the data files you have - Make sure you have all the files you need • Are all metadata present? - Make sure there is a codebook - Make sure all the dates, times, and data types match up 8.3.2 Rectangle your data Some data sets will come in as “rectangles” - easy to manage data sets (possibly tidy already) that you can directly use in R. But some spreadsheets will have extra header rows, JSON files will read in as lists, databases may have multiple tables you need to join. The first step toward exploring your data is often “rectangling” the data. At this stage, it doesn’t have to be tidy data. You may split out rows or columns, clean up values, spread your data set out, or make it into a long, narrow table. But most data analytic operations in terms of plotting, counting, and more rely on rectangled data. So this is often a good first step. 8.3.3 Look at the top and bottom Check the first few rows - Do the values match expectations? - Are the right variables present? Check the last few rows; make sure all rows were read properly and there’s no crud at the end - Are there weird values at the end? - Missing values? - Corrupted file endings? Time/Date data often sorted; make sure all dates/ times are in appropriate range 8.3.4 Always be counting Count various aspects of your dataset - Number of rows - Number of columns - Number of levels of factors - Number of missing values Compare counts with landmarks - Does the row number match the sample size? - Do the number of columns match your number of variables? - Are there less than or equal to 31 days per month? Number of subjects (unique IDs), number of visits per subject, number of locations, number of missing observations, etc. Always be counting at every phase (“checking mindset”) - Assume the data may have errors! 8.3.5 Make a plot Actually make a ton of plots. You can’t have too many exploratory plots. Go overboard, most of these will be left on the cutting room floor. Plot every variable, plot most of the comparisons between pairs of variables. Consider heatmaps and other multi-variable plots as well. Plots show expectations and deviations from those expectations (i.e. distribution mean and outliers) Tables generally only show summaries, not deviations; also everything on the same “scale” Draw a “fake plot” first - set your expectations 8.3.6 Validate with an external data source Compare your data to something outside the dataset Even a single number/summary statistic comparison can be useful - Is the range of values what you’d see in the literature? - Is the number of observations plausible for this type of data? Compare your measurements to another similar measurement to check that they’re correlated - Get external upper/lower bounds - Ex: number of people should exceed total population - Ex: Check for negative values when they should be positive 8.3.7 Try the easy solution Before you fit a complicated statistical model, try the easiest possible solution. Are you looking for a correlation? Just plot x vs y and see what it looks like colored by various potential confounders. Are you looking for a trend over time? Plot the variable vs time and look for patterns and deviations. Are you looking to compare the mean of two groups? Make a boxplot. First step in building a primary model Build prima facie evidence Basic argument, without nuance (that comes later) Maybe just one plot (or table) 8.3.8 Follow up Do you have the right question? - Questions often change once you see the data - Typically the scope narrows Do you have the right data? - Sometimes you might realize you don’t have the data to answer your original question Do you need other data? - Can you find it? Or is it better to treat this as a limitation Could you sketch the solution? Is there signal in the data? - From painful experience, if you don’t see much in the first pass simple analysis, it is often hard to find it even with much more effort. 8.4 Organizing an EDA When performing an EDA, I typically create one or more analysis files. Often my files are placed in the code/ - raw_code/ folder with most of my plots being saved to figures/ -exploratory figures I generally try to name my files in an order that I hope will make sense later. So something like: 0_data_pull.R 1_first_plots.R I generally use .R files (not Rmd files or Jupyter notebooks) since I will likely be moving fast and don’t have a lot of time for formatting. One thing I like to do is add comments when I find something interesting with words I hope will be searchable later. So something like: ######### ### Interesting correlation between x and y ######### cor(x,y) 8.5 Additional Resources Karl Broman Lecture on Data Wrangling Roger Peng Lecture on EDA 8.6 Homework Template Repo: https://github.com/advdatasci/homework7 Repo Name: homework7-ind-yourgithubusername Pull Date: 2020/10/12 9:00AM Baltimore Time "],["week-8.html", "9 Week 8 9.1 Week 8 Learning objectives 9.2 A framework for modeling 9.3 Additional Resources 9.4 Homework", " 9 Week 8 9.1 Week 8 Learning objectives At the end of this lesson you will be able to: Define the central dogmas of prediction and inference Identify the key components of a modeling process (signal, systematic noise, random noise) Apply the steps in statistical modeling for data science Know how to use “wrong” models to get correct inference for specific trends 9.2 A framework for modeling Statistical modeling and machine learning are often considered the key components of data science. There are entire courses in our department at the Johns Hopkins Bloomberg School of Public Health and across all of academics focused squarely on these topics. We aren’t going to try to cover these whole topics in a single lecture! Instead, our focus is to cover the key concepts and ideas behind how you can fit the tools you already know into the data science process we have been learning about over the course of this class. In last week’s lecture we covered the principles of exploratory data analysis (EDA). The goal of EDA is to familiarize yourself with the structure, quirks, and potential flaws in the data set. The final step is an initial “sketch” for the statistical modeling approach that you plan to use. While it is easier to teach data munging, exploratory data analysis and statistical modeling as separate lectures in a course, the reality is that these components form a tightly interconnected feedback loop. The statistical modeling component of this feedback loop focuses on creating a precise quantification of both the signals in the data set and the uncertainty we may have about those signals. To do that we deploy a variety of mathematical models but at the heart of these models is a goal to understand the way the world works. So the mathematical models you use, whether for statistical inference, prediction, or causal inference should be developed with the understanding they are part of the overall arc of the data analytic story. There is a famous phrase in statistics: All models are wrong, some are useful - George Box Like “correlation does not imply causation” this is a pithy phrase that gets tossed around a lot. But what does it actually mean? It means that when we are doing statistical analysis or machine learning it will be nearly impossible for us to get all of the right variables in the equations in all of the right forms. This means that no matter how hard we try our model will be “wrong”. But if we are careful about how we interpret the model - quantifying important trends and documenting artifacts and uncertainty - we can say something about the way the world works. So the model may be “useful”. It is helpful to remember when performing statistical modeling that the goal isn’t to quantify the “truth”. The goal is to fairly represent a summary of the trends in the data. 9.2.1 Identify your goal We discussed the different types of statistical questions in the first lecture of this course: You can use statistical models to address any of the types of questions, from descriptions of the data to mechanistic models. However, for this lecture we will on statistical inference and statistical prediction (sometimes called machine learning). These are the two most popular data analytic tasks; moreover most other types of analysis rely on the same models used for either statistical inference or machine learning with the addition or subtraction of some assumptions. 9.2.1.1 Statistical inference The goal of statistical inference is to report sources of “signal” in a data set, while documenting and accounting for both systematic and “random” sources of errors. Just as there is a central dogma of molecular biology there is also a central dogma of statistics, which I first saw coined in Josh Akey’s lecture notes: Statistical inference is focused on using probability to sample from a population, take measurements on that sample, and use the samples to infer something about the characteristics of the population on average. The characteristic of the population you are estimating is called a parameter and you use a statistical estimate to try to guess what that parameter might be. You can then use the information you have about sources of uncertainty to infer how accurate and precise you think your estimate will be. 9.2.1.2 Machine learning The goal of machine learning is to use a data set to create a prediction function that can be used to predict a new value of the outcome on the basis of a set of input features. The central dogma of machine learning is similar to the central dogma of statistics in the sense that you are performing a statistical calculation the basis of some observed data. However, the goal is ultimately here to create an algorithm that will make predictions for new data values. This prediction will ultimately also be subject to potential artifacts, sampling bias, and noise. However, the target is creating an accurate prediction function and typically the error is measured by how close the predictions are to the truth. 9.2.1.3 Internal “study design” It is important to know your statistical analysis goal in advance. It has implications for most of the steps in your analysis. For example, with statistical inference you may choose more parsimonious models that are easier to understand and interpret; whereas for machine learning you may choose more sophisticated non-linear models if they improve prediction accuracy. One of the most important distinctions occurs right at the beginning of the analysis. If you are performing an inferential analysis you typically analyze the entire data set together at once, with the goal of making an estimate of uncertainty using the whole sample. When performing a statistical prediction or machine learning analysis you typically separate the data into training, testing, and validation sets so that you can build the statistical prediction in the training set, tune it in the testing set, and get an independent estimate of how well it works in the validation set. 9.2.2 Form an analysis plan When you perform a statistical analysis you should start with a plan. This plan can be as simple as a list of steps and models you plan to perform or it can be as complicated as a complete set of code. But the important part is that you should write your plan down in advance. You can write down a very high level sketch of your analysis before you even begin exploration and a second, more thorough, analysis plan after you complete exploration. This is a particularly important step to complete if you have a complex, or high dimensional data set, if you have a vested interest or motivated collaborators who want the data to say something in particular, or if you are worried about over interpreting your data. The purpose of the analysis plan is to help you document all the post-hoc decisions that you made when analyzing your data. This documentation will allow both you and your collaborators or bosses to evaluate whether the decisions may lead to bias in your analysis. “Researcher degrees of freedom” is a term that was invented to refer to all the ways that you, as the analyst, can manipulate or change your analysis plan to try to reach a conclusion you already wanted in advance. The title of their paper included the statement: …Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant They were specifically referring to statistical significance in the sense of identifying results with a P-value less than 0.05 as statistically significant. However, this same type of flexibility can lead to over-optimism in prediction, biased estimates, and generally incorrect analysis if they are not appropriately accounted for. So it is worth writing down an analysis plan you can compare to later when you set off to analyze any new data set. 9.2.3 Model signal When you perform your exploratory analysis you will be looking for the “signal” in the data set. What is a signal? Typically we think of signal as the relationship between one or more variables. For example if you are looking for a relationship between x and y then the “signal” here is pretty obvious. ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union In fact, in this case the data are generated from the model: \\[ y = x^3 + e\\] where \\(x \\sim N(0,1)\\) and \\(e \\sim N(0,1)\\). What we call the “signal” is the systematic relationship between \\(x\\) and \\(y\\) - so the \\(x^3\\) part of the equation above. This represents the typical relationship statisticians use to model data - they think of modeling a “surface” where the surface represents some simple function of the data with a noise term. In this case we might fit a model of the form: \\[ y_i = f(x_i) + e_i\\] where \\(y_i\\) is the \\(i\\)th data point, \\(f\\) is a function relating \\(x\\) to \\(y\\) and \\(e_i\\) represents unmodeled “noise” - which may be assumed to be random. In our simple example the function \\(f(x) = x^3\\). When performing inference - or any statistical modeling - there is a tradeoff between simplifying interpretation and trying to capture the signal as precisely as possible. On the simple side of the scale, a default reaction for most data analysts is to start with a linear model. It is often a reasonable first summary of the data. library(modelr) lm1 = lm(y ~ x, data=dat) dat = dat %&gt;% add_predictions(lm1) dat %&gt;% ggplot(aes(x = x,y=y)) + geom_point(color=&quot;grey&quot;) + geom_line(aes(x=x,y=pred),color=&quot;black&quot;) + theme_minimal() Here this doesn’t seem to capture the entire relationship between \\(x\\) and \\(y\\). But remember “all models are wrong…”. We can still think about the linear relationship between x and y even if it isn’t the perfect model for the signal. In particular, this model has the form: \\[y_i = \\beta_0 + \\beta_1 x_i + e_i\\] Where \\(\\beta_0\\) is the average value of \\(y\\) when \\(x = 0\\) and \\(\\beta_1\\) is the average increase in \\(y\\) for a one unit increase in \\(x\\). In this example we get parameters estimates for each of the terms: library(broom) ## ## Attaching package: &#39;broom&#39; ## The following object is masked from &#39;package:modelr&#39;: ## ## bootstrap lm1 %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.0173 0.0888 0.195 8.46e- 1 ## 2 x 3.28 0.0891 36.8 3.83e-188 So far so good. Remember, this line doesn’t perfectly represent the signal between \\(x\\) and \\(y\\). But it does represent our best estimate of the linear trend. Let’s imagine that we could sample an infinite number of points. With infinite data you might get something that looks (approximately) like this: ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select This limiting case of infinite data is called the “super population”. You can think of applying the same linear regression model to this infinite super population of data. If you do, the \\(\\beta_1\\) you get is the “parameter” you are estimating. The coefficient \\(\\beta_1\\) when fit to this infinite data is the exact value we are trying to estimate with our regression model. This seems pretty convoluted. In this case we can tell what the signal is exactly. So why think about the super population and define the parameter estimate as the “linear trend we would have observed in an infinite sample of data”? The reason is that while this case is simple and we know the true signal, we rarely will. So we are almost always using a summary of the data calculated with some simplified model. It is useful to think about what that model is trying to capture and what the result would be if we applied that summary to a data set where we could perfectly capture the same trend. The advantage of this approach is simple. If we are estimating the linear trend in this data, it does exist in the limit and when we get a parameter estimate we can interpret it easily: is the average change in \\(y\\) values for a one unit change in \\(x\\) values. An alternative approach to capturing the “signal” is less focused on attribution of the signal to a particular trend and more focused on capturing the most accurate representation we can with our simplified model. In that case we might fit a smooth function to the data. There are a number of ways to fit a smoother but one example is to fit a generalized additive model. These models break what might be a complicated function of multiple variables: \\[ y = f(x_1,x_2,...,x_n) + e\\] and simplify them by assuming the terms are additive: \\[ y = f(x_1) + f(x_2) + ...+ f(x_n) + e\\] Where the \\(f()\\) functions can be as complicated or as simple as we like. We can fit this kind of model using the gam R package. library(mgcv) gam1 = gam(y ~ s(x),data=dat) dat %&gt;% mutate(smooth = gam1$fitted) %&gt;% ggplot(aes(x, y)) + geom_point(size = 3, alpha = .5, color = &quot;grey&quot;) + geom_line(aes(x,smooth), color=&quot;red&quot;) Here we “capture” the signal much better. But the resulting interpretation is a little bit harder. We have a smooth term (the \\(f()\\) function), with an estimated number of degrees of freedom (a term describing how flexible the \\(f()\\) function is). gam1 %&gt;% tidy() ## # A tibble: 1 x 5 ## term edf ref.df statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 s(x) 8.86 8.99 1988. 0 This term doesn’t have a neat interpretation of “a one unit change in x leads to a change of \\(\\beta_1\\) in y”. Instead, we have to carefully describe the function and interpret what it means for the data. Typically, when modeling even complicated data sets, it makes sense to start out with the simple linear regression models. Fitting these models does not imply you think that the signal has a linear form - you are simply calculating a specific summary of the data. Then, if the simple linear models do not represent sufficiently useful summaries you can build more complicated models for the signal - carefully considering how you will interpret the resulting functions you estimate. 9.2.4 Account for artifacts We typically think about the noise in a statistical model being random. However, they don’t have to be! Let’s take a really simple, totally deterministic example and show how the signals in the data can be due to unmeasured, systematic factors. This example was borrowed from Ken Rice’s linear models class. Imagine we have some resistors that can be of one of two types - gold (whose resistance we denote \\(X\\)) and silver (whose resistance we denote \\(Z\\)). Our outcome is the total resistance \\(Y\\). In this case everything is fully deterministic. If you show the resistance of gold versus the resistance of silver you see that they exactly add. We can make this look like “data” by simply plotting the values of the total resistance vs the number of gold striped resistors (middle panel) and then remove the coloring corresponding to the silver striped resistors (right panel) you get something that looks like regression data. If you fit a regression model to this data it will give you the “right” estimate for the amount of resistance in each gold striped resistor. However, if instead we have a design where the number of gold and silver resistors are related by an unknown relationship (left panel) and perform the same process by plotting the total resistance versus the gold stripes (middle panel), and removing the silver stripe information (right panel) - it still looks like a regression model! But the underlying, missed variable here causes big problems - since the slope of the regression model is now too big and it “looks” like the gold resistance stripes have a larger resistance on average than they do: Remember, nothing was random here! The “data” are totally deterministic. But it helps to show how regression models can produce inaccurate results when you have artifacts in your data. This is an example of a confounder but as you will know from your epidemiology classes there are a number of ways that unmeasured variables can have an impact. When you model your data there are two ways you can look for artifacts: Look for relationships among the measured variables that might impact your results. The best way to do this is to plot the outcome you care about versus the predictor you care about. Then color the points by other variables and look for patterns. For example if you see something like this: library(tibble) library(ggplot2) library(dplyr) set.seed(1234) dat2 = tibble(x = rnorm(1000), z = rnorm(1000,mean=rep(c(1,-1),each=500)), y = rnorm(1000,mean=x+z)) dat2 %&gt;% ggplot(aes(x=x,y=y)) + geom_point() + theme_minimal() It might seem like your regression model is fine. But if you color by the third variable: library(viridis) ## Loading required package: viridisLite dat2 %&gt;% ggplot(aes(x=x,y=y,color=z)) + geom_point() + scale_color_viridis() + theme_minimal() You can see all of the values at the top of the plot have high levels of z and all the values at the bottom have low levels of z. This suggests z is an important variable to consider when you are modeling. Look for the “unknown unknowns”. This is a bit trickier, since you don’t have an obvious pattern to look out for. For example, in the data above, if you didn’t know z, the plot would look like there was a pretty reasonable linear regression fit to the relationship between y and z. However, there are a few tips you can use to look for “unknown unknowns”: For any variable measured over time, plot that variable versus time, day of the week and month of the year to look for clustering or seasonal patterns. Look at pairs plots of variables and look for non-random patterns or groupings within the pairs of variables. Look for any variable that might have a multi-modal distribution - especially if there isn’t another variable in your data set that might explain the groups. If you have high-dimensional data look at dimension reduction techniques (we will discuss this more later) Plot residuals from regression models and look for patterns in the residuals - Look for differences in variability among groups or across continuous variables For example, if we plot the residual histogram from the simple regression of y on x, you might start to notice that it looks a little strange - with something going on that might indicate a missing variable: lm2 = lm(y ~ x, data=dat2) dat2 %&gt;% add_residuals(lm2) %&gt;% ggplot(aes(x=resid)) + geom_histogram() + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The danger of unknown unknowns is extremely real. There have been multiple papers that have had to be retracted because very good data analysts simply missed a variable that was important. For example - there was a famous paper linking genetic variation to human longeviity - it was ultimately retracted because the sample processing time appeared to be a major confounder of the analysis. However, this was a variable not necessarily known to the authors at the time of writing the paper: In my experience one of the best ways to detect a missing variable is to be suspicious when signals I am looking for are “too strong”. By too strong, I mean if I find a signal in a data set where the estimate is double or more the next closest I have seen or seems pretty unrealistc - then I immediately get suspicious there may be a hidden relationshp driving that signal. 9.2.5 Build models up sequentially When building statistical models for practical problems, the data is usually already pretty complicated. It helps to not over complicate your analysis unless it is necessary. It is a good idea to start with a univariate model or the simplest model you can imagine and then sequentially build up layers of variables. When you perform an analysis like this you shouldn’t perform inference on each of these sub-models, rather it should be viewed as an exploration of the data to create a regression model that best explains signal, systematic noise or artifacts, and random noise. There is a very nice chapter on sequential model building in the R for Data Science Book 9.2.6 Model uncertainty Uncertainty modeling depends on the type of modeling you are doing. Here we will break down the potential error measures used for statistical inference and statistical prediction. For other applications (such as causal inference) there are often similar error measures used but the interpretations vary somewhat. 9.2.6.1 Uncertainty for inference Recall that the central dogma of statistical inference is to say something about the population based on the sample you have taken. Typically by the time you are down to modeling uncertainty you have accounted for the main sources of signal as well as systematic artifacts. The remaining sources of noise may be due to unmodeled variables or due to “random” noise. Regardless, the purpose of uncertainty modeling is to tell you something about what is going on in the population. There are two types of uncertainty measures typically used for inference: Estimation uncertainty - are measures of uncertainty designed to tell us something about the values of parameters we care about. Typical examples are confidence or credible intervals for parameters of interest. The goal here is to say something about the most likely range for a parameter. It is highly recommended to include these, particularly if you only care about one or a small number of parameters. Decision uncertainty - are measures that help us decide whether a signal is “real”. Examples include p-values, Bayes factors, and posterior probabilities for decision rules. These measures have fallen out of favor, largely because of Goodhart’s law: When a measure becomes a target, it ceases to be a good measure However, I still believe these types of measures can be useful largely because it is often important to make decisions about whether you think an effect is “real” or not when reporting your results. Regardless of the uncertainty measure you use, the goal is to say something about the population at large. However, we are working with a sample so all of these uncertainty measures should be understood in that context. In particular a few things to keep in mind when reporting uncertainty measures: All uncertainty measures are sample size dependent. If you have a huge sample size your p-values will be tiny even for negligible signals - you will also have ultra tiny confidence intervals. This also applies to Bayesian measures of uncertainty. All uncertainty measures assume that there is no hidden systematic variation. If there is such variation and you have missed it in the modeling step, the uncertainty measures will no longer be accurately calibrated. Uncertainty measures, like estimates, vary from sample to sample. This means that its worth considering in your interpretation that the uncertainty estimate is dependent on the sample and may vary from sample to sample. 9.2.6.2 Uncertainty for machine learning The central problem in machine learning can be thus written very simply as minimizing a distance metric. Let \\(\\hat{Y} = f(\\vec{X})\\) then our goal is to minimize the distance from our estimated function of the predictors to the actual value. \\[d(Y - f(\\vec{X}))\\] \\(d(\\cdot)\\) could be something as simple as the mean squared distance or something much more complex. Perhaps the most important part of any machine learning problem is defining what success looks like. This choice very much depends on the application and what you are trying to do. For example, when we talk about the goal in ML we are usually talking about the error rate we want to minimize and how small we want to make it. Consider for each observation we have an outcome \\(y\\) and a set of features \\(\\vec{x}\\). Our goal is to create a function \\(\\hat{y} = \\hat{f}(\\vec{x})\\) such that the distance, \\(d(y,\\hat{f}(\\vec{x}))\\), between the observed and the predicted \\(y\\) is minimized. The two most common distances that show up in machine learning (and the ones you’ll always be using if you don’t change the defaults!) are: Root mean squared error (RMSE) - this is the most common error measure for regression (read: continuous outcome) problems. \\(d(y,\\hat{f}(\\vec{x})) = \\sqrt{\\sum_i \\left(y_i-\\hat{f}(\\vec{x}_i)\\right)^2}\\) Accuracy - this is the most common error measure for classification (read: factor outcomes) problems. \\(d(y,\\hat{f}(\\vec{x})) = \\sum_i 1\\left(y=\\hat{f}(\\vec{x})\\right)\\) Here we are going to use simple accuracy and say that anything better than guessing is “good enough”. But in general there are a number of other potential error measures: Here are a few examples of how they might be relevant. Predictive value of a positive - in classification if one group is much less frequent than another, then even high sensitivity/high specificity tests can produce lots of false positives (the classic example is cancer screening, but very relevant for any screening problem). Mean absolute error - in regression sometimes you want your error to be less sensitive to a few outliers (this might be true in predicting highly skewed outcomes like income or property values) and MAE can reduce the influence of those outliers. Specificity - when the cost of a false negative is really high compared to a false positive and you never want to miss any negatives (say for example missing a person standing in front of a self driving car) In general you need to spend a good amount of time thinking about what the goal is, what the tradeoff of various different errors are and then build that into your model. 9.2.7 Compare to your analysis plan When you perform an analysis you will invariably have to make a large number of choices that weren’t in your analysis plan. There is understandably a lot of attention on multiple testing problems. Usually when people talk about multiple testing, they are focused on the documented tests included in an analysis. But there are a few levels of potential multiple testing in a data set: Multiple testing - Considering multiple hypotheses. Often refers to a fixed set of hypotheses. Garden of forking paths - Considering many analysis decisions, often without quantifying how many decisions are made P-hacking - Considering many analysis decisions, with a metric in mind and trying to optimize that value Quantifying multiple testing where your models are documented is pretty straightforward. You can use techniques like the Bonferroni correction or the Benjamini-Hochberg method for false discovery rate control. However, this often represents the smallest part of the variation in decision uncertainty quantification. Generally there is an iceberg of decisions that have been made before a p-value is calculated that could impact the uncertainty measure: The best way to combat this issue is to document the choices you made throughout the analysis and in particular, compare them to your original analysis plan. If you include this documentation with justifications for your modeling decisions it will both provide a way for you to hold yourself responsible as well as to help others evaluate your data analytic process. 9.2.8 Understand incentives Once you begin to put things in concrete terms the incentives behind a data analysis typically become much more clear. For example: You may want to find a result to make a paper publishable Your collaborator may be looking for a result to make a paper publishable Your boss may want you to show you can predict sales to justify the data science team Your analysis may underly important decisions that are already being made or have been made It is difficult to navigate these as a data analyst.At a basic level it is important to be aware what these incentives are and have a plan for navigating those incentives while accurately representing what is going on in the data. As we discussed earlier, you can put together a ‘successful’ data analysis by addressing these incentives directly. However, it is better to accuratley represent what is in the data since the consequences - paper retractions, misallocated resources, or poor decisions - can result. We will discuss more the ways this plays into data analytic relationships later in the course. 9.3 Additional Resources Roger Peng’s post on partioning variation in a data set Rafa Irizarry’s Lecture on Regression Models Jeff Leek’s lecture on multiple testing Model Buidling from the R For Data Science Book 9.4 Homework Template Repo: https://github.com/advdatasci/homework8 Repo Name: homework8-ind-yourgithubusername Pull Date: 2020/10/26 9:00AM Baltimore Time "],["week-9.html", "10 Week 9 10.1 Week 9 Learning objectives 10.2 Story Telling in Data Analysis 10.3 The journey to interesting and true 10.4 Data Analysis Papers 10.5 A few matters of form 10.6 Additional Resources 10.7 Homework", " 10 Week 9 10.1 Week 9 Learning objectives At the end of this lesson you will be able to: Understand the central dramatic argument, structure, and form of a data analysis Use “therefore” and “but” as linkers in your analysis instead of “and then” Be able to define the journeys to correct and interesting in a data analysis Know which parts of your analysis appear in each section of a data analytic paper 10.2 Story Telling in Data Analysis As we have discussed earlier in the class, a data analysis always involves a communication between the analyst and their audience. A big part of that is telling the story of a data analysis. At the beginning of an analysis you will start out with a question and then through exploration you may chase dozens of leads and create hundreds of plots. But a key to success in data analysis is taking these disparate analyses and weaving them into a coherent story. 10.2.1 The Central Dramatic Argument The story or “arc” of a data analysis tells the central dramatic argument of the data and evidence. You can think of this a bit like “dimension reduction” for your analytic results. You are trying to summarize the key points and put them in order so that your audience doesn’t have to follow all of the potential dead ends you may have chased when performing your analysis. Sometimes the dramatic argument is obvious - its right in the title like in this paper by Roger and his colleagues. But it isn’t always as clear what the central dramatic argument is in some papers. For example, this paper the central dramatic argument appears in the abstract: “For point processes, effective use of such residual analysis makes it possible to find features of the data set that are not captured in the model.” Depending on your writing style different approaches can be successful to creating a central dramatic arc. But it is critical to know that arc and follow it with your data analysis. It is also important to keep in mind that often we can agree on the data but disagree on what they tell us. Your job as the analyst, is to organize the evidence, present it clearly and help the audience to understand the arc through the data you are trying to present. 10.2.2 Thematic structure and causality Most people who come to data analysis do so through a technical or empirical discipline like epidemiology or biostatistics. We have often have extensive and deep technical training but have spent less time learning about writing, exposition and telling stories. However, being good at writing can dramatically improve the impact of even your most technical writing. Since writing isn’t often a component of technical curricula we need to look outside to find key ideas. In Roger’s lecture he points to this great quote from the Scriptnotes podcast: “The purpose of the story is to take a character from ignorance of the truth of the theme to embodiment of theme through action.” -Craig Mazin, Scriptnotes Podcast, Ep. 403 In that same episode they discuss the key thematic structure of a movie, which has some lessons for us as data analysts: The important thing about this structure is that there is a natural reason each step follows from the next and they flow from one to the other. A really nice simplification of this idea was proposed by the creators of the crass, but wildly successfull South Park television series - Matt Stone and Trey Parker. They lay out their simple model for creating narrative in this Youtube video (a bit of strong language - no surprise) They discuss how the narrative ideas in a story should be linked using the words “therefore” and “but”. They also point out that stories where the narrative ideas are separated by “and then” are often much less coherent and harder to follow. The same rules apply in data analysis! When you are putting together the pieces of your analysis you should be thinking about causality between the beats of your story. For example this narrative structure takes the reader along with you on your path through the analysis: “In our exploratory analysis we observed that height and weight were correlated, therefore we fit a linear model, but after plotting the residuals and coloring by age of the participant we realized it was a confounder, therefore we included it as an adjustment factor.” However, a typical “and then” analysis looks more like a catalog of things you tried without a coherent picture. “We made a plot of height versus weight and then we made a plot of height versus age and then we fit a model that related height to weight adjusting for age.” When writing your analysis, think carefully about how to construct the narrative so it flows naturally from step to step rather than feeling like a catalog. 10.2.3 Format The format of the presentation should match nature of the story being told. In some cases the format will be more formally structured, for example when reporting an analysis for a journal the structure may be dictated by the requirements of that journal. Other times, it may be more informal and free flowing. For example you may be reporting your analysis in a variety of formats: Blog post Report Paper Email Interactive presentation Each will have its own set of requirements. For blog posts it often makes sense to restrict the dramatic arc to a single, coherent thought process. An email may be only a few lines of writing with a single figure meant to convey just one point. More in depth interactive reporting like The Follower Factory may include interactive elements that guide the reader through the story. Below we will focus on the format for a data analytic scientific paper, but this format will depend heavily on the outlet you plan to ultimately pursue for communicating your results to your audience. 10.2.4 Trust and Belief An audience can trust you did a good analysis, but still not believe your central dramatic argument. Trust - I accept the analysis, the data were analyzed properly and thoroughly - Trust is particular to the analysis and the person doing the analysis Belief - I believe the conclusion / central argument, is true - Depends on context, previous work, factors outside the analysis Regardless, the story you tell is part of a larger negotiation with the audience to accept the analysis. Depending on your relationship to the audience you may need to alter the way that you tell your story. If you are well known in an area and have a track record of solid analyses - you may be able to streamline your reporting or move significant pieces of the analysis to supplementary documents. But if you are new to an audience, or working outside your standard field, you will need to be more careful about documenting your choices. In his piece on trustworthy data analysis Roger discusses how there are three parts to a data analysis: The parts you did and presented The parts you did but did not present The parts you did not do As he puts it: “As an analyst it might be useful to think of what are the things that will fall into components A, B, and C. In particular, how one thinks about the three components will likely depend on the audience to which the presentation is being made. In fact, the “presentation” may range from sending a simple email, to delivering a class lecture, or a keynote talk. The manner in which you present the results of an analysis is part of the analysis and will play a large role in determining the success of the analysis. If you are unfamiliar with the audience, or believe they are unfamiliar with you, you may need to place more elements in components A (the presentation), and perhaps talk a little faster. But if you already have a long-term relationship with the audience, a quick summary (with lots of things placed into component B) may be enough.\" 10.3 The journey to interesting and true In her outstanding JSM 2020 talk Lucy D’Agostion McGowan makes a two by two table of the types of data analysis you can observe. On one axis is whether the analysis is interesting or not and on the other is whether it is true or not. Untrue and uninteresting analyses are not usually worth that much interest. The other three quandrants are worth discussing briefly. Interesting and true analyses are what we are typically trying to create as data analysts. They both tell a story people care about and they present the data fairly. Not interesting and true analyses are probably the most common, they occur when you present the data fairly, tell the best story you can, but there isn’t a compelling story behind the data. These are ok! They should be reported more often than they are. Interesting and untrue unfortunately these types of data analyses can be very popular and have extremely negative consequences. They are the type of analysis you want to avoid at all costs. Lucy points out that there are two separate journeys we take when performing and communicating a data analysis. The first is the journey to true. This involves not only that the analysis is technically correct, but also that it is correctly disseminated, marketed, and interpreted by an audience. One important point to keep in mind is that you don’t always control some of these steps! A great example is a Youtube video Lucy and I made for TedEd, trying to communicate how to avoid being misled by scientific headlines. When the video came out on Youtube they used the title, “This one weird trick will help you spot Clickbait” and had “Clickbait” written in gigantic font on the video: Needless to say, this approach to dissemination and marketing reduced the veracity of our argument and made it harder for people to believe what we were saying! The journey to interesting is also a multi-step process which includes, picking the right format, length, complexity, focus, visuals, and marketing. These components depend on the forum, the audience you are communicating with, and the goal of your data analysis. When you put these ideas together you get the full journey to an interesting and true analysis: One reason that these analyses can be rare is that it really takes a lot of components working together to influence the results of an analysis. It is a lot to keep track of. But improving each of these components a bit has multiplicative effects on the success of your data analysis. 10.4 Data Analysis Papers While there are a number of formats for producing data analyses and sharing them, in advanced data science our focus will be on producing data analytic papers. While this is a somewhat academic-flavored view of data analytic products, it is also extremely common to be asked to produce data analytic reports in industry and government, which will share many of the characteristics of good data analytic papers. 10.4.1 How do you know when to start writing? Sometimes this is an easy question to answer. If you started with a very concrete question at the beginning then once you have done enough analysis to convince yourself that you have the answer to the question. If the answer to the question is interesting/surprising then it is time to stop and write. If the answer isn’t interesting/surprising but you started with a concrete question it is also time to stop and write. But things often get more tricky with this type of paper as most journals when reviewing papers filter for “interest” so sometimes a paper without a really “big” result will be harder to publish. This is ok!! Even though it may take longer to publish the paper, it is important to publish even results that aren’t surprising/novel. It is much better that you come to an answer you are comfortable with and you go through a little pain trying to get it published than you keep pushing until you get an “interesting” result, which may or may not be justifiable (see the journey above). If you started with a question that wasn’t so concrete then it gets a little trickier. The basic idea here is that you have convinced yourself you have a result that is worth reporting. Usually this takes the form of between 1 and 5 figures that show a coherent story that you could explain to someone in your field. We will talk more about publication quality figures next week. For the first paper you will probably get a lot of help from an advisor or mentor on when to stop. But in general one thing you should be working on in graduate school is your own internal timer that tells you, “ok we have done enough, time to write this up”. I found this one of the hardest things to learn in graduate school, but it is a critical skill in both academics and industry. There are rarely deadlines for paper writing (unless you are submitting to CS conferences) so it will eventually be up to you when to start writing. If you don’t have a good clock, this can really slow down your ability to get things published or complete projects at work and get promoted. One good principle to keep in mind is “the perfect is the enemy of the very good” Another one is that a published paper in a respectable journal beats a paper you just never submit because you want to get it into the “best” journal. David Robinson really accurately describes the importance of completing projects and putting them out: “Things that are still on your computer are approximately useless.” -@drob #eUSR #eUSR2017 pic.twitter.com/nS3IBiRHBn — Amelia McNamara (@AmeliaMN) November 3, 2017 10.4.2 Structure A scientific paper can be distilled into four parts: A set of methodologies A description of data A set of results A set of claims When you (or anyone else) writes a paper the goal is to communicate clearly items 1-3 so that they can justify the set of claims you are making. Before you can even write down 4 you have to do 1-3. So that is where you start when writing a paper. This paper in PLoS Computational Biology lays out some simple rules for writing papers - these rules apply across many fields, but are really useful to think about when writing data analytic papers. In particular I really like this visual structure of a data analytic paper: That helps to break down how each section works together. In the remainder of this lesson we will focus on some of the most important parts of a data analytic paper. These components should be included 10.4.3 Titles Should be very short, declarative, and should state the main result. Example, “A new method for sequencing data that shows how to cure cancer”. Here you want to make sure people will read the paper without overselling your results - this is a delicate balance. Unless you have a really good reason not to, the dramatic argument of your analysis should be the title. It is important to keep in mind that titles are way more important than you think; even if you think they are important. I read at most a couple of papers a week at this stage. But I read hundreds of paper titles. Only those that really hook me or are really important for my current research will get me to read further. 10.4.4 Abstracts If I read hundreds of titles a week I might read a dozen paper abstracts. Depending on where you are submitting your paper abstracts may have pretty strict requirements and they vary quite a bit. For example consider the abstract structures at JAMA versus something like Nature - they are very different! I do think that the Nature structure provides a good general purpose approach to writing abstracts when a journal doesn’t have concrete requirements. Lead with a widely accessible description of the area (this can be skipped for more specialized journal) A sentence or two of background information about why the problem is important 3. A sentence or two describing the key question A sentence stating your key dramatic argument A few sentences putting this result in context and providing broader perspective Geting this much information into an abstract is a skill! It takes practice and is worth writing a much longer form of what you want to say and cutting it down. https://plos.org/resource/how-to-write-a-great-abstract/ http://www.cbs.umn.edu/sites/default/files/public/downloads/Annotated_Nature_abstract.pdf 10.4.5 Introductions An introduction should be a more lengthy (1-3 pages) explanation of the problem you are solving, why people should care, and how you are solving it. Always start with the big picture! Provide the audience with enough background that they understand why you are tackling this problem and what others have done. Another important structural element is that introductions should go from general to specific. So you should start by putting the idea in its general context, e.g. “cancer is important to study” and then narrow down to the problem you are addressing specifically, e.g. “measuring RNA abundance of genes helps us build biomarkers for cancer”. Here you also review what other people have done in the area. The most critical thing is never underestimate how little people know or care about what you are working on. It is your job to explain to them why they should. 10.4.6 Results Results and methods may appear in either order, depending on the journal you are submitting to and the audience you are addressing. The key to a really solid results section is the combination of a coherent set of 1-5 figures that make up the “beats” of your analysis and then using the narrative connections of “therefore” and “but” to link these results together clearly. One of the “beats” may be a statistical model. It is good to try to condense your reported analysis to a small number of statistical models (one is ideal!) so that the reader doesn’t have to keep up with which model you are fitting and how it has changed from analysis to analysis. If you must switch models, make sure to specify them clearly with numbers or abbreviations so readers can follow which model you are discussing at any given time. A common order for results sections is: Describing the experimental design Briefly describing the data set Decribing your main analyses and conclusions Summarizing your arc and narrative Do not report every analysis you performed! Many analyses, particularly exploratory analyses, will not be useful to explaining your result and interpretation. Before including an analysis in the final report, ask whether it contributes to the story or explains a crucial fact about the data set that can’t be left out. When describing the experimental design, explain where the data came from, how they were collected, and relevant information about the technologies and systems used to collect the data briefly. You will expand on this in the methods section. When describing the data set explain what processing you did to the data, and the tidy data you produced. It is common to lead with a table summarizing the variables in the tidy data set, including sample sizes, number of variables, averages and variances or standard deviations for each variable. This component of an analysis is critical to identify data versioning issues. You will typically be referring to the methods section of your paper for more details, so you may leave out mathematical detail when writing your results section. Each of your figures should convey ideally a single, coherent point and they should be linked together logically into an arc of a story. For each parameter of interest report an estimate and interpretation on the scale of interest. When reporting an estimate do not say that we estimated a=3, instead report that we estimated a change of 3 pounds in weight for one inch in height. For each parameter report a measure of uncertainty on the scientific scaleFor every estimate you report you should report a measure of uncertainty on the scale of interest. Report that a 95% confidence interval for the estimated change in weight for a unit change in height is 1 and 5 inches. Typical measures of uncertainty are standard deviations, confidence intervals, or credible intervals. You should also report potential problems with the analysis. If you fit a model and you observe that there may be missing data, or that outliers may be driving a particular estimate, report that as part of the results. 10.4.7 Methods sections In the methods section you will go into more detail about all of the components of your analysis. Here you are focused on building trust with the audience. You should provide the full details of your: Experimental design - including any potential flaws in your design (sampling bias, etc.) Data preprocessing - ideally both describing these steps in mathematical language in the text and through code attached as supplementary material. Statistical models - you should write the full mathematical description of each model you use. Depending on the length of these equations you may choose to use symbols and letters, or words to describe parameters. Words are often easier to follow, but only work for shorter equations. Summary statistics calculation - what statistics did you calculate, what were the sample sizes, which models did you use for which statistics and so forth. Every model must be mathematically specified. This model may be specified in the main text of the writing if the audience is statistical or in an appendix if the audience is non-technical. Modeling procedures should be completely specified using equations or algorithms with explicit definitions of each input and output. Each term and index in the equation should be explained in plain language. When possible, use letters and symbols that are abbreviations of the variables in your model. For example, if modeling the relationship between weight and height you might write the model W = a + b H + e and then explain that W stands for weight, a is the weight for a person with 0 height, b is the increase in weight units for a person with one additional height unit, H is the height of the person, and e is measurement error or noise. Every statistical or machine learning method should be referenced. When using models it is important to give credit to the person who developed them. It is also important so that the reader can go back to the original source of the model and understand it. There is a convention that some statistical methods are not cited like maximum likelihood or least squares - but at minimum a reference to a textbook should be made when using even common techniques. 10.4.8 Conclusions and discussion Remember “tell them what you are going to tell them, tell them, then tell them what you told them”. In your introduction you explained what you were going to tell your audience, in the results and methods you told them, and in the conclusions and discussion you should start with a summary of what you told them. It is then a good idea to highlight potential limitations of your analysis. Be honest! It builds trust when an analysis highlights potential issues for you, so that you don’t have to discover them yourself. After you have explained the limitations you can conclude with the strengths of your study and the potential next steps for your audience. This is often “more research” in academics, but may be a call to action for your audience on the basis of what you have found. The more succicint this statement is, the better, since it is the last thing your audience will hear from you. 10.4.9 Supplementary material Often your writing will be space-constrained, sometimes severely. In those cases, you may include additional details of your methods or results in supplementary information. Supplementary information should be structured to match the narrative flow of your text and should be organized into clear sections with labels that match the corresponding sections in the text. Every supplementary section should be referred to concretely by something in the main text. If it is not, then it should probably be removed from the supplementary material. A real danger of the supplementary material is to stuff the “and then” part of your analysis into the supplement - creating a complete catalogue of everything you did. The supplementary material should be streamlined to include only the componets that substantiate your main text. 10.5 A few matters of form Avoid jargon at all costs and use shorter words when possible Never include computer code directly in your write up Report estimates followed by parentheses. The increase is 5.3 units (95% CI: 3.1, 4.3 units) When reporting P-values do not report numbers below machine precision. P-values less than 2 x 10e-16 are generally below machine precision and inaccurate. Reporting a P-value of 1.35 x 10e-25 is effectively reporting a P-value of 0 and caution should be urged. A common approach is to report censored P-values such as P &lt; 1 x 10e-8. When reporting permutation P-values avoid reporting a value of zero. P-values should be calculated as (K + 1)/(B + 1) where B is the number of permutations and K is the number of times the null statistic is more extreme than the upper bound. Do not report estimates with over-precision. If measurements are only accurate to the tenths digit, do not report an estimate of 6.8932 When programing variable names should be lower case, with words separated by underscores, and as explicit as possible in data frames you are analyzing. The date of visiting a website might be named date_of_visit. In written analysis variable names should always be reported in plain language, not as variable names. The date of visiting a website would be described as “the date of visit variable”. 10.6 Additional Resources Lucy D’Agostino McGowan’s 2020 JSM Talk Roger Peng’s Lecture on Storytelling in Data Analysis Jeff Leek’s Guide to Writing your First Paper 10 Simple Rules for Structuring Papers The Write Stuff 10.7 Homework Template Repo: https://github.com/advdatasci/homework9 Repo Name: homework9-ind-yourgithubusername Pull Date: 2020/11/02 9:00AM Baltimore Time "],["week-10.html", "11 Week 10 11.1 Week 10 Learning objectives 11.2 Exploratoy vs expository 11.3 Creating Expository Graphs 11.4 Creating Expository Tables 11.5 Scientific graphics vs infographics 11.6 Additional Resources 11.7 Homework", " 11 Week 10 11.1 Week 10 Learning objectives At the end of this lesson you will be able to: Distinguish exploratory from expository figures Apply the steps to create expository, publication quality, figures Apply the steps to create publication quality tables 11.2 Exploratoy vs expository When you start off analyzing a data set you almost always start with exploratory analysis. This includes quick tables, plots, and graphs for you to explore data characteristics and get a handle on what is going on with the data. Exploratory graphs and tables should be made exceptionally quickly and you should make a ton of them! The only audience of an exploratory graphic is you so you shouldn’t worry about things like fonts, axis labels, titles, legends, and labeling. Similarly, exploratory tables may be as simple as a quick dplyr::count command and shouldn’t be cleaned up and organized. Recall the parable of the blind men and the elephant from week 2 of the course: Expository graphs and tables, on the other hand, are meant to be shared with an audience. Rather than trying to discover structure in the data for you personally, you are trying to communicate what you have found to your audience. The key thing to keep in mind is that: People spend way less time reading your work than you think Think about the last couple of papers you read. You probably quickly skimmed to start, maybe did a deeper read if it was part of an assignment or if you needed to use the results in your research. But for most of the papers you read, you are likely glancing them over pretty quickly. What does this mean? It means all of your figures should be designed to convey their message quickly, ideally in a glance. Furthermore, your figures should ideally be self-contained so that if a person only looks at the figures in your work, they can still follow the story you were trying to tell. Similarly, if you are making a table to communicate a result to your audience, you should think about what you are trying to communicate and how to highlight the message you want your audience to take home. A good expository graph has clear, large axis labels; color and size carefully used for communication; minimal abbreviations in axis labels and legends; shows the data; have figure captions with a declarative summary statement and self-sufficient labeling; and highlights take home messages with titles or annotation. A good expository table is simplified as much as possible; does not use excess digits in numbers; is not overly dense and complicated; uses color for highlighting key messages; have table captions with a declarative summary statement and self-sufficient labeling; and labels take home messages with a title. One thing to keep in mind is that there are multiple different plotting and tabling systems out there (both in R and in other languages!). One thing to keep in mind is that regardless of which system you use, making a good expository graph will typically take a lot of tweaking. For example this nice bar graph from the Tufte in R site: Takes a significant amount of code to create in base R: library(psych) d &lt;- colMeans(msq[,c(2,7,34,36,42,43,46,55,68)], na.rm = T)*10 barplot(d, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, ylab=&quot;&quot;, border=F, width=c(.35), space=1.8) axis(1, at=(1:length(d))-.26, labels=names(d), tick=F, family=&quot;serif&quot;) axis(2, at=seq(1, 5, 1), las=2, tick=F, family=&quot;serif&quot;) abline(h=seq(1, 5, 1), col=&quot;white&quot;, lwd=3) abline(h=0, col=&quot;gray&quot;, lwd=2) text(min(d)/2, max(d)/1.2, pos = 4, family=&quot;serif&quot;, &quot;Average scores\\non negative emotion traits\\nfrom 3896 participants\\n(Watson et al., 1988)&quot;) But it also takes a significant amount of code in lattice: library(lattice) library(psych) d &lt;- colMeans(msq[,c(2,7,34,36,42,43,46,55,68)],na.rm = T)*10 barchart(sort(d), xlab=&quot;&quot;, ylab=&quot;&quot;, col = &quot;grey&quot;, origin=1, border = &quot;transparent&quot;, box.ratio=0.5, panel = function(x,y,...) { panel.barchart(x,y,...) panel.abline(v=seq(1,6,1), col=&quot;white&quot;, lwd=3)}, par.settings = list(axis.line = list(col = &quot;transparent&quot;))) ltext(current.panel.limits()$xlim[2]-50, adj=1, current.panel.limits()$ylim[1]-100, &quot;Average scores\\non negative emotion traits\\nfrom 3896 participants\\n(Watson et al., 1988)&quot;) or in the ggplot2 plotting system: library(ggplot2) library(ggthemes) library(psych) library(reshape2) d &lt;- melt(colMeans(msq[,c(2,7,34,36,42,43,46,55,68)],na.rm = T)*10) d$trait &lt;- rownames(d) ggplot(d, aes(x=trait, y=value)) + theme_tufte(base_size=14, ticks=F) + geom_bar(width=0.25, fill=&quot;gray&quot;, stat = &quot;identity&quot;) + theme(axis.title=element_blank()) + scale_y_continuous(breaks=seq(1, 5, 1)) + geom_hline(yintercept=seq(1, 5, 1), col=&quot;white&quot;, lwd=1) + annotate(&quot;text&quot;, x = 3.5, y = 5, adj=1, family=&quot;serif&quot;, label = c(&quot;Average scores\\non negative emotion traits from 3896 participants\\n(Watson et al., 1988)&quot;)) The important thing to keep in mind is that expository graphs and tables will be significantly more work than exploratory graphs. They also typically take quite a bit of Googling to make sure you get them to look just right so they will stand out and be easily understood. In the remainder of this lesson we will focus on taking exploratory figures and tables and turning them into expository figures and tables - highlighting some of the key tools for making these conversions. 11.3 Creating Expository Graphs For this example we will be using the awesome Palmer Penguins data set! The artwork for this data is made by the amazing [@allison_horst](https://twitter.com/allison_horst) We can start out by looking at the data library(palmerpenguins) library(magrittr) library(ggplot2) penguins ## # A tibble: 344 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torge… 39.1 18.7 181 3750 ## 2 Adelie Torge… 39.5 17.4 186 3800 ## 3 Adelie Torge… 40.3 18 195 3250 ## 4 Adelie Torge… NA NA NA NA ## 5 Adelie Torge… 36.7 19.3 193 3450 ## 6 Adelie Torge… 39.3 20.6 190 3650 ## 7 Adelie Torge… 38.9 17.8 181 3625 ## 8 Adelie Torge… 39.2 19.6 195 4675 ## 9 Adelie Torge… 34.1 18.1 193 3475 ## 10 Adelie Torge… 42 20.2 190 4250 ## # … with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; and making what would be a fairly typical exploratory graph. penguins %&gt;% ggplot(aes(x=flipper_length_mm,y=body_mass_g)) + geom_point() ## Warning: Removed 2 rows containing missing values (geom_point). This isn’t bad! It’s a pretty decent way to start and you can already see the “take home message” that increasing flipper length is associated with increasing body mass. But we can do better, let’s label the different species penguins %&gt;% ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) + geom_point() ## Warning: Removed 2 rows containing missing values (geom_point). So far so good, and here we see that the relationship is increasing regardless of the species type. This graph looks almost good enough to put in a paper, but we are missing a few key components. 11.3.1 Background color In general, less is more when it comes to background color. In his talk The glamour of graphics Will Chase points out that: White space is like garlic; take the amount you think you need, then triple it. He also penguins %&gt;% ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + theme_minimal() ## Warning: Removed 2 rows containing missing values (geom_point). 11.3.2 Axis titles and axis labels Axis titles and labels need to be big enough to read. They should include labels of units (when appropriate) and should be written in full words, not in abbreviations. If you can avoid it you should also try to make it so that your audience doesn’t have to turn their head to read your plot. We can rotate the y-axis label and increase the font on the tick marks. penguins %&gt;% ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + ylab(&quot;Body \\n Mass \\n (g)&quot;) + xlab(&quot;Flipper Length (mm)&quot;) + theme_minimal() + theme(axis.title.y = element_text(angle = 0, vjust = 0.5,size=14), axis.text = element_text(size=12)) ## Warning: Removed 2 rows containing missing values (geom_point). 11.3.3 Legends (or not!) When used, legends should have labeling in plain language that makes the graph self contained. So for example we could update the figure legend: penguins %&gt;% ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + ylab(&quot;Body \\n Mass \\n (g)&quot;) + xlab(&quot;Flipper Length (mm)&quot;) + labs(color=&quot;Penguin Species&quot;)+ theme_minimal() + theme(axis.title.y = element_text(angle = 0, vjust = 0.5,size=14), axis.text = element_text(size=12)) ## Warning: Removed 2 rows containing missing values (geom_point). One challenge here is the legend is taking up a lot of space, that makes the plot harder to read. You have a couple of solutions. One is you could move the legend inside the plot. Another is you could just label the points directly with annotation. Here I’m using the basic ggplot2 functionality, but the ggforce has some nice labeling functionality. penguins %&gt;% dplyr::filter(!is.na(flipper_length_mm)) %&gt;% ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + ylab(&quot;Body \\n Mass \\n (g)&quot;) + xlab(&quot;Flipper Length (mm)&quot;) + labs(color=&quot;Penguin Species&quot;)+ annotate( geom = &quot;curve&quot;, x = 180, y = 5000, xend = 182, yend = 3100, curvature = .3, arrow = arrow(length = unit(2, &quot;mm&quot;)) ) + geom_text(aes(x=180, y=5000, label=&quot;Adelie Penguins&quot;, color=&quot;Adelie&quot;), show_guide=F) + annotate( geom = &quot;curve&quot;, x = 215, y = 3000, xend = 198, yend = 3600, curvature = .3, arrow = arrow(length = unit(2, &quot;mm&quot;)) ) + geom_text(aes(x=215, y=3000, label=&quot;Chinstrap Penguins&quot;, color=&quot;Chinstrap&quot;), show_guide=F)+ annotate( geom = &quot;curve&quot;, x = 185, y = 6000, xend = 220, yend = 5000, curvature = .3, arrow = arrow(length = unit(2, &quot;mm&quot;)) ) + geom_text(aes(x=185, y=6000, label=&quot;Gentoo Penguins&quot;, color=&quot;Gentoo&quot;), show_guide=F)+ theme_minimal() + theme(axis.title.y = element_text(angle = 0, vjust = 0.5,size=14), axis.text = element_text(size=12), legend.position = &quot;none&quot;) ## Warning: `show_guide` has been deprecated. Please use `show.legend` instead. ## Warning: `show_guide` has been deprecated. Please use `show.legend` instead. ## Warning: `show_guide` has been deprecated. Please use `show.legend` instead. 11.3.4 Titles (with color!) This at least removes all the legend space, but I still find it pretty hard to read. I find that titles labeled with colors are much easier to follow, so something like this, which we can create with the ggtext package and a little HTML: library(ggtext) penguins %&gt;% ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + ylab(&quot;Body \\n Mass \\n (g)&quot;) + xlab(&quot;Flipper Length (mm)&quot;) + labs(color=&quot;Penguin Species&quot;, title = &quot;&lt;b&gt; Penguin body mass increases with flipper length&lt;/b&gt;&lt;br&gt; &lt;span style = &#39;font-size:10pt&#39;&gt;Penguins of three species, &lt;span style=&#39;color:#F8766D&#39;&gt;Adelie&lt;/span&gt;, &lt;span style=&#39;color:#00BA38&#39;&gt;Chinstrap &lt;/span&gt;, and &lt;span style=&#39;color:#619CFF&#39;&gt;Gentoo&lt;/span&gt;, have body mass that increase with flipper length. &lt;/span&gt;&quot;)+ theme_minimal() + theme( plot.title.position = &quot;plot&quot;, plot.title = element_textbox_simple(size=12), axis.title.y = element_text(angle = 0, vjust = 0.5,size=14), axis.text = element_text(size=12), legend.position = &quot;none&quot;) ## Warning: Removed 2 rows containing missing values (geom_point). 11.3.5 Adding a model fit You may add a smooth term or model fit to highlight key trends in the data. The ggplot2 package makes this relatively easy by using the term geom_smooth but you should be careful to make sure you understand what kind of model is being fit “under the hood”. For example here the regression model is actually fit to each penguin species seperately - implying a full interaction model between species and both the slope and intercept coefficients. library(ggtext) penguins %&gt;% ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + ylab(&quot;Body \\n Mass \\n (g)&quot;) + xlab(&quot;Flipper Length (mm)&quot;) + labs(color=&quot;Penguin Species&quot;, title = &quot;&lt;b&gt; Penguin body mass increases with flipper length&lt;/b&gt;&lt;br&gt; &lt;span style = &#39;font-size:10pt&#39;&gt;Penguins of three species, &lt;span style=&#39;color:#F8766D&#39;&gt;Adelie&lt;/span&gt;, &lt;span style=&#39;color:#00BA38&#39;&gt;Chinstrap &lt;/span&gt;, and &lt;span style=&#39;color:#619CFF&#39;&gt;Gentoo&lt;/span&gt;, have body mass that increase with flipper length. &lt;/span&gt;&quot;)+ theme_minimal() + theme( plot.title.position = &quot;plot&quot;, plot.title = element_textbox_simple(size=12), axis.title.y = element_text(angle = 0, vjust = 0.5,size=14), axis.text = element_text(size=12), legend.position = &quot;none&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). 11.3.6 Figure captions If you were going to include this figure in a paper you should also include a figure caption. The figure caption should be detailed. Think too detailed. You should try to imagine handing the figure and figure caption over to a friend and they should be able to tell the whole “story” of that figure from only those two pieces of information. It is a good idea for each of your figures to have a topic sentence, typically in bold, that highlights the key take home message for your audience. So for example, the figure caption for the above plot could be: Figure 1. Penguin body mass increases with flipper length across three penguin species A plot of body mass in grams (g) versus flipper length in milimeters (mm) for three Penguin species: Adelie (red), Chinstrap (green), and Gentoo (blue). A separate linear regression fit to each species type is also shown, highlighting that penguin body mass increases with flipper length at different rates across different species. It is not surprising to see this correlation, since body size and body mass are typically highly correlated phenotypes. 11.3.7 Colors and fonts Will Chase’s excellent talk on the Glamour of Graphics goes into some details about choices of fonts and colors. One thing to keep in mind when producing graphics for both papers and talks is that some people are colorblind so can not distinguish between certain combinations of colors. There are different types of colorblindness. You can check how your plot will appear to color blind people by uploading it to websites like this. In R there is also a package for simulating colorblindness: # remotes::install_github(&quot;clauswilke/colorblindr&quot;) library(colorblindr) ## Loading required package: colorspace p = penguins %&gt;% ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + ylab(&quot;Body \\n Mass \\n (g)&quot;) + xlab(&quot;Flipper Length (mm)&quot;) + labs(color=&quot;Penguin Species&quot;, title = &quot;&lt;b&gt; Penguin body mass increases with flipper length&lt;/b&gt;&lt;br&gt; &lt;span style = &#39;font-size:10pt&#39;&gt;Penguins of three species, &lt;span style=&#39;color:#F8766D&#39;&gt;Adelie&lt;/span&gt;, &lt;span style=&#39;color:#00BA38&#39;&gt;Chinstrap &lt;/span&gt;, and &lt;span style=&#39;color:#619CFF&#39;&gt;Gentoo&lt;/span&gt;, have body mass that increase with flipper length. &lt;/span&gt;&quot;)+ theme_minimal() + theme( plot.title.position = &quot;plot&quot;, plot.title = element_textbox_simple(size=12), axis.title.y = element_text(angle = 0, vjust = 0.5,size=14), axis.text = element_text(size=12), legend.position = &quot;none&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) cvd_grid(p) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). You can try changing your color palette to correct these issues. But don’t forget to change your title colors as well! A common set of more color-blind friendly palettes are the viridis palettes. p = penguins %&gt;% ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + ylab(&quot;Body \\n Mass \\n (g)&quot;) + xlab(&quot;Flipper Length (mm)&quot;) + labs(color=&quot;Penguin Species&quot;, title = &quot;&lt;b&gt; Penguin body mass increases with flipper length&lt;/b&gt;&lt;br&gt; &lt;span style = &#39;font-size:10pt&#39;&gt;Penguins of three species, &lt;span style=&#39;color:#440154&#39;&gt;Adelie&lt;/span&gt;, &lt;span style=&#39;color:#21908C&#39;&gt;Chinstrap &lt;/span&gt;, and &lt;span style=&#39;color:#FDE725&#39;&gt;Gentoo&lt;/span&gt;, have body mass that increase with flipper length. &lt;/span&gt;&quot;)+ scale_color_viridis_d()+ theme_minimal() + theme( plot.title.position = &quot;plot&quot;, plot.title = element_textbox_simple(size=12), axis.title.y = element_text(angle = 0, vjust = 0.5,size=14), axis.text = element_text(size=12), legend.position = &quot;none&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) p ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). These look a bit better for people with color blindness: cvd_grid(p) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). 11.3.8 Multi-panel plots If you are creating multi-panel plots, each panel should be labeled with either a lowercase or upper case letter. You should head the warning that “whitespace is like garlic” and make sure you don’t overcrowd your figures. In general, when creating multi-panel figures, each panel should be part of the same “story” or focused on the same question. If you are building multi-panel plots, it can be a lot easier to put the panels into presentation building software like Powerpoint, Keynote or Google Slides and arrange your panels together. However, the patchwork R package is pretty slick and gives you pretty fine-grained control over how to organize plots together. Multi-panel figure captions should include descriptions of each figure separately, labeled with the same values as in the multi-panel plots. 11.3.9 Miscellaneous Advice When you have a lot of points in a plot, consider using a heatmap - for example geom_hex. When you save your figures - especially if they have a large number of points - you will find much smaller file sizes if you save them as .png files. Err on the side of simplicity and showing the data whenever possible. Mapping geographically and plotting timeseries have entirely different sets of aesthetics and manipulations you will need to do. Google is your friend. The plots above included these searches for your instructor: - “check colors colorblindness R” - “geom_smooth with groups” - “ggtext color to match plot colors” - “label group of points ggrepel” (wrong turn) - “remove grid lines theme minimal” - and many more! The R graph gallery is your friend 11.4 Creating Expository Tables There have been many arguments for why tables should be turned into figures. Your instructor has that natural inclination as well. However, sometimes you will need to produce a table either because the format of the journal requires it (Table 1 in a clinical paper consists of descriptive statististcs has a very defined format, and usually can’t be replaced with a figure). In R there are a variety of packages for making tables. During exploration, I generally make them using the skimr or dplyr packages. I like skimr for the quick overview: penguins %&gt;% skimr::skim() Table 11.1: Data summary Name Piped data Number of rows 344 Number of columns 8 _______________________ Column type frequency: factor 3 numeric 5 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts species 0 1.00 FALSE 3 Ade: 152, Gen: 124, Chi: 68 island 0 1.00 FALSE 3 Bis: 168, Dre: 124, Tor: 52 sex 11 0.97 FALSE 2 mal: 168, fem: 165 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist bill_length_mm 2 0.99 43.92 5.46 32.1 39.23 44.45 48.5 59.6 ▃▇▇▆▁ bill_depth_mm 2 0.99 17.15 1.97 13.1 15.60 17.30 18.7 21.5 ▅▅▇▇▂ flipper_length_mm 2 0.99 200.92 14.06 172.0 190.00 197.00 213.0 231.0 ▂▇▃▅▂ body_mass_g 2 0.99 4201.75 801.95 2700.0 3550.00 4050.00 4750.0 6300.0 ▃▇▆▃▂ year 0 1.00 2008.03 0.82 2007.0 2007.00 2008.00 2009.0 2009.0 ▇▁▇▁▇ But I often use dplyr more frequently for looking at individual variables or pairs of variables: penguins %&gt;% dplyr::count(is.na(sex),species) ## # A tibble: 5 x 3 ## `is.na(sex)` species n ## &lt;lgl&gt; &lt;fct&gt; &lt;int&gt; ## 1 FALSE Adelie 146 ## 2 FALSE Chinstrap 68 ## 3 FALSE Gentoo 119 ## 4 TRUE Adelie 6 ## 5 TRUE Gentoo 5 11.4.1 Table 1 As previously mentioned, Table 1 is a table of descriptive statistics and is frequently required in a number of different types of journals - most notably medical journals. There are thus usually highly specialized tools for creating this type of table. For example, in R the table1 package can be used to make descriptive tables: library(table1) ## ## Attaching package: &#39;table1&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## units, units&lt;- penguins %&gt;% table1(~ factor(sex) + as.factor(island)+ bill_length_mm + bill_depth_mm | species,data=.) Adelie(N=152) Chinstrap(N=68) Gentoo(N=124) Overall(N=344) factor(sex) female 73 (48.0%) 34 (50.0%) 58 (46.8%) 165 (48.0%) male 73 (48.0%) 34 (50.0%) 61 (49.2%) 168 (48.8%) Missing 6 (3.9%) 0 (0%) 5 (4.0%) 11 (3.2%) as.factor(island) Biscoe 44 (28.9%) 0 (0%) 124 (100%) 168 (48.8%) Dream 56 (36.8%) 68 (100%) 0 (0%) 124 (36.0%) Torgersen 52 (34.2%) 0 (0%) 0 (0%) 52 (15.1%) bill_length_mm Mean (SD) 38.8 (2.66) 48.8 (3.34) 47.5 (3.08) 43.9 (5.46) Median [Min, Max] 38.8 [32.1, 46.0] 49.6 [40.9, 58.0] 47.3 [40.9, 59.6] 44.5 [32.1, 59.6] Missing 1 (0.7%) 0 (0%) 1 (0.8%) 2 (0.6%) bill_depth_mm Mean (SD) 18.3 (1.22) 18.4 (1.14) 15.0 (0.981) 17.2 (1.97) Median [Min, Max] 18.4 [15.5, 21.5] 18.5 [16.4, 20.8] 15.0 [13.1, 17.3] 17.3 [13.1, 21.5] Missing 1 (0.7%) 0 (0%) 1 (0.8%) 2 (0.6%) However, note that there are R variable names in this table. We can remove these by creating new variables with easier to read names and units. penguins2 = penguins %&gt;% dplyr::mutate( sex = factor(sex, levels=c(&quot;female&quot;,&quot;male&quot;,&quot;Missing&quot;), labels= c(&quot;Female&quot;,&quot;Male&quot;,&quot;Missing&quot;)) ) label(penguins2$sex) = &quot;Sex&quot; label(penguins2$island) = &quot;Island&quot; label(penguins2$bill_length_mm) = &quot;Bill Length&quot; label(penguins2$bill_depth_mm) = &quot;Bill Depth&quot; units(penguins2$bill_length_mm) = &quot;mm&quot; units(penguins2$bill_depth_mm) = &quot;mm&quot; penguins2 %&gt;% table1(~ sex + island + bill_length_mm + bill_depth_mm | species,data=.) Adelie(N=152) Chinstrap(N=68) Gentoo(N=124) Overall(N=344) Sex Female 73 (48.0%) 34 (50.0%) 58 (46.8%) 165 (48.0%) Male 73 (48.0%) 34 (50.0%) 61 (49.2%) 168 (48.8%) Missing 0 (0%) 0 (0%) 0 (0%) 0 (0%) Missing 6 (3.9%) 0 (0%) 5 (4.0%) 11 (3.2%) Island Biscoe 44 (28.9%) 0 (0%) 124 (100%) 168 (48.8%) Dream 56 (36.8%) 68 (100%) 0 (0%) 124 (36.0%) Torgersen 52 (34.2%) 0 (0%) 0 (0%) 52 (15.1%) Bill Length (mm) Mean (SD) 38.8 (2.66) 48.8 (3.34) 47.5 (3.08) 43.9 (5.46) Median [Min, Max] 38.8 [32.1, 46.0] 49.6 [40.9, 58.0] 47.3 [40.9, 59.6] 44.5 [32.1, 59.6] Missing 1 (0.7%) 0 (0%) 1 (0.8%) 2 (0.6%) Bill Depth (mm) Mean (SD) 18.3 (1.22) 18.4 (1.14) 15.0 (0.981) 17.2 (1.97) Median [Min, Max] 18.4 [15.5, 21.5] 18.5 [16.4, 20.8] 15.0 [13.1, 17.3] 17.3 [13.1, 21.5] Missing 1 (0.7%) 0 (0%) 1 (0.8%) 2 (0.6%) That is a pretty passable Table 1! Note that there is only one significant digit past the decimal. Some other nice things about this table that should be emulated are: (1) reporting of N for each column, (2) reporting of both N and the percentages for factor variables, (3) for conditional tables like this also including the overall total. 11.4.2 Color in tables While Table 1 is the most common use of tables that you will run into, they are often also used for reporting large collections of results across factor variables. For example, when reporting the results of state level polls, it is easier to summarize them in tabular format as the website FiveThirtyEight does: Most data analytic languages now make it possible for you to create expository tables with colors like these professional tables. In R there are a number of packages for making pretty tables. The gt package is an early effort by the folks at Rstudio to make a ggplot2 equivalent grammar of tables. That is one to keep your eye on if you like the grammar style approach to composition. But some of the other table-making options are more mature and have more bells and whistles. For example with the formattable package you can add within-cell barcharts, colors and icons - Laura Ellis has a nice walkthrough on how to make tables like this: It can be useful to highlight important changes with arrows or +/- scales. Particularly when looking at data year over year. 11.4.3 Captions As with figures, your expository tables should have captions with a declarative summary statement in bold, followed by a complete description of the table, sufficient for a reader to understand the table as a self contained unit. One common difference is that Table captions are typically shown above tables, as opposed to below. 11.5 Scientific graphics vs infographics In his paper on infographics and statistical visualization Andrew Gelman talks about two types of graphics: Statistical data visualization, which is focused not on visual appeal but on facilitating an understanding of patterns in an applied problem, both in directing readers to specific information and allowing the readers to see for themselves. Infographics, which ideally should be attractive, grab one’s attention, tell a story and encourage the viewer to think about a particular dataset, both as individual measurements and as a representation of larger patterns. Both can be valuable, though for the purposes of this course we are more focused on the former. That being said, one thing that statisticians could certainly learn from infographic designers is that the focus on story can make even the most complicated and dense statistical graphics more interpretable to your audience. 11.6 Additional Resources Glamour of Graphics (slides) Glamour of Graphics (talk) Infovis and Statistical Graphics: Different Goals, Different Looks How to make beautiful tables in R table1 R package ggtext R package ggforce R package 11.7 Homework Template Repo: https://github.com/advdatasci/homework10 Repo Name: homework10-ind-yourgithubusername Pull Date: 2020/11/09 9:00AM Baltimore Time "],["week-11.html", "12 Week 11 12.1 Week 11 Learning objectives 12.2 Methodology vs Data Analysis 12.3 The sections of a methodological paper 12.4 Comparisons 12.5 Transparency 12.6 Software 12.7 Additional Resources 12.8 Homework", " 12 Week 11 12.1 Week 11 Learning objectives At the end of this lesson you will be able to: Identify the differences between a methodological and data analysis paper Structure and write the sections of methodological papers Understand the role and importance of simulation in methodological work Be able to perform appropriate comparisons to previous methodology 12.2 Methodology vs Data Analysis This course is primarily focused on performing and communicating data analysis. But it is also incredibly common for a data scientist to be called on to create new techniques or methodologies to answer important scientific questions. Just as there is an accepted structure for communicating a data analysis, there is also standard structure for communicating a new technique or methodology. Before we go into this standard structure it is a good idea to describe what we mean by a new “methodology”. In (bio)statistics methodology often involves the development of a new mathematical model. As Leo Breiman puts it in his classic paper Statistical Modeling: The Two Cultures There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools. In data science we sometimes develop new mathematical models, but often data science methodological development involves a variety of other kinds of methodological development. Common examples could be: Putting together a collection of mathematical models in a specific order Developing a piece of software that does a specific kind of data processing Creating a new workflow for analyzing a generic data type Creating or applying a new class of machine learning models Identifying a new way to repurpose existing data to answer new questions With traditional statistical development, we can rely on purely mathematical arguments about optimality. One challenge of the broader definition of methodological development is that it can be harder to know what makes a method “good”. In this context the comparison and evaluation will largely be empirical and will need to follow the same best practices as any thoughtful data analysis. However the purpose of a methodological paper or report is very different than a standard data analysis. The primary difference is that the scientific question you are trying to answer is about how well the proposed method will behave in a variety of contexts. Direct answers to a scientific question may be a piece of that puzzle, but they are not sufficient by themselves to know that a method works or is good. 12.3 The sections of a methodological paper As we discussed in week 9; a scientific paper can be distilled into four parts: A set of methodologies A description of data A set of results A set of claims When you (or anyone else) writes a paper the goal is to communicate clearly items 1-3 so that they can justify the set of claims you are making. Before you can even write down 4 you have to do 1-3. So that is where you start when writing a paper. Good methodological papers have the following characteristics: States the scientific problem the method is trying to solve at the beginning of the abstract and introduction. Describes a recent and relevant data set where this scientific problem occurs. Describes a method, justifying decisions based on how they are connected to the scientific problem Performs a thorough comparison to the best methods available through simulation or data examples. We don’t expect authors to implement other people’s methods in software just to compare them, but we would like to see some comparison to the best methods that have software available. Explains the metrics for comparison and why the comparison shows the proposed method represents a practical advance. Applies the method to the example data set and explains the solution to the problem. Includes well-documented scripts and data that perform the analyses presented in the paper. 12.3.1 Abstracts Recall the generic structure for an abstract from Nature: You can use this similar structure with a few modifications: Here I have annotated the abstract of a highly-regarded paper from the journal Biostatistics. The key components of the abtract are: Background: Here you should discuss the broad scientific field and methodological challenges. Specific problem: Here you should provide a discussion of what other methods exist and why there is still a need for a new method. Brief description: Here you should describe the method you have developed briefly. Comparison Results: Here you should describe how your method performs relative to the state of the art. Method sharing: Here you should describe how others can get access to your work - typically in the form of a piece of software 12.3.2 Introductions An introduction should be a more lengthy (1-3 pages) explanation of the scientific problem motivating your methodology, why there is still a need for methodological development and people should care, how your method works, and a comparison to the state of the art. Always start with the big picture! Provide the audience with enough background that they understand why you are tackling this problem and what others have done. Another important structural element is that introductions should go from general to specific. So you should start by putting the idea in its general context, e.g. “we need methods to help us address confounding in genomics experiments” and then narrow down to the problem you are addressing specifically, e.g. “we need methods to correct for batch effects”. One device you can use in the introduction or the beginning of the methods section is to create a simple simulated, or fake, example where the problem is illustrated more clearly than it would be in real data. Brad Efron is an example of a statistician who often uses this technique very effectively. It helps to orient the reader before you launch into more complicated statistical or computational ideas. You should also briefly review what other people have done in the area. The most critical thing is never underestimate how little people know or care about what you are working on. It is your job to explain to them why they should. 12.3.3 Description of your method One of the biggest differences with a standard data analytic paper is that the methods are the results in a methodological paper. So most methodological journals and reports will flip the order of methods and results, putting the methods first. When describing your method you should remember that the audience needs to know three things: What goes in to your method: Do you start with the raw data from the sensing equipment? A set of tidy data that have already been organized? JSON data from an API? What does your method do to the data: Here you want to describe in precise detail how your method works using equations, psuedocode, or a very precise plain language description. What does your method produce: Do you get predictions? Estimates? Confidence intervals? For data processing pipelines - what does the data look like when it leaves your workflow or pipeline? Mathematical language is often used for statistical or machine learning methods. There is a temptation to be overly complicated with your notation - this temptation may be reinforced by reviewers looking for novelty and your own struggles with how to make a complicated method easy to understand. Resist this temptation at all costs! To make sure your method is understandable: Reduce the number of parameters in your model when possible Use parameter values that are common to your field or easy to follow (don’t use \\(\\alpha\\) for type II errors, don’t use \\(\\beta\\) for error terms) Explain every parameter that appears in an equation in plain language directly following the equation Minimize the use of subscripts and superscripts wherever possible You may also be describing a workflow or a set of computational steps that a user must perform. You migtht be doing nothing more than combining a set of previously developed software in a particular order, or applying software from a new field to your area of application. When describing software steps you will often use pseudocode which outline the steps that are being performed. The difficult tradeoff with pseudocode is between simplicity and ensuring that all steps are concretely documented. Regardless, if you are describing software or workflows, then you should ensure your code is made available. 12.3.4 Simulation results A key component of methodological papers is simulation results. Simulated data is “made up data” that you control completely. Simulation is commonly used by statisticians/data analysts to: (1) estimate variability/improve predictors, (2) to evaluate the space of potential outcomes, and (3) to evaluate the properties of new algorithms or procedures. It can be really useful for understanding when a method works and doesn’t work. Simulated data can come in different flavors like: Fully parametric and made up * You simply simulate from known distributions Parametric but modeling real data characteristics - You may model certain characteristics in real data - Then use those model fits to generate data from known distributions Non-parametric - resampling real data - You take a real data set - You sample from that data set and dadd “signal” Non-parametric - just real data - You use real data where some set of controls lets you control the signal - It is often the most realistic, but can be hard to know the “right answer” As an example, I often work with data from RNA-sequencing experiments. One of the things we often do is count the number of “reads” that correspond to a particular gene: You could simulate this data Fully parametric and made up - By simulating from a negative binomial distribution Parametric but modeling real data characteristics - By modeling characteristics of the sequencing machine, then simulating from a negative binomial distribution Non-parametric - resampling real data - By resampling from different groups of samples and genes to create signal Non-parametric - just real data - By sampling data from only Y or X chromosomes and making comparisons across males and females Regardless, the goals of these simulations are to show how your methods will behave. Some statisticians, particularly applied statisticians, aren’t fond of simulation for evaluating methods. One reason is that you can always simulate a situation that meets all of your assumptions and make your approach look good. Real data rarely conform to model assumptions and so are harder to “trick”. On the other hand, I really like simulation, it can reveal a lot about how and when a method will work well and it allows you to explore scenarios - particularly for new or difficult to obtain data. The best papers consider at least the “extremes”: Simulation where the assumptions are true There are a surprising number of proposed methods/analysis procedures/analyses that fail or perform poorly even when the model assumptions hold. This could be because the methods overfit, have a bug, are computationally unstable, are on the wrong place on the bias/variance tradeoff curve, etc. etc. etc. I always do at least one simulation for every method where the answer should be easy to get, because I know if I don’t get the right answer, it is back to the drawing board. Simulation where things should definitely fail I like to try out a few realistic scenarios where I’m pretty sure my model assumptions won’t hold and the method should fail. This kind of simulation is good for two reasons: (1) sometimes I’m pleasantly surprised and the model will hold up and (2) (the more common scenario) I can find out where the model assumption boundaries are so that I can give concrete guidance to users about when/where the method will work and when/where it will fail. The first type of simulation is easy to come up with - generally you can just simulate from the model. The second type is much harder. You have to creatively think about reasonable ways that your model can fail. I’ve found that using real data for simulations can be the best way to start coming up with ideas to try - but I usually find that it is worth building on those ideas to imagine even more extreme circumstances. Playing the evil demon for my own methods often leads me to new ideas/improvements I hadn’t thought of before. It also helps me to evaluate the work of other people - since I’ve tried to explore the contexts where methods likely fail. In any case, simulation sections are most believable when they show both where a method is likely to work and where it is likely to fail. 12.3.5 Empirical results Most methodological papers will also include results on actual empirical data in addition to simulation. Typically, we won’t know the “right” answer on these data so the analysis is typically done for illustration purposes. Though in some cases, there will be a data set that is designed specifically for evaluation purposes, with signal “spiked in” so that one can determine if the method is able to capture it. Empirical results sections are often shorter in methodological papers but are critically important. They show that the method works out in the wild, without the simplification of simulation. A good empirical results section: Describes the data and why they pose the specific challenge your method is trying to address Describes how the method was applied in detail including any tuning parameters that you may need to set to get the method to work Describes the results, especially in comparison to the state of the art. 12.3.6 Conclusions and discussion Remember “tell them what you are going to tell them, tell them, then tell them what you told them”. In your introduction you explained what you were going to tell your audience, in the methods and results you told them, and in the conclusions and discussion you should start with a summary of what you told them. It is then a good idea to highlight potential limitations of your methods and to provide a fair assessment of how they perform relative to others. Be honest! It builds trust when a methods paper highlights potential issues for you, so that you don’t have to discover them yourself. After you have explained the limitations you can conclude with the strengths of your method and the potential next steps for your audience - often with a reminder to where they can find your tool or software. 12.4 Comparisons In some very rare cases, you will propose a method to address a problem that no one has ever thought of before. But the majority of the time, you will need to compare your method to existing software, tools, workflows, or statistical models. A common criteria used to evaluate methodds is how well they stack up against the “state of the art”. The most sophisticated methodological journals, like Genome Biology, make it an explicit requirement for publication: Method articles should describe novel methods that are shown to be a clear advance over existing state-of-the-art methods in a side-by-side demonstration, where possible. For computational methods, demonstration of superiority should be carried out using the same dataset. Where possible, the method should be benchmarked using a synthetic dataset (or other dataset where the ground truth is known), and its utility on real data demonstrated. A Method article need not necessarily provide novel biological insights, but these can help to demonstrate the method’s utility. One of the most obvious “failure modes” for methological papers is the lack of a sufficiently robust comparison to existing methodology. These comparisons involve both a literature review - to discover your potential competitors - and a simulation or empirical study - where you apply your method and the other methods to the same data. A good comparison does both of these steps well. One of the challenges you may run into is that competitor methods may not have software available. In this case, a fair comparison is simply to state that software for that method does not exist. You aren’t expected to create software for competivive methods. 12.5 Transparency Data science methods can often have major implications financially, medically, culturally, and ethically. It is therefore critical that we can evaluate these methods in as complete a manner as possible. When developing a method, it is critical that you provide the code and data from your work for others to test and build on. Recently a group from Google proposed an artificial intelligence approach for diagnosing breast cancer from mammography screens they claimed was superior to clinical judgement: However, a number of the details behind this approach where not made available: But as we have seen previously this kind of thing can go seriously wrong. The details must be available to make a method really understandable, useable, and scientific. As we put it in a response: Scientific progress depends on the ability of independent researchers to scrutinize the results of a research study, to reproduce the study’s main results using its materials, and to build on them in future studies (https://www.nature.com/nature-research/editorial-policies/reporting-standards). Publication of insufficiently documented research does not meet the core requirements underlying scientific discovery2,3. Merely textual descriptions of deep-learning models can hide their high level of complexity. Nuances in the computer code may have marked effects on the training and evaluation of results4, potentially leading to unintended consequences5. Therefore, transparency in the form of the actual computer code used to train a model and arrive at its final set of parameters is essential for research reproducibility. To be transparent, you should at minimum include the code you used in your analysis, all simulated data, and a way to request access to empirical data. 12.6 Software Incentives in academia lean heavily toward producing papers and less toward producing and maintaining software. However, to really make your methodological contribution useful for others you will need to wrap it into user friendly software that is well documented. But there are some real reasons to write software as a data scientist that are critically important: You probably got into data science to have an impact. One of the most direct and quantifiable ways to have an impact on the world is to write software that other scientists, educators, and data scientists use. If you write a data science method paper with no software the chance of impacting the world is dramatically reduced. Software is the new publication. I couldn’t name one paper written by a graduate student (other than mine) in the last 2-3 years. But I could tell you about tons of software packages written by students/postdocs (both mine and at other places) that I use. It is the number one way to get your name out there in the data science community. If you write a software package you need for yourself, you will save yourself tons of time in sourcing scripts, and remembering where all your code/functions are. Most importantly might be that creating an R package is building something. It is something you can point to and say, “I made that”. Leaving aside all the tangible benefits to your career, the profession, etc. it is maybe the most gratifying feeling you get when working on research. If you write in R, there are a number of good guides for creating R packages. In Jeff’s research group they use this guide. One thing to keep in mind is that even if your code is a bit messy or hard to use, it can often be rescued with good documentation: 12.7 Additional Resources What makes a good applied methods paper Writing Tips: Why we Publish Methods Papers Ten Simple Rules for Reproducible Computational Research Marie Davidian’s simulation lecture notes 12.8 Homework Template Repo: https://github.com/advdatasci/homework11 Repo Name: homework11-ind-yourgithubusername Pull Date: 2020/11/16 9:00AM Baltimore Time "],["week-12.html", "13 Week 12 13.1 Week 12 Learning objectives 13.2 Why give data analytic presentations? 13.3 Types of data analytic presentations 13.4 Structure of your talk 13.5 Slide style 13.6 Presenting 13.7 Equations 13.8 Figures 13.9 Job talk specific issues 13.10 Answering hard questions 13.11 Finishing on time 13.12 Where you should put your talk 13.13 Example Presentation: Methods 13.14 Example Presentation: Analysis 13.15 Presenting to a General Audience 13.16 Presenting to a Technical Audience 13.17 Additional Resources 13.18 Homework", " 13 Week 12 13.1 Week 12 Learning objectives At the end of this lesson you will be able to: Identify the types and lengths of data analysis presentations Identify and create the key components of data analytic presentations Combine components to create a data analytic story Be able to identify and use different styles of data analytic presentations This lecture borrows significantly from Shannon Ellis’ excellent work in creating the lectures for Written and Oral Communication in Data Science. 13.2 Why give data analytic presentations? When you are first starting out you should accept pretty much every opportunity to speak about your research you get. In approximate order of importance, the value of talks early in your career are: To meet people To make people excited about your ideas/software/results To make people understand your ideas/software/results To practice speaking Later the reasons evolve, altough not be as much as you’d think. The main change is that 4 evolves more into “to show people you are a good speaker”. The importance of point 1 can’t be overstated. At the beginning of your career, the primary reason you are giving talks is for people to get to know you. Being well regarded is absolutely not the goal of academia. However, being well known and well regarded can make a huge range of parts of your job easier. So first and foremost make sure you don’t forget to talk to people before, after, and during your talk. Point 2 is more important than point 3. As a scientist, it is hard to accept that the primary purpose of a talk is advertising, not science. See for example Hilary Mason’s great presentation Entertain, don’t teach. Here are reasons why entertainment is more important: People will legit fall asleep on you if you don’t keep them awake - this is embarrassing People will never understand your ideas/software/results if they fell asleep and didn’t hear about them There is no way to convey any reasonably complicated scientific idea completely in an hour - even less so in shorter periods of time If you entertain people they might go read your paper/use your code which is the best way to achieve goal 3. That being said, be very careful to avoid giving a TED talk (unless you are giving a Ted talk!). If you are giving a scientific presentation the goal is to communicate scientific ideas. So while you are entertaining, don’t forget why you are entertaining. 13.2.1 Why is this part of advanced data science? The technical components of data science (statistics, computing, databases, etc.) are usually the first things you learn in the field. You might take a course in R programming or in biostatistics to learn how to fit regression models. There is sometimes the mistaken impression when you are taught these tools for the first time that because data science is empirical it means that it isn’t a human enterprise. But as we discussed in Week 9, the more advanced you become in data analysis, the more it becomes a communication artform. Just as with written data analyses, it is critical that you are able to convey your data analysis message in oral presentations. Without effective communication the technical, statistical, and computational analyses you perform will not have any impact. 13.3 Types of data analytic presentations There are a few different types of data analytic presentations that you will give over the course of your career. The big categories include: Informal group or lab meeting presentations Goal: Update people on what you are doing and get help. What to talk about: Research in progress. This may either be a brief update or an attempt to get help. Short conference talks Goal: Introduce yourself and your research so people will want to learn more. What to talk about: Research you have completed, distilled into the highest level form and accessible to a broad audience. Long form seminar style talks Goal: Introduce yourself and your research so people will want to learn more. What to talk about: Research you have completed with a little bit more detail, but still at a high enough level that most people can follow along. Job talks Goal: Get a job. What to talk about: Research you have completed that highlights your potential, some technical details that are impressive, and a summary of your plans going forward. Flash talks Goal: Give people a teaser of what you are up to and intrigue them enough to read your work or talk to you. What to talk about: Just the high level problem and your most high impact result. There are three “typical” lengths with a little bit of wiggle room on the exact timing: flash (2-5 minutes), short form conference talks or research group updates (15-20 min), and long form seminar talks (50min-1hr). While this description is pretty specific to academic style talks; getting good at each of these forms of presentation is critical, whether you are in academia, industry or government. Elevator pitches in industry, like flash talks in academia, are how you get your foot in the door with potential funders, collaborators, employees, employers, or bosses. Short form talks are a part of updates whether you are in a research group or as a part of a presentation to your management teams. Long form talks are probably a little less common in industry (based on my convenience sample of industry folks) but it is worth learning how to present well anyway in case you want to end up on the keynote lecture circuit and raise your profile in industry. 13.4 Structure of your talk The biggest trap in giving a talk is assuming that other people will follow you because you follow the talk. In general it is always better to assume your audience knows less than you think they do. People like to feel smart. I have rarely heard complaints about people who went too basic in their explanations, but frequently hear complaints about people being lost. That being said, here are some structural tips. Your mileage may vary. Always lead with a brief, understandable to everyone statement of your problem. Explain the data and measurement technology before you explain the features you will use Explain the features you will use to model data before you explain the model When presenting results, make sure you are telling a story. The last point is particularly important. Usually by the results section people are getting a little antsy. So a completely disparate set of results with little story behind them is going to drive people bonkers. Make sure you explain up front where the results are going (e.g. “Results will show our method is the fastest/most accurate/best ever”), then make sure that your results are divided into sections by what point they are making and organized just like they would be if you were telling a story. 13.4.1 Flash talk structure As a person who has been stuck in a large number of interminable flash talks, the main piece of structural advice is to not go long! Typically a flash talk will consist of between 2-5 slides. 1-2 slides to introduce the key problem 1-2 slides to introduce the solution One slide to point people to a website where they can learn more. A key problem people often run into with flash talks (as they are often required to introduce yourself generally to an audience) is that they try to cover too much ground. A flash talk should only have one take home message. 13.4.2 Short form talk structure In 15 minutes you have very little time to get into detail. Typically you might expand the scope of your talk into: A few slides to introduce the background A few slides to introduce the data A few slides describing the methods A few slides describing the results A slide summarizing conclusions and pointing to further reading A shorter conference talk may have a couple of key messages, but with this little time, you will likely not have any room to get into detail. The purpose of the talk is to get the audience hooked so that they will read whatever follow up material you point them to, approach you and talk to you after the conference, or invite you to speak in a longer form format. 13.4.3 Long form talk structure A long form talk allows you more space to introduce your topic more thoroughly. But remember, even in 50 minutes, it will be hard to explain everything you did. Nor should you! Remember a data analytic presentation is about telling a story, so you want to keep the same sort of arc: Background Introduce the Data Introduce the Methods Describe the results Summarize and point people to next steps Some of the more painful talks to attend are those that fail to touch one or more of these key points - say by only covering background and methods, or by only describing the results. The point is to take the listener along through the story so they can see the main conclusions, then give them something to do after. 13.5 Slide style Now that you’ve outlined what you’ll talk about in your presentation, it’s time to decide how you’re going to put those ideas onto slides. There are many things to consider when designing visually-appealing slides. 13.5.1 Text A few guidelines when deciding the words you’ll include on slides: limit the number of ideas on a single slide limit words on slides encourage listening (limit reading) include references on the slides directly (and at the end!) Do not try to cram all your ideas onto a single slide. And, everything you say does not have to be up on the slides. You’re speaking for a reason. The slides should help guide you along in telling your story, but your audience should have to listen to what you’re saying to get the full picture of your story. 13.5.2 Fonts To convey textual information to your audience, choosing a good font is important. It’s generally safe to stick to Sans Serif font. Sans Serif fonts can be searched through at Google Fonts. And, when in doubt, Helvetica is a safe font choice. Further, use consistent fonts throughout your talk. More than one font can be used to draw viewers attention to something, but typically there should be a consistent font used throughout the presentation. Finally, as with most rules, there are exceptions, so take time deciding on what font looks best for each part of your presentation. 13.5.3 Size Regardless of what font you choose for your text, your text should be very large. It’s often argued that you should design your slides for the back of the room. The same goes for images. There’s no need to leave empty space on your slides around images. Expand your images to take up as much space on the slide as possible. 13.5.4 Colors Finally, with regards to slide design, colors matter. Choosing a dark background (such as dark gray) means that your text should be light (maybe white). If your background is light, your text should be dark. Contrast will make it easier on your audience to read. Additionally, use color to focus on text you want your audience to focus in on. If all your text is white, highlight a few words in a brighter color to draw your listener’s attention to what’s important. The same goes for figures and plots. All images should have colors that are viewable and distinguishable by every person in your audience. Consider color blindness. Consider colors that tend not to project well (such as yellows or light pastels). Then, design your slides using colors that will work for your audience. 13.5.5 Alignment Visually, things should be aligned. Center things in the middle of your slide. Make sure text boxes that should line up do line up. It may seem trivial, but it’s important to line things up so viewers aren’t distracted by something like alignment and can instead focus on your talk. 13.6 Presenting When it comes time to give the presentation, there are a few things to keep in mind. First and foremost, speak clearly. Whether you are a loud-talker, a soft-speaker, someone who speaks slowly, or a fast-talker, the most important thing is to make sure that the words you say are clear to your audience. Second, you should be excited. If you’re not excited or passionate about what you’re talking about, why would your audience be? If you’re not excited, it may be best to reconsider your outline and re-work your story until you are. To avoid being boring during your talk, you can vary the speed and volume of your voice. You can move around so you’re not always in one place. And, you can use pauses effectively. It’s ok to not speak every second that you’re up there. You can pause and give your audience some time to look at a graph before you talk about it. Silence for short periods of time is allowed! Finally, be yourself. If you naturally like to tell jokes, tell jokes during your presentation. If stories are more your style, work a story or two in. Ultimately, if you’re not comfortable in front of your audience, check to make sure you’re being true to who you are as an individual. Finally, when presenting it’s important to remember that you’ve seen the data and figures you’re presenting many times, but your audience has not. They’ll need time to get up to speed! Thus, it’s important to always introduce your axes (tell your audience what is on the x-axis and what is on the y-axis), explain the colors on your plot, and walk the audience through the figure you’re presenting. 13.6.1 Be prepared The day of your presentation, things may go wrong. Power goes out, projectors don’t always present your material exactly the same way it looks on your computer screen. Technology doesn’t always behave as we expect it too. In these cases, you just have to roll with the punches. To avoid any big issues, it’s best to bring a copy of your talk on a USB drive, carry any adapters you may need should you have to (or choose to) present from your own computer. Don’t expect the location to have the adapter you need. And, be sure your phone is on silent. After that, just roll with anything that goes wrong. The audience will be forgiving, as they all know that technology can sometimes act unexpectedly. 13.7 Equations If you are giving a data science talk you will probably have some equations. That is ok. But the way you present them is critically important to giving an understandable talk. Here are a few important points: Before presenting an equation explain the data Whenever possible use words instead of symbols in equations (Expression = Noise + Signal is better than E = N + S is better than Y = X + E) No more than two subscripts When explaining an equation First explain the point of the equation (we are trying to model expression as a function of noise and signal) Then explain what each symbol is (E is expression, N is noise, etc.) Then explain how the model relates them (E is a linear function of signal and noise) Try to avoid subscripts, and no more than two are allowed. 13.8 Figures If you have a figure in your talk you should present it in the following way. Explain what the figure is supposed to communicate (e.g. “this figure shows our method has higher accuracy”) Explain the figure axes (e.g. “the y-axis is sensitivity the x-axis is 1-specificity”) Explain what trends the audience should look for (e.g. “curves that go straight up at zero and across the top of the plot are best”) 13.9 Job talk specific issues When giving a job talk you have an additional job on top of the four main goals we talked about above. The goal is to present your complete professional persona to a bunch of people who (mostly) won’t know who you are. You should tailor this a little bit to the place you are applying to a job, but don’t try to pretend to be something you are not. You can show a little more theory at a theory place and a few more results at an applied place, but don’t claim you are proving theorems all the time if you aren’t. You should include both at the beginning and the end of the talk brief summaries of all the stuff you have worked on and plan to work on so they get an idea of who you are in a complete sense. But only talk about one specific project when giving the talk. There is nothing more detrimental to your chances of getting a job than going way over time or not getting to give your whole talk. Two things that I think you want to convey when giving a job talk are: You would be a fun person to work with and can play well with others You have some unique expertise that will strengthen the place you are applying Traditionally in data science, (bio)statistics and computer science departments, people demonstrated #2 by showing that they could prove really hard theorems or do really difficult computations. At some places, that is still a really good thing to do. Other things that might make you unique are the ability to write amazing R packages that get used, the ability to analyze massive data sets other people can’t, the ability to teach courses that students will want to take but the department doesn’t have, or ideas about a brand new research area (with some data to back them up). Before giving a job talk ask around and get a feel for the sorts of things that people have heard about the place you are applying so you can be smart about your choices of how/what to present. 13.10 Answering hard questions Inevitably you will get hard questions during your talk. The most important point is not to panic and not to get defensive. It is way better to just say I don’t know, then to get upset. When you get asked a really hard question you should: Take a second and a deep breath. If necessary, ask the person to repeat the question. Answer the question the best you can come up with on the spot Say I don’t know if you don’t know. If you say I don’t know, then you can give your best guess and explain it is a guess. The key is to distinguish what kind of response you are giving. Are you giving a response where you know the answer because you actually looked into that? Are you giving a complete guess where you have no idea? Or, what is more likely, are you somewhere in between? Most importantly, don’t feel embarrassed! Hard questions happen to everyone and if you are polite, explain how sure you are about your answer, and try your best it won’t be a problem. 13.10.1 That one person who keeps going bonkers on you Almost everywhere you give a talk there will be a person who is upset/intense/aggressive. Do not fall into the temptation to be aggressive in return. Answer their first questions politely just like everyone else. If they keep asking really aggressive/lots of questions, you are within your rightst to say: “You are bringing up a lot of good issues, I’d be happy to discuss with you after the presentation more in depth, but in the interest of time I’m going to move on to the next part of my talk”. It is ok to keep things moving and to finish on time. 13.11 Finishing on time Do it. People will love you for it. If you are the last speaker in a session and others have gone long, adapt and go shorter. The value you gain by making your audience happy &gt;&gt;&gt;&gt; the extra 5 minutes of details you could have explained 13.12 Where you should put your talk If you have weblinks in your talk you need to post it online. There is no way people will write down any of the links in your talk. Two good places to put your talks are https://speakerdeck.com/ or http://www.slideshare.net/. But then link to all the talks from one, single, memorable site you can show people at the very end of your talk. All my talks are at http://jtleek.com/talks/. You are welcome to send a pull request with your talk there if it is Leek group related, or you can put them on your own short and sweet web link. I personally like Slideshare a little better these days because the slides are much easier to view on mobile phones and iPads, which is the most likely place someone will read your talk. 13.13 Example Presentation: Methods To walk you through an example of a data science presentation, we’re going to use a talk presented at a statistics conference by Lucy D’Agostino McGowan, a well-known biostatistician and R programmer. The slides used in the talk and a video of the talk itself (!) can be found here. The title of Lucy’s talk was “Harnessing the Power of the Web via R Clients for Web APIs.” In this talk she discussed accessing information from APIs using the httr package. So, if you need a refresher on this topic, the slides and video here would be a great reference! 13.13.1 Talk Outline In the introduction, Lucy expresses her excitement and gives her audience an outline of the talk. The three topics she’s going to cover are: What is an API? How are APIs accessed from R Case studies. We didn’t discuss this above; however, a general rule of thumb is that three main topics is about the most you should include in a talk, as humans’ attention spans are not very long. Lucy nailed this one with exactly three main topics to discuss, all of which tie together into a story about how to get information from Web APIs! 13.13.2 Slide Design As Lucy dives into the content of her talk, she follows many of the best practices discussed above. We’ll pull a few slides from the talk to discuss here; however, do reference the whole talk to see it in its entirety! Lucy uses large fonts and colors that can be distinguished by anyone in the audience and that present well. As a result, everyone in the room, no matter where there seat was, was able to learn and be entertained. Additionally, Lucy uses effective highlighting of text using different colors. Here, the parts of the text that Lucy wants the audience to focus on are in a brighter blue and a brighter green! Also note that there is not a lot of blank space on the slides. Additionally, Lucy’s slides guided what she was going to say but didn’t include all the words on the slides. Here, we see large font and large images but no words. This allowed the audience to listen to what she was saying rather than read all the words on the slides! 13.13.3 Presentation In addition to having well-designed slides, Lucy was excited as she spoke, was clear in her speech, spoke at an easy-to-listen-to pace, and her personality came through as she spoke. This is all evident if you listen to the video of her presentation. Lucy being true to her personality helps make it a wonderful presentation. If your personality is different than Lucy’s, that’s totally ok! Just be sure that when you present, you’re presenting in a way that you’re most comfortable! 13.14 Example Presentation: Analysis Lucy’s presentation is an example of a wonderful, educational, and helpful talk; however, her goal wasn’t to talk in depth about an analysis she’s done. Often, as a data scientist, you’ll be tasked with presenting an analysis you’ve done. To do so, you’ll want to use the same guidelines we previously discussed, but with a focus on presenting the story of your data analysis. This includes the plots, analytical approach, and findings of your project. That said, your slides should still be well-designed, you should still tell a story, and your text and colors should all be readable when projected; however, in this case, there are a few additional caveats to consider. To discuss these, we’ll use an example presentation from Julia Silge, who is a data scientist at Stack Overflow and a wonderful presenter. In 2017 at the rstudio::conf, Julia Silge presented “Text Mining the tidy Way”. The video and visuals from her presentation are available online. This talk discusses the tidytext package (which she and David Robinson developed) and example analyses using this awesome R package. We’ve discussed this package previously, but if you want a refresher on tidytext, how to work with textual data, and sentiment analysis, this is a great resource! As above, we’ll highlight a few slides from the talk and discuss considerations to make when presenting a data analysis. 13.14.1 Talk Outline As in Lucy’s talk, Julia Silge, in addition to an introduction and a conclusion, had three main parts to her talk: tidytext package Jane Austen example NASA example In data analysis talks, it’s often best to start with background information. Julia covers this information when discussing what the tidytext package is. Then, she delves into discussing a few examples of the types of analyses you can do with this package. 13.14.2 Slide Design As discussed above with Lucy’s presentation, in Julia’s slides, she limits the amount of information she puts on any one slide, thus allowing the audience to listen to the words she’s saying. And, she has text that’s large enough and legible when projected. However, what’s different about this presentation is how Julia presents her analyses. When presenting an analysis, it’s generally best to first explain the approach and then present the results. For example, Julia uses TF-IDF to analzze text in Jane Austen’s novels. She first explains what TF-IDF is. She includes the bare minimum amount of information on her slide, so that her audience is only seeing what’s necessary. Then, she verbally describes what TF-IDF is. On subsequent slides she includes the code she used to run the analysis, discussing what is going on with the code on each slide and describing the output. Finally, for each part of her analysis, she includes a visualization of the results. These figures have text large enough to be seen by all in the audience, are well-labeled, and include an informative title. Note that Julia does not go into every single detail of every analysis, nor just she talk about all the things she tried in a single presentation. She includes enough information so that her audience learns and can go try this type of analysis on their own, if they’re interested! 13.14.3 Presentation Watching the 18 minute video will give you the best sense of how Julia presented the information in her tidytext presentation; however, we’ll highlight a few things here. Julia spoke at a reasonable pace and clearly, keeping her audience interested. Additionally, she presented true to her personality. Julia and Lucy are two different people and that is clear in their presentation styles. Differences are wonderful and help keep presentations interesting! It would be boring if everyone were the same. Additionally, Julia knew her audience. She knew that there would be many programmers in her audience, and that most of them would be familiar with the R programming language. Thus, she didn’t need to explain what the R syntax was. She could put R code on her slides, and her audience would successfully follow along. But, she knew that not everyone has worked with the tidytext package. Thus, she focused her time here, discussing the purpose of and basics on how to use the tidytext package and provided clear examples of analyses using this package. When outlining and preparing presentations, always consider your audience. If your audience is more technical, include more details about how you did the analysis. If they’re less technical, be sure to explain what you did at an appropriate level. And, of course, always make sure your figures are clear, regardless of your audience. We’ll note that at around 17:45 in the presentation, Julia went to show an image but it didn’t immediately show up how she had intended. This is a great example of rolling with the punches. She didn’t panic. She simply acknowledged that it didn’t work the first time and tried again. The audience, of course, didn’t mind, but when you’re presenting, these small occurrences can feel like a big deal. When it happens to you in the future, know that things don’t always work as anticipated and that this happens to everyone. If this happens to you, take a play out of Julia’s book and simply acknowledge what happened, try again, and continue on! 13.15 Presenting to a General Audience When considering presenting to a general audience, this would be an audience that doesn’t know a lot about data science or quantitative analysis. This would be an audience that generally does not code or analyze data on a regular basis. This could be individuals at your company who are more focused on day-to-day operations or an audience of high schoolers getting their first introduction to data science. 13.15.1 The Goal Regardless of exactly to whom you’re presenting, the goal of this type of presentation is to teach at a high level what you did. Your presentation should still inspire, educate, and entertain, but it should leave out some of the nitty gritty details and keep in more background and general explanation of your approach. 13.15.2 General Presentation Example To discuss giving presentations to a general audience using a specific example, we’re going to walk through a presentation that was given to a general audience. The audience for this presentation was college students with a year of calculus but no particular background in statistics. This means that any statistical concepts would have to be explained, but it could be assumed that the audience was fairly analytical. The slides for this talk can be viewed here. The title of this talk was “Upcycling genomics data: From publicly-available ‘junk’ to priceless ‘treasure’.” The outline of this presentation was as follows: Introduction Part 1: recount Part 2: phenopredict Part 3: application Conclusion Click on this link and scroll through the slides to get an idea of what the presentation looked like. The content of the talk is not what we’ll be focusing on, so it doesn’t actually matter if you look at these slides and understand what they’re talking about. Rather, we’re going to walk through how this presentation was organized and designed to focus on a general audience. We’ll also walk through the slides for a talk on the exact same topic, that that was designed and presented to a technical audience. To walk through how this talk was organized, we’re going to use the following figure: Each slide in the presentation was assigned to one of 7 categories. Each category is a different color on the figure: Question - the motivating question used to tie the story together Outline - to remind the audience what we’ve already discussed and what we’re about to discuss Background - Information the audience is required to know in order to understand the project/analyses Approach - How the data were analyzed Teaching - A quick lesson about a specific topic to ensure the results make sense to the audience Conclusion - The end of the slides, wrapping up the presentation If we consider background, teaching, and explaining the approach, we see that about 65% of the time was spent providing information to the audience that had nothing to do with the specific new information being presented about this analysis. 13.16 Presenting to a Technical Audience When presenting to a technical audience, you’re presenting to an audience of your peers. These are individuals who regularly analyze data and/or are familiar with your work and the way you approach data analysis. This could be other members of your team or other individuals at a conference in your specific field of expertise. As a result, you can often limit the amount of information you provide as introductory material in this type of presentation and really focus on the details of your analysis. 13.16.1 The Goal Unlike a general audience presentation where the goal is to teach at a high level what you did, the goal of this type of presentation is to really explain details of what you did. The details and caveats of your approach should be the focus here. 13.16.2 Technical Presentation Example Like in the case of a general talk, we’re going to walk through a technical talk. The title of the technical talk we’ll be discussing is “Improving the value of public data with recount2 and phenotype prediction” and the slides are viewable here. Go to these slides now and scroll through them. Note the mathematical notation on slides 70 through 87 – this level of detail was not included for the general audience (slides 96-110 [here for the general audience comparison(bit.ly/general_audience)]. While the title and presentation details differ from the general audience presentation, the outline of the talk remains the same: Introduction Part 1: recount Part 2: phenopredict Part 3: application Conclusion We’ll break down this talk in the same way that we did the general audience talk. We can see that the bulk of the presentation is spent on background, results, and approach. There is not much teaching in this technical presentation. 13.16.3 General vs Technical Each presentation was on the same topic. Thus, it would be beneficial to compare the two directly. Here we see the general presentation visualized at the top and technical beneath it. The focus on teaching and approach in a general talk and the focus on approach and results in a technical talk becomes evident when compared on the same image. To summarize this graphically, using the same bar plots, here we have the same categories we’ve looked at previously, but we see the results for each type of presentation. General talk is in green. Technical talk is in orange. When we focus on the results and teaching bars, we can see this general talk did a lot more teaching, while the technical talk focused more time presenting results. 13.17 Additional Resources Example talks Jeff/Leekgroup talks Karl Broman Zach Holmann Hilary Mason Jean Fan Talk guides http://www.howtogiveatalk.com/ The talk on talks How to Give a Scientific Presentation Talk software xaringan xaringanthemer 13.18 Homework Template Repo: https://github.com/advdatasci/homework12 Repo Name: homework12-ind-yourgithubusername Pull Date: 2020/11/23 9:00AM Baltimore Time "],["week-13.html", "14 Week 13 14.1 Week 13 Learning objectives 14.2 Introduction 14.3 Basic Data Analytic Iteration 14.4 Statistical Methods Systems 14.5 The Set of Expected Outcomes 14.6 Anomaly Space 14.7 Debugging Anomalies 14.8 Summarizing Root Causes 14.9 Homework", " 14 Week 13 14.1 Week 13 Learning objectives At the end of this lesson you will be able to: Define the set of expected outcomes for an analytic system Define a data analytic anomaly Describe the process of debugging a data analysis Identify root causes of data analytic anomalies 14.2 Introduction Lucky Week 13 has arrived! And perhaps fittingly, we will discuss the process of what to do when things go wrong. The presentation of how data analyses are conducted is typically done in a forward manner. A question is posed, data are collected, and given the question and data, a system of statistical methods is assembled to produce evidence. That evidence is then interpreted in the context of the original question. While such a description provides a useful model, it is incomplete in that it assumes the statistical methods are completely determined by the question and the data. In practice, there is an equally important “backwards” process that data analysts use to either revise their statistical approach or investigate potential problems with the data. This process of revision is driven by observing deviations between the data and what we expect the data to look like. Much previous work dedicated to studying the data analysis process has focused on the notion of “statistical thinking,” or the cognitive processes that occur within the analyst while doing data analysis. Grolemund and Wickham refer to these “forwards” and “backwards” aspects of data analysis as part of a sense-making process, where analysts attempt to reconcile schema that describe the world with data that are measured from the world. Should there be a discrepancy between the schema and the data, the analyst can update the schema to better match the observed data. These updates ultimately result in knowledge being accumulated. 14.3 Basic Data Analytic Iteration In the course of any data analysis, at some point we are confronted with a basic activity, which can be called the basic data analytic iteration. This iteration is at the core of the sense-making process described by Grolemund nad Wickham. The iteration only begins once we look at some data. At that point, we must decide to do something (or do nothing). Once we make that decision, the iteration begins anew. In the course of executing this iteration, we may observe something that is unexpected, and that is what this lecture is about. These unexpected outcomes are what we will call anomalies. Characterizing anomalies in data analysis requires that we have a thorough understanding of the entire system of statistical methods that are being applied to a given dataset. Included in this system are traditional statistical tools such as linear regression or hypothesis tests, as well as methods for reading data from their raw source, pre-processing, feature extraction, post-processing, and any final visualization tools that are applied to the model output. Ultimately, anomalies can be caused by any component of the system, not just the statistical model, and it is the data scientist’s job to understand the behavior of the entire system. Yet, there is little in the statistical literature that considers the complexity of such systems and how they might behave under real-world conditions. Once we have set off with a scientific question (even if vaguely stated), we can go on to collecting or assembling some data to address that question. Typically, we will need to build and develop develop a system of statistical methods that we can apply to the data to generate evidence. This system includes every aspect of contact with the data, including reading the data from its source, pre-processing, modeling, visualization, and generation of output. The basic data analytic iteration comes in four parts. Once a question has been established and a plan for obtaining or collecting data is available, we can do the following: Construct a set of expected outcomes Apply a data analytic system to the data Diagnose any anomalies in the analytic system output Make a decision about what to do next In this lecture, we are going to focus on the first three steps of this iteration, and in particular, Step 3. But first, we need to talk about statistical methods systems and the set of expected outcomes. 14.4 Statistical Methods Systems To study the manner in which data analyses can produced unexpected results, it is useful to first consider data analyses as a system of connected components that produce specific outputs. A statistical methods system is a collection of data analytic elements, procedures, and tools that are connected together to produce a data analysis output, such as a plot, summary statistic, parameter estimate, or other statistical quantity. By connecting these elements and tools together, we create a complex system through which data are transformed, summarized, visualized, and modeled. Each of the components in the system will have its own inputs and outputs and tracing the path of those intermediate results plays a key role in developing an understanding of the system. There are also contextual inputs to the data analysis, such as the main question or problem being addressed, the data, the choice of programming language to use, the audience, and the document or container for the analysis, such as Jupyter or R Notebooks. While these inputs are not necessarily decided or fundamentally modified by the analyst, the data analyst may be expected to provide feedback on some of these inputs. In particular, should an analysis produce an unexpected result, the analyst might identify one of these contextual inputs as the root cause of the problem. A statistical methods system can be characterized as a sequence of steps or a deterministic algorithm. The algorithm is not necessarily uni-directional and may double-back on itself and branch into multiple sections. Ultimately, the algorithm produces an output that may be interpreted by the analyst or passed on to serve as input to another system. In describing the behavior of any system, one must be careful to define the resolution of the system (i.e. how detailed we want to specify steps) and the boundaries of the system diagram. In particular, we should acknowledge what elements are excluded from the diagram, such as application-specific context or other assumptions. The development of a statistical methods system would typically be guided by statistical theory, as well as knowledge of the scientific question, any relevant context or previous research, available resources, and any design requirements for the system output. 14.4.1 Example: A Simple Linear Model System Consider a system that fits a simple linear model as part of a data analysis. This system reads in some data on a pair of variables \\(x\\) and \\(y\\), fits a simple linear regression via least squares, and outputs the intercept and slope estimates. A depiction of this system along with some representative R code is shown in Figure 1. Simple linear regression statistical methods system with pseudo-code. The diagram indicates that understanding how this system operates requires knowledge of (1) how the data are read in; (2) how the model is fit to the data; and (3) how the estimated coefficients are extracted from the model fit and outputted. Specifically, if we are using R to analyze the data, we must have an understanding of the read.csv(), lm(), coef(), and print() functions. 14.5 The Set of Expected Outcomes Once a statistical methods system is built, but before it is applied to the data, we can develop a set of expected outcomes for the system. These expected outcomes represent our current state of scientific knowledge and may reflect any gaps or biases in our understanding of the data, the problem at hand, or the behavior of the statistical methods system. The overarching goal of the data analysis is to produce outputs that will in some way improve our understanding of the scientific problem. Without any expected outcomes, it is challenging to interpret the output of the system or determine how the output informs our understanding of the underlying data generation process. An important property of the set of expected outcomes is that the expected outcomes are always stated in terms of the observed output of the system, not any underlying unobserved population parameters. We draw a distinction here between hypotheses, which are statements about the underlying population, and expected outcomes, which are statements about the observed sample data. Another property of the set of expected outcomes is that they will generally have sharp boundaries. Therefore, once we observe the output from the statistical methods system, we know immediately and with complete certainty whether the output is expected or unexpected (i.e falls into the set or not). Boundaries of this nature are important for the analyst so that decisions can be made regarding any next steps in the analysis. Developing a useful set of expected outcomes is part of the design process for a statistical methods system and depends on many factors, including our knowledge and assumptions about the underlying data generation process, our ability to model that process using statistical tools, our knowledge of the theoretical properties of those tools, and our understanding of the uncertainty or variability of the observed data across multiple samples. Hence, even though the underlying truth might be thought of as fixed, it is reasonable to assume that different analysts might develop different sets of expected outcomes, reflecting differing levels of familiarity with the various factors involved and different biases towards existing evidence or data. 14.5.1 Example: Expected Outcomes for the Sample Mean In some contexts, the set of expected outcomes may be derived from formal statistical hypotheses. For example, we may design a system to compute the sample mean \\(\\bar{x}\\) of a dataset \\(x_1,\\dots,x_n\\) that we model as being sampled independently from a \\(\\mathcal{N}(\\mu,1)\\) distribution. In this case, the output from the system is \\(\\bar{x}\\) and based on our current knowledge we may hypothesize that \\(\\mu=0\\). Under that hypothesis, we might expect \\(\\bar{x}\\) to fall between \\(-2/\\sqrt{n}\\) and \\(2/\\sqrt{n}\\). For \\(n=10\\), this interval is \\([-0.63,0.63]\\) and any observed value of \\(\\bar{x}\\) outside that interval would be an anomaly. Another analyst might be more familiar with this data generation process and therefore hypothesize that the underlying population mean is \\(\\mu=3\\) without assuming a Normal distribution. This analyst might also know that the data collection process can be problematic, leading to very large outlier observations on occasion. Therefore, based on experience and intuition, this analyst has a wider expected outcome interval of \\([1, 5]\\). In both examples here, the set of expected outcomes was a statement about \\(\\bar{x}\\), the output of the system applied to the observed data. The set of expected outcomes was also a fixed interval with clear boundaries, making it straightforward to determine whether the output would fall in the interval or not. 14.6 Anomaly Space An anomaly occurs only if there is a clearly defined set of expected outcomes for a system, and the observed output from the system does not fall into that set. The specification of an anomaly then requires three separate elements: A description of what specific system output or collection of outputs is observed; A description of how the outputs deviate from their expected outcomes; and An indication of when or under what conditions the deviation is observed. Continuing the example from the previous section, an anomaly for the sample mean could be “\\(\\bar{x}\\) is outside the expected interval of \\([-0.63, 0.63]\\) when a sample of size \\(n=10\\) is inputted to the system”. The observed output is \\(\\bar{x}\\), the deviation is \"outside the interval \\([-0.63, 0.63]\\)\", and the event occurs when \\(n=10\\). The anomaly space of a statistical methods system consists of the collection of potential outputs from the system which would indicate that an anomaly has occurred. Fundamentally, the anomaly space is the complement of the set of expected outcomes. Not all areas of the anomaly space are equally important and in some applications it may be that anomalies occuring in certain subsets of the anomaly space are more interesting than anomalies occurring elsewhere. The size of the anomaly space of a statistical methods system is determined by the outputs produced by the system. Looking back to the simple linear model system in Figure 1, there are only two outputs (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)) that define the anomaly space. Therefore, any anomalies for that system must be determined by those two values. As the number of system outputs grows, the size of the anomaly space may grow accordingly. For example, we can increase the size of the anomaly space for the simple linear model system by also returning the standard errors of the coefficients. With each additional output, we increase the number of ways in which anomalies can occur. Different systems with different sets of outputs will induce anomaly spaces of differing sizes and the nature of the anomaly space associated with a system may serve as a factor in choosing between systems. Also, because the anomaly space depends on the specification of the set of expected outcomes, different analysts with different expectations could induce different anomaly spaces for the same statistical methods system. Once a statistical methods system has been applied to the data and an anomaly has been observed, the “forward” aspect of data analysis is complete and the analyst must begin the “backward” aspect to determine what if any changes should be made to the analysis. Such changes could involve modifying the statistical methods system itself or could require changes to our set of expected outcomes based on this new information. However, before any decision can be made in response to observing an anomaly, a data analyst must enumerate the possible root causes of the anomaly and determine which root causes should be investigated. 14.7 Debugging Anomalies Once a statistical methods system has been applied to a dataset and the output is observed, we can determine whether an anomaly has occurred based on our understanding of the set of expected outcomes. If an anomaly occurs, we must attempt to identify its root cause. To do so, we can use the following process: State the anomaly in terms of what output, how it deviates, and when it is observed to occur. Reconstruct the entire sequence events (or as much as possible based on available information) that occurs leading up to the anomaly. This can be a sequence of code statements or a more abstract system diagram. The reconstruction can be detailed or not depending on how much information is available. With a reproducible analysis, it should be possible to reconstruct all the steps. Starting with the output, trace back through the system diagram or sequence of code statements and enumerate any possible sources of error and their likelihood of occurring. This process may branch off in different directions at each stage of the system diagram/code. Stop the process once we either reach an explanation that lies outside the system or we have identified a set of root causes that is not worth developing any further. Summarize the root causes. Once you’ve made your best efforts to identify the root causes of an anomaly, you must then decide what to do. We will not go into detail here about that decision process, but typically, you will want to follow up further on any key root causes. For example, if it’s possible that there is a problem with the data collection process, you might want to follow up with the people who originally collected the data to find out more information. 14.7.1 Basic Example Consider the basic system shown below. Basic System Diagram If we observe an anomaly in the “Output” portion of the sytem, we can trace backwards through the system to identify possible causes. In this process it is important to take a very myopic and step-by-step approach to ensure that we do not skip over anything (i.e. “think small”). In the figure above, if there is a problem with the “Output” then that problem might have originated in component “B”, which generates the output. So the first cause might be Bad output from “B” Moving back further, we might say either Good input to “B”, but bad output from “B”; or Bad input to “B” The event 2 above represents a failure of component “B”, which probably should be investigated. However, event 3 suggests there is a problem with component “A”. Here, the root causes are either A problem with component “B” even when the input is good A problem with component “A” Given the simplicity of the system diagram in this example, it’s not surprising that the set of root causes is small. Depending on the complexity of the system diagram, the debugging process could get similarly complicated. One possible way to simplify things is to break the system down into sub-systems in order to compartmentalize the different (independent) aspects of the system. 14.7.2 Example: Debugging a Simple Linear Model System We can describe the process of debugging the simple linear model system shown below. Simple linear regression statistical methods system with pseudo-code. Let’s say, for example, that under normal operation, we might expect that \\(\\hat{\\beta}_1\\approx 2\\). With typical random variation in the data we might expect \\(\\hat{\\beta}_1\\) to range from 0 to about 4. Therefore, based on past experience, it would be highly unexpected to observe \\(\\hat{\\beta}_1&lt;0\\) or \\(\\hat{\\beta}_1&gt;4\\). For this example, we will define the anomaly of interest as \\(\\hat{\\beta}_1 &lt; 0\\) when printed to the console. Note that although the set of expected outcomes is the interval \\([0, 4]\\), we define the anomaly as \\(\\hat{\\beta}_1&lt;0\\) and ignore the part of the anomaly space defined by \\(\\hat{\\beta}_1&gt;4\\). Similarly, possible anomalies concerning the intercept \\(\\hat{\\beta}_0\\) will not be developed here. Starting with the system description in the figure above, we can walk through the system backwards to identify any root causes. First, we might consider any problems with the print() function as it’s possible that the beta vector is fine but the print() function somehow corrupted it. This scenario is highly unlikely so we will then move backwards to the coef() function. Again, here, it’s unlikely thet coef() function is the cause of the problem as it’s very simple and has been in use a long time with linear models. This then brings us to the lm() function, which produces the model fit. Should we observe \\(\\hat{\\beta}_1&lt;0\\) there are perhaps two possibilities we might consider: The structural relationship between \\(x\\) and \\(y\\) has changed to no longer reflect the simple linear model; or The underlying structural relationship remains, but the input data to the linear model has been contaminated, perhaps with outliers. The first case represents our expectations being incorrect, which is always a possibility. It is possible that we perhaps misunderstood the relationship between \\(x\\) and \\(y\\) in the first place. For example, it could be that the data are naturally more variable than we thought they were, and so our set of expected outcomes for \\(\\hat{\\beta}_1\\) should be much larger. In any case, any structural change also needs to be large enough (relative to the noise in the data) so that we are able to observe it in the data. Developing the “contaminated data” event a bit further, we can propose that either there are outliers present in the raw data or outliers were somehow introduced into the data before inputting to the regression model. Note the difference here: One version says there are contaminated data in the raw dataset that originates from outside the system. The other version says that the outliers were introduced somehow by reading the data in. This can happen if functions like read.csv() attempt to convert data from one format to another. Another possibility is that missing data are encoded in a special way (e.g. using -99 is common) that is unknown to the read.csv() function. While the presence of outliers can be a root cause of the anomaly \\(\\hat{\\beta}_1&lt;0\\), outliers do not always cause an unexpected change in \\(\\hat{\\beta}_1\\). The outliers also have to be arranged in such a manner that they cause \\(\\hat{\\beta}_1\\) to be \\(&lt; 0\\), perhaps because of some selection process in the outlier generation/designation. 14.8 Summarizing Root Causes Once a debugging analysis has been done to a statistical methods system regarding a particular anomaly we must summarize the root causes. In the simplie linear model example above, the root causes of the anomaly “\\(\\hat{\\beta}_1&lt;0\\) when printed to the console” could be caused by A change in the structural relationship between \\(x\\) and \\(y\\) AND the change is large enough to be observed over the noise levels Contaminated observations in the raw data AND there is a selection process that arranges the contaminated data to produce \\(\\hat{\\beta}_1&lt;0\\) Contamination is introduced in reading the data AND there is a selection process that arranges the contaminated data to produce \\(\\hat{\\beta}_1&lt;0\\) Once the set of root causes are identified, a decision must be made about what to do next. This decision will depend on a number of factors, including the likelihood of each root cause of occurring. For example, if it is highly likely that our model is wrong, we may not bother to investigate the possibility of contaminated data. However, if this is a process that we have worked with for a long time and are confident in the model, we may be more concerned with contamination in the data. Another action we might take is to modify the system so that future applications to future datasets will provide more informative output. If it turns out that contaminated data can occasionally enter the system, we may want to introduce a step that filters those data, or perhaps stops the system when contaminated data are detected. Another possibility is to use a robust method that downweights extreme values. Ultimately, making decisions about what to do in each cycle of a basic data analytic iteration depends on knowing the set of options. Debugging data analytic anomalies is about systematically generating a set of root causes so that the data scientist can choose what to investigate, what to ignore, and what to modify. 14.9 Homework Template Repo: https://github.com/advdatasci/homework13 Repo Name: homework13-ind-yourgithubusername Pull Date: 2020/12/07 9:00AM Baltimore Time "]]
